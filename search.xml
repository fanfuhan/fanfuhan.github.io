<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[opencv-016-图像ROI与ROI操作]]></title>
    <url>%2F2019%2F03%2F28%2Fopencv-016%2F</url>
    <content type="text"><![CDATA[知识点图像的ROI(region of interest)是指图像中感兴趣区域、在OpenCV中图像设置图像ROI区域，实现只对ROI区域操作。 矩形ROI区域提取 矩形ROI区域copy 不规则ROI区域 ROI区域mask生成 像素位 and操作 提取到ROI区域 加背景or操作 add 背景与ROI区域 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * ROI及相关操作 */int main() &#123; Mat src = imread("../images/test.png"); imshow("input", src); int h = src.rows; int w = src.cols; // 获取ROI int cy = h / 2; int cx = w / 2; Rect rect(cx - 100, cy - 100, 200, 200); // 注意：roi 与 src指向同一块内存区域，改变roi,src也会改变 Mat roi = src(rect); imshow("roi", roi); // 人物背景图，换背景 // load image Mat image = imread("../images/boy.jpg"); imshow("input", image); // generate mask Mat hsv, mask, mask_not; cvtColor(image, hsv, COLOR_BGR2HSV); inRange(hsv, Scalar(35, 43, 46), Scalar(99, 255, 255), mask); imshow("mask", mask); // extract person Mat person; bitwise_not(mask, mask_not); imshow("mask_not", mask_not); bitwise_and(image, image, person, mask_not); imshow("person", person); // gengerate background Mat background = Mat::zeros(image.size(), image.type()); background.setTo(Scalar(255, 0 ,0)); imshow("background", background); // combine background + person Mat dst; bitwise_or(person, background, dst, mask); add(dst, person, dst); imshow("dst", dst); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import cv2 as cvimport numpy as npsrc = cv.imread("D:/javaopencv/dahlia_4.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)h, w = src.shape[:2]# 获取ROIcy = h//2cx = w//2roi = src[cy-100:cy+100,cx-100:cx+100,:]cv.imshow("roi", roi)# copy ROIimage = np.copy(roi)# modify ROIroi[:, :, 0] = 0cv.imshow("result", src)# modify copy roiimage[:, :, 2] = 0cv.imshow("result", src)cv.imshow("copy roi", image)# example with ROI - generate masksrc2 = cv.imread("D:/javaopencv/tinygreen.png");cv.imshow("src2", src2)hsv = cv.cvtColor(src2, cv.COLOR_BGR2HSV)mask = cv.inRange(hsv, (35, 43, 46), (99, 255, 255))# extract person ROImask = cv.bitwise_not(mask)person = cv.bitwise_and(src2, src2, mask=mask);# generate backgroundresult = np.zeros(src2.shape, src2.dtype)result[:,:,0] = 255# combine background + personmask = cv.bitwise_not(mask)dst = cv.bitwise_or(person, result, mask=mask)dst = cv.add(dst, person)cv.imshow("dst", dst)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像ROI与ROI操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-05-绘制几何形状及随机数的生成]]></title>
    <url>%2F2019%2F03%2F28%2Fopencv-015%2F</url>
    <content type="text"><![CDATA[知识点绘制几何形状 绘制直线 绘制圆 绘制矩形 绘制椭圆 填充几何形状 OpenCV没有专门的填充方法，只是把绘制几何形状时候的线宽thickness参数值设置为负数即表示填充该几何形状或者使用参数CV_FILLED 随机数方法：RNG 表示OpenCV C++版本中的随机数对象，rng.uniform(a, b)生成[a, b)之间的随机数，包含a，但是不包含b。 np.random.rand() 表示numpy中随机数生成，生成浮点数0～1的随机数, 包含0，不包含1。 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 绘制几何形状及随机数 */int main() &#123; Mat image = Mat::zeros(Size(512, 512), CV_8UC3); Rect rect(100, 100, 200, 200); // 绘制 rectangle(image, rect, Scalar(255, 0, 0), 2, LINE_8, 0); circle(image, Point(256, 256), 50, Scalar(0, 255, 0), 2, LINE_8, 0); ellipse(image, Point(256, 256), Size(150, 50), 360, 0, 360, Scalar(0, 0, 255), 2, LINE_8, 0); imshow("image_draw", image); // 填充 thickness=-1 or FILLED rectangle(image, rect, Scalar(255, 0, 0), FILLED, LINE_8, 0); ellipse(image, Point(256, 256), Size(150, 50), 360, 0, 360, Scalar(0, 0, 255), FILLED, LINE_8, 0); circle(image, Point(256, 256), 50, Scalar(0, 255, 0), -1, LINE_8, 0); imshow("image_fill", image); // 随机数 RNG rng(0xFFFFFF); image.setTo(Scalar(0, 0, 0)); Mat image_copy = image.clone(); for (int i = 0; i &lt; 100000; ++i) &#123; int x1 = rng.uniform(0, 512); int y1 = rng.uniform(0, 512); int x2 = rng.uniform(0, 512); int y2 = rng.uniform(0, 512); int b = rng.uniform(0, 256); int g = rng.uniform(0, 256); int r = rng.uniform(0, 256); rect.x = x1; rect.y = y1; rect.width = x2 - x1; rect.height = y2 - y1; // LINE_AA 反锯齿 line(image, Point(x1, y1), Point(x2, y2), Scalar(b, g, r), 1, LINE_AA, 0); rectangle(image_copy, rect, Scalar(b, g, r), 1, LINE_AA, 0); imshow("image_line", image); imshow("image_rect", image_copy); char c = waitKey(20); if (c == 27)&#123; // ESC break; &#125; &#125; waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526272829303132import cv2 as cvimport numpy as npimage = np.zeros((512, 512, 3), dtype=np.uint8)cv.rectangle(image, (100, 100), (300, 300), (255, 0, 0), 2, cv.LINE_8, 0)cv.circle(image, (256, 256), 50, (0, 0, 255), 2, cv.LINE_8, 0)cv.ellipse(image, (256, 256), (150, 50), 360, 0, 360, (0, 255, 0), 2, cv.LINE_8, 0)cv.imshow("image", image)cv.waitKey(0)for i in range(100000): image[:,:,:]= 0 x1 = np.random.rand() * 512 y1 = np.random.rand() * 512 x2 = np.random.rand() * 512 y2 = np.random.rand() * 512 b = np.random.randint(0, 256) g = np.random.randint(0, 256) r = np.random.randint(0, 256) # cv.line(image, (np.int(x1), np.int(y1)), (np.int(x2), np.int(y2)), (b, g, r), 4, cv.LINE_8, 0) cv.rectangle(image, (np.int(x1), np.int(y1)), (np.int(x2), np.int(y2)), (b, g, r), 1, cv.LINE_8, 0) cv.imshow("image", image) c = cv.waitKey(20) if c == 27: break # ESCä¸cv.imshow("image", image)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>绘制几何形状</tag>
        <tag>随机数生成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-014-使用resize进行图像插值(Image Interpolation)]]></title>
    <url>%2F2019%2F03%2F27%2Fopencv-014%2F</url>
    <content type="text"><![CDATA[知识点最常见四种插值算法 INTER_NEAREST = 0 #最近邻插值，速度快，没考虑周围像素影响 INTER_LINEAR = 1 #双线性插值 INTER_CUBIC = 2 #双立方插值，高质量 INTER_LANCZOS4 = 4 #高质量 关于这四种插值算法的详细代码实现与解释 三种常见双立方插值算法-CSDN 图像放缩之双立方插值 图像放缩之双线性内插值 Lanczos采样放缩算法 相关的应用场景几何变换、透视变换、插值计算新像素 API resize(InputArray src, OutputArray dst, Size dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR ) 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像插值 */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); int h = src.rows; int w = src.cols; float fx = 0.0, fy = 0.0; Mat dst = Mat::zeros(src.size(), src.type()); Size S(w * 2, h * 2); resize(src, dst, S, fx, fy, INTER_NEAREST); imshow("INTER_NEAREST", dst); resize(src, dst, S, fx, fy, INTER_LINEAR); imshow("INTER_LINEAR", dst); resize(src, dst, S, fx, fy, INTER_CUBIC); imshow("INTER_CUBIC", dst); resize(src, dst, S, fx, fy, INTER_LANCZOS4); imshow("INTER_LANCZOS4", dst); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324import cv2 as cvsrc = cv.imread("D:/vcprojects/images/test.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)h, w = src.shape[:2]print(h, w)dst = cv.resize(src, (w*2, h*2), fx=0.75, fy=0.75, interpolation=cv.INTER_NEAREST)cv.imshow("INTER_NEAREST", dst)dst = cv.resize(src, (w*2, h*2), interpolation=cv.INTER_LINEAR)cv.imshow("INTER_LINEAR", dst)dst = cv.resize(src, (w*2, h*2), interpolation=cv.INTER_CUBIC)cv.imshow("INTER_CUBIC", dst)dst = cv.resize(src, (w*2, h*2), interpolation=cv.INTER_LANCZOS4)cv.imshow("INTER_LANCZOS4", dst)cv.warpAffine()cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像插值</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-013-图像翻转(Image Flip)]]></title>
    <url>%2F2019%2F03%2F27%2Fopencv-013%2F</url>
    <content type="text"><![CDATA[知识点图像翻转的本质像素映射，OpenCV支持三种图像翻转方式 X轴翻转，flipcode = 0 Y轴翻转, flipcode = 1 XY轴翻转, flipcode = -1 相关的APIflip(src, dst, flipcode) src输入参数 dst 翻转后图像 flipcode 应用：摄像头拍摄后经常需要翻转 代码（c++,python）1234567891011121314151617181920212223242526272829303132#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像翻转 */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); Mat dst; // X轴 倒影 flip(src, dst, 0); imshow("x_flip", dst); // Y轴 镜像 flip(src, dst, 1); imshow("y_flip", dst); // XY轴 对角 flip(src, dst, -1); imshow("xy_flip", dst); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324252627282930import cv2 as cvimport numpy as npsrc = cv.imread("D:/vcprojects/images/test.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# X Flip 倒影dst1 = cv.flip(src, 0);cv.imshow("x-flip", dst1);# Y Flip 镜像dst2 = cv.flip(src, 1);cv.imshow("y-flip", dst2);# XY Flip 对角dst3 = cv.flip(src, -1);cv.imshow("xy-flip", dst3);# custom y-fliph, w, ch = src.shapedst = np.zeros(src.shape, src.dtype)for row in range(h): for col in range(w): b, g, r = src[row, col] dst[row, w - col - 1] = [b, g, r]cv.imshow("custom-y-flip", dst)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像翻转</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-012-视频读写]]></title>
    <url>%2F2019%2F03%2F26%2Fopencv-012%2F</url>
    <content type="text"><![CDATA[知识点VideoCapture 视频文件读取、摄像头读取、视频流读取VideoWriter 视频写出、文件保存、 CAP_PROP_FRAME_HEIGHT #高度 CAP_PROP_FRAME_WIDTH #宽度 CAP_PROP_FRAME_COUNT #数量 CAP_PROP_FPS #帧率 不支持音频编码与解码保存，不是一个音视频处理的库！主要是分析与解析视频内容。保存文件最大支持单个文件为2G。 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 视频读写 */int main() &#123; // 打开摄像头 // VideoCapture capture(0); // 打开视频文件 VideoCapture capture; capture.open("../images/vtest.avi"); if (!capture.isOpened()) &#123; cout &lt;&lt; "could not load video.." &lt;&lt; endl; return -1; &#125; Size S = Size((int) capture.get(CAP_PROP_FRAME_WIDTH), (int) capture.get(CAP_PROP_FRAME_HEIGHT)); int fps = capture.get(CAP_PROP_FPS); cout &lt;&lt; "capture fps: " &lt;&lt; fps &lt;&lt; endl; VideoWriter writer("D:/test.mp4", cv::VideoWriter::fourcc('D', 'I','V','X'), fps, S, true); Mat frame; while(capture.read(frame))&#123; imshow("input", frame); writer.write(frame); char c = waitKey(50); if(c == 27)&#123; break; &#125; &#125; capture.release(); writer.release(); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526import cv2 as cvimport numpy as npcapture = cv.VideoCapture("D:/vcprojects/images/768x576.avi")# capture = cv.VideoCapture(0) 打开摄像头height = capture.get(cv.CAP_PROP_FRAME_HEIGHT)width = capture.get(cv.CAP_PROP_FRAME_WIDTH)count = capture.get(cv.CAP_PROP_FRAME_COUNT)fps = capture.get(cv.CAP_PROP_FPS)print(height, width, count, fps)out = cv.VideoWriter("D:/test.mp4", cv.VideoWriter_fourcc('D', 'I', 'V', 'X'), 15, (np.int(width), np.int(height)), True)while True: ret, frame = capture.read() if ret is True: cv.imshow("video-input", frame) out.write(frame) c = cv.waitKey(50) if c == 27: # ESC break else: breakcapture.release()out.release() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>视频读写</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-011-图像像素归一化]]></title>
    <url>%2F2019%2F03%2F25%2Fopencv-011%2F</url>
    <content type="text"><![CDATA[知识点OpenCV中提供了四种归一化的方法 NORM_MINMAX NORM_INF NORM_L1 NORM_L2 最常用的就是NORM_MINMAX归一化方法. 四种归一化方法示例 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像像素归一化 */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; // imshow("input", src); Mat gray; cvtColor(src, gray, COLOR_BGR2GRAY); imshow("input", gray); // 显示图像用uchar类型，计算时转为float类型 gray.convertTo(gray, CV_32F); // NORM_MINMAX Mat dst = Mat::zeros(gray.size(), CV_32FC1); normalize(gray, dst, 1.0, 0, NORM_MINMAX); Mat res = dst * 255; res.convertTo(dst, CV_8UC1); // 显示图像用uchar类型 imshow("NORM_MINMAX", dst); // scale and shift by NORM_INF normalize(gray, dst, 1.0, 0, NORM_INF); res = dst * 255; res.convertTo(dst, CV_8UC1); imshow("NORM_INF", dst); // scale and shift by NORM_L1 normalize(gray, dst, 1.0, 0, NORM_L1); res = dst * 10000000; res.convertTo(dst, CV_8UC1); imshow("NORM_L1", dst); // scale and shift by NORM_L2 normalize(gray, dst, 1.0, 0, NORM_L2); res = dst * 10000; res.convertTo(dst, CV_8UC1); imshow("NORM_L2", dst); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526272829303132333435363738import cv2 as cvimport numpy as npsrc = cv.imread("D:/vcprojects/images/test.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)# 转换为浮点数类型数组gray = np.float32(gray)print(gray)# scale and shift by NORM_MINMAXdst = np.zeros(gray.shape, dtype=np.float32)cv.normalize(gray, dst=dst, alpha=0, beta=1.0, norm_type=cv.NORM_MINMAX)print(dst)cv.imshow("NORM_MINMAX", np.uint8(dst*255))# scale and shift by NORM_INFdst = np.zeros(gray.shape, dtype=np.float32)cv.normalize(gray, dst=dst, alpha=1.0, beta=0, norm_type=cv.NORM_INF)print(dst)cv.imshow("NORM_INF", np.uint8(dst*255))# scale and shift by NORM_L1dst = np.zeros(gray.shape, dtype=np.float32)cv.normalize(gray, dst=dst, alpha=1.0, beta=0, norm_type=cv.NORM_L1)print(dst)cv.imshow("NORM_L1", np.uint8(dst*10000000))# scale and shift by NORM_L2dst = np.zeros(gray.shape, dtype=np.float32)cv.normalize(gray, dst=dst, alpha=1.0, beta=0, norm_type=cv.NORM_L2)print(dst)cv.imshow("NORM_L2", np.uint8(dst*10000))cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像像素归一化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-010-图像像素值统计及应用（普通图像转化为二值图像）]]></title>
    <url>%2F2019%2F03%2F25%2Fopencv-010%2F</url>
    <content type="text"><![CDATA[知识点 最小(min) 最大(max) 均值(mean) 标准方差(standard deviation) API知识点 最大最小值minMaxLoc 计算均值与标准方差meanStdDev 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像像素值统计及应用（普通图像转化为二值图像） */int main() &#123; Mat src_bgr = imread("../images/test.png"); Mat src_gray; cvtColor(src_bgr, src_gray, COLOR_BGR2GRAY); if (src_bgr.empty() || src_gray.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input_bgr", src_bgr); // 计算灰度图像的最大最小值 double minVal, maxVal; Point minLoc, maxLoc; minMaxLoc(src_gray, &amp;minVal, &amp;maxVal, &amp;minLoc, &amp;maxLoc); cout &lt;&lt; "paramenters of src_gray:" &lt;&lt; endl; printf("min:%.2f, max:%.2f \n", minVal, maxVal); printf("min loc: (%d, %d) \n", minLoc.x, minLoc.y); printf("max loc: (%d, %d) \n", maxLoc.x, maxLoc.y); // 普通图像转二值图像 Mat mean, stddev; meanStdDev(src_bgr, mean, stddev); cout &lt;&lt; "paramenters of src_bgr:" &lt;&lt; endl; printf("blue channel mean:%.2f, stddev: %.2f \n", mean.at&lt;double&gt;(0, 0), stddev.at&lt;double&gt;(0, 0)); printf("green channel mean:%.2f, stddev: %.2f \n", mean.at&lt;double&gt;(1, 0), stddev.at&lt;double&gt;(1, 0)); printf("red channel mean:%.2f, stddev: %.2f \n", mean.at&lt;double&gt;(2, 0), stddev.at&lt;double&gt;(2, 0)); for (int row = 0; row &lt; src_bgr.rows; ++row) &#123; for (int col = 0; col &lt; src_bgr.cols; ++col) &#123; Vec3b bgr = src_bgr.at&lt;Vec3b&gt;(row, col); bgr[0] = bgr[0] &lt; mean.at&lt;double&gt;(0, 0) ? 0 : 255; bgr[1] = bgr[1] &lt; mean.at&lt;double&gt;(1, 0) ? 0 : 255; bgr[2] = bgr[2] &lt; mean.at&lt;double&gt;(2, 0) ? 0 : 255; src_bgr.at&lt;Vec3b&gt;(row, col) = bgr; &#125; &#125; imshow("binary", src_bgr); waitKey(0); return 0;&#125; 1234567891011121314151617181920import cv2 as cvimport numpy as npsrc = cv.imread("../images/test.png", cv.IMREAD_GRAYSCALE)cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)min, max, minLoc, maxLoc = cv.minMaxLoc(src)print("min: %.2f, max: %.2f"% (min, max))print("min loc: ", minLoc)print("max loc: ", maxLoc)means, stddev = cv.meanStdDev(src)print("mean: %.2f, stddev: %.2f"% (means, stddev))src[np.where(src &lt; means)] = 0src[np.where(src &gt; means)] = 255cv.imshow("binary", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像像素值统计</tag>
        <tag>普通图像转化为二值图像</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-009-色彩空间及其应用（提取图像的前景和背景）]]></title>
    <url>%2F2019%2F03%2F25%2Fopencv-009%2F</url>
    <content type="text"><![CDATA[知识点 RGB色彩空间 HSV色彩空间 -维基百科 ### 直方图算法中常用 YUV色彩空间 YCrCb色彩空间 # 皮肤检测常用 API知识点 色彩空间转换cvtColor 提取指定色彩范围区域inRange 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 色彩空间及其应用 */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // RGB ==&gt; HSV YUV YCrCb Mat hsv, yuv, ycrcb; cvtColor(src, hsv, COLOR_BGR2HSV); cvtColor(src, yuv, COLOR_BGR2YUV); cvtColor(src, ycrcb, COLOR_BGR2YCrCb); imshow("hsv", hsv); imshow("yuv", yuv); imshow("ycrcb", ycrcb); /* * 提取图像前景和背景 */ Mat src2 = imread("../images/boy.jpg"); imshow("input boy", src2); cvtColor(src2, hsv, COLOR_BGR2HSV); // 从HSV表中查到绿色的最低值和最高值，建立掩模 Mat mask, mask_not; inRange(hsv, Scalar(35, 43, 46), Scalar(77, 255, 255), mask); imshow("mask", mask); Mat fg, bg; // 提取背景 bitwise_and(src2, src2, bg, mask); // 提取前景 bitwise_not(mask, mask_not); imshow("mask_not", mask_not); bitwise_and(src2, src2, fg, mask_not); imshow("background", bg); imshow("foreground" ,fg); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425262728import cv2 as cvsrc = cv.imread("../images/test.png")cv.namedWindow("rgb", cv.WINDOW_AUTOSIZE)cv.imshow("rgb", src)# RGB to HSVhsv = cv.cvtColor(src, cv.COLOR_BGR2HSV)cv.imshow("hsv", hsv)# RGB to YUVyuv = cv.cvtColor(src, cv.COLOR_BGR2YUV)cv.imshow("yuv", yuv)# RGB to YUVycrcb = cv.cvtColor(src, cv.COLOR_BGR2YCrCb)cv.imshow("ycrcb", ycrcb)src2 = cv.imread("../images/boy.jpg");cv.imshow("src2", src2)hsv = cv.cvtColor(src2, cv.COLOR_BGR2HSV)mask = cv.inRange(hsv, (35, 43, 46), (99, 255, 255))dst = cv.bitwise_and(src2, src2, mask=mask)cv.imshow("mask", mask)cv.imshow("dst", dst)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>色彩空间</tag>
        <tag>提取图像前景和背景</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-008-图像通道的分离与合并]]></title>
    <url>%2F2019%2F03%2F24%2Fopencv-008%2F</url>
    <content type="text"><![CDATA[知识点OpenCV中默认imread函数加载图像文件，加载进来的是三通道彩色图像，色彩空间是RGB色彩空间、通道顺序是BGR（蓝色、绿色、红色）、对于三通道的图像OpenCV中提供了两个API函数用以实现通道分离与合并。 split // 通道分类 merge // 通道合并 扩展在很多CNN的卷积神经网络中输入的图像一般会要求[h, w, ch]其中h是高度、w是指宽度、ch是指通道数数目、OpenCV DNN模块中关于图像分类的googlenet模型输入[224,224,3]表示的就是224x224大小的三通道的彩色图像输入。 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像通道分离与合并 */int main() &#123; Mat src = imread("../images/baboon.jpg"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); vector&lt;Mat&gt; mv; // mv用于存储图像分离后各通道像素 Mat dst1, dst2, dst3; // 令蓝色通道为0 split(src, mv); mv[0] = Scalar(0); merge(mv, dst1); imshow("blue == 0", dst1); // 令绿色通道为0 split(src, mv); mv[1] = Scalar(0); merge(mv, dst2); imshow("green == 0", dst2); // 令红色通道为0 split(src, mv); mv[2] = Scalar(0); merge(mv, dst3); imshow("red == 0", dst3); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526import cv2 as cvsrc = cv.imread("../images/baboon.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# 蓝色通道为零mv = cv.split(src)mv[0][:, :] = 0dst1 = cv.merge(mv)cv.imshow("output1", dst1)# 绿色通道为零mv = cv.split(src)mv[1][:, :] = 0dst2 = cv.merge(mv)cv.imshow("output2", dst2)# 红色通道为零mv = cv.split(src)mv[2][:, :] = 0dst3 = cv.merge(mv)cv.imshow("output3", dst3)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像通道的分离与合并</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-007-图像像素之逻辑操作]]></title>
    <url>%2F2019%2F03%2F24%2Fopencv-007%2F</url>
    <content type="text"><![CDATA[知识点下面三个操作类似，都是针对两张图像的位操作 bitwise_and bitwise_xor bitwise_or 针对输入图像, 图像取反操作，二值图像分析中经常用 bitwise_not 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像像素的逻辑操作 */int main() &#123; // create image one, CV_8UC3创建三通道图像 Mat src1 = Mat::zeros(Size(400, 400), CV_8UC3); Rect rect(100,100,100,100); // Scalar() 参数为BGR三通道值，绿色和红色加起来是黄色 src1(rect) = Scalar(0, 255, 255); imshow("input1", src1); // create image two Mat src2 = Mat::zeros(Size(400, 400), CV_8UC3); rect.x = 150; rect.y = 150; src2(rect) = Scalar(0, 0, 255); imshow("input2", src2); // 逻辑操作 Mat dst1, dst2, dst3; bitwise_and(src1, src2, dst1); bitwise_xor(src1, src2, dst2); bitwise_or(src1, src2, dst3); imshow("and", dst1); imshow("xor", dst2); imshow("or", dst3); // 演示取反操作 Mat src = imread("../images/test1.jpg"); Mat dst; imshow("input", src); bitwise_not(src,dst); imshow("not", dst); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526272829import cv2 as cvimport numpy as np# create image onesrc1 = np.zeros(shape=[400, 400, 3], dtype=np.uint8)src1[100:200, 100:200, 1] = 255src1[100:200, 100:200, 2] = 255cv.imshow("input1", src1)# create image twosrc2 = np.zeros(shape=[400, 400, 3], dtype=np.uint8)src2[150:250, 150:250, 2] = 255cv.imshow("input2", src2)dst1 = cv.bitwise_and(src1, src2)dst2 = cv.bitwise_xor(src1, src2)dst3 = cv.bitwise_or(src1, src2)cv.imshow("dst1", dst1)cv.imshow("dst2", dst2)cv.imshow("dst3", dst3)src = cv.imread("../images/test1.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)dst = cv.bitwise_not(src)cv.imshow("dst", dst)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像像素逻辑操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生成假数据用于卷积神经网络模型训练]]></title>
    <url>%2F2019%2F03%2F24%2Fget_faked_data%2F</url>
    <content type="text"><![CDATA[背景在设计神经网络时，用于测试的imageNet等数据集太大，所以生成假数据用来测试神经网络能不能正常运行 代码123456789101112131415161718192021import tensorflow as tf# 参数设置batch_size = 32image_size = 24image_channel = 3n_classes = 10# 生成假数据用于训练模型def get_faked_train_batch(batch_size): images = tf.Variable(tf.random_normal(shape=[batch_size, image_size, image_size, image_channel], mean=0.0, stddev=1.0, dtype=tf.float32)) # tf.random_uniform() 标准均匀分布 labels = tf.Variable(tf.random_uniform(shape=[batch_size], minval=0, maxval=n_classes, dtype=tf.int32)) return images, labels # 生成假数据用于测试模型def get_faked_test_batch(batch_size): images = tf.Variable(tf.random_normal(shape=[batch_size, image_size, image_size, image_channel], mean=0.0, stddev=1.0, dtype=tf.float32)) # tf.random_uniform() 标准均匀分布 labels = tf.Variable(tf.random_uniform(shape=[batch_size], minval=0, maxval=n_classes, dtype=tf.int32)) return images, labels]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>卷积神经网络假数据生成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-006-Look Up Table(LUT)查找表的使用]]></title>
    <url>%2F2019%2F03%2F23%2Fopencv-006%2F</url>
    <content type="text"><![CDATA[知识点LUT查找表的简单原理 LUT查找表的作用 颜色匹配，比如讲灰度图像进行伪彩色增强 加快计算速度 API：applyColorMap(src, dst, COLORMAP) src 表示输入图像 dst表示输出图像 匹配到的颜色LUT， OpenCV支持13种颜色风格的查找表映射 COLORMAP ：13种色彩风格 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace std;using namespace cv;// 自定义LUTMat &amp;myColorMap(Mat &amp;image);/* * Look Up Table(LUT)查找表的使用 */int main() &#123; Mat src = imread("../images/LinuxLogo.jpg"); imshow("input", src); // 使用LUT Mat dst; applyColorMap(src, dst, COLORMAP_SUMMER); imshow("colorMap", dst); // 使用自己的LUT Mat my_dst, gray; cvtColor(src, gray, COLOR_BGR2GRAY); my_dst = myColorMap(gray); imshow("my_dst", my_dst); waitKey(0); return 0;&#125;// 自定义LUTMat &amp;myColorMap(Mat &amp;image) &#123; int lut[256]; for (int i = 0; i &lt; 256; ++i) &#123; if (i &lt; 127) lut[i] = 0; else lut[i] = 255; &#125; for (int row = 0; row &lt; image.rows; ++row) &#123; for (int col = 0; col &lt; image.cols; ++col) &#123; int pv = image.at&lt;uchar&gt;(row, col); image.at&lt;uchar&gt;(row, col) = lut[pv]; &#125; &#125; return image;&#125; 12345678910import cv2 as cvsrc = cv.imread("../images/LinuxLogo.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)dst = cv.applyColorMap(src, cv.COLORMAP_COOL)cv.imshow("output", dst)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>查找表（LUT）</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-005-像素算术操作]]></title>
    <url>%2F2019%2F03%2F23%2Fopencv-005%2F</url>
    <content type="text"><![CDATA[知识点像素算术操作 加add、减subtract、乘multiply、除divide saturate_cast&lt;T&gt;(value) # 类型转换注意点：图像的数据类型、通道数目、大小必须相同 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace std;using namespace cv;/* * 图像像素的加减乘除，两张图像大小类型要完全相同 */ int main()&#123; Mat src1 = imread("../images/opencv_images/LinuxLogo.jpg"); Mat src2 = imread("../images/opencv_images/WindowsLogo.jpg"); if(src1.empty() || src2.empty())&#123; cout&lt;&lt;"conld not read image..."&lt;&lt;endl; return -1; &#125; imshow("input1", src1); imshow("input2", src2); // 加法 Mat add_result = Mat::zeros(src1.size(),src1.type()); add(src1, src2, add_result); imshow("add_result", add_result); // 带权重的加法，一般推荐使用这个 Mat add_weight_result = Mat::zeros(src1.size(),src1.type()); addWeighted(src1, 0.5, src2, (1.0 - 0.5), 0.0, add_weight_result); imshow("add_weight_result", add_weight_result); // 减法 Mat sub_result = Mat::zeros(src1.size(),src1.type()); subtract(src1, src2, sub_result); imshow("sub_result", sub_result); // 乘法 Mat mul_result = Mat::zeros(src1.size(),src1.type()); multiply(src1, src2, mul_result); imshow("mul_result", mul_result); // 除法 Mat div_result = Mat::zeros(src1.size(),src1.type()); divide(src1, src2, div_result); imshow("div_result", div_result); // 自己实现加法操作 int b1 = 0, g1 = 0, r1 = 0; int b2 = 0, g2 = 0, r2 = 0; int b = 0, g = 0, r = 0; Mat my_add_result = Mat::zeros(src1.size(), src1.type()); for (int row = 0; row &lt; src1.rows; ++row) &#123; for (int col = 0; col &lt; src1.cols; ++col) &#123; b1 = src1.at&lt;Vec3b&gt;(row, col)[0]; g1 = src1.at&lt;Vec3b&gt;(row, col)[1]; r1 = src1.at&lt;Vec3b&gt;(row, col)[2]; b2 = src2.at&lt;Vec3b&gt;(row, col)[0]; g2 = src2.at&lt;Vec3b&gt;(row, col)[1]; r2 = src2.at&lt;Vec3b&gt;(row, col)[2]; // b1:0~255,b2:0~255, b1+b2可能大于255，所以需要转换，通过saturate_cast&lt;uchar&gt;() my_add_result.at&lt;Vec3b&gt;(row, col)[0] = saturate_cast&lt;uchar&gt;(b1 + b2); my_add_result.at&lt;Vec3b&gt;(row, col)[1] = saturate_cast&lt;uchar&gt;(g1 + g2); my_add_result.at&lt;Vec3b&gt;(row, col)[2] = saturate_cast&lt;uchar&gt;(r1 + r2); &#125; &#125; imshow("my_add_result", my_add_result); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425262728import cv2 as cvimport numpy as npsrc1 = cv.imread("../images/opencv_images/LinuxLogo.jpg");src2 = cv.imread("../images/opencv_images/WindowsLogo.jpg");cv.imshow("input1", src1)cv.imshow("input2", src2)h, w, ch = src1.shapeprint("h , w, ch", h, w, ch)add_result = np.zeros(src1.shape, src1.dtype);cv.add(src1, src2, add_result);cv.imshow("add_result", add_result);sub_result = np.zeros(src1.shape, src1.dtype);cv.subtract(src1, src2, sub_result);cv.imshow("sub_result", sub_result);mul_result = np.zeros(src1.shape, src1.dtype);cv.multiply(src1, src2, mul_result);cv.imshow("mul_result", mul_result);div_result = np.zeros(src1.shape, src1.dtype);cv.divide(src1, src2, div_result);cv.imshow("div_result", div_result);cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像像素算术操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用tensorflow对cifar10数据集进行图像分类]]></title>
    <url>%2F2019%2F03%2F22%2Fcifar10%2F</url>
    <content type="text"><![CDATA[步骤 定义神经网络计算图 运行计算图 导包12345import tensorflow as tfimport osimport cifar10_input # tensorflow/modle模块中自带案例，可以去github下载import numpy as npos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' 设置算法超参数1234567891011learning_rate_init = 0.001l2loss_ratio = 0.001keep_prob = 0.7 #dropouttraining_epochs = 5batch_size = 100display_step = 100conv1_kernel_num = 64conv2_kernel_num = 64fc1_units_num = 256fc2_units_num = 128fc3_units_num = cifar10_input.NUM_CLASSES 数据集中输入图像的参数123456dataset_dir = './cifar10_data/'image_size = cifar10_input.IMAGE_SIZEimage_channel = 3n_classes = cifar10_input.NUM_CLASSESnum_examples_per_epoch_for_train = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAINnum_examples_per_epoch_for_eval = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL 得到每一批次的训练数据123456def get_distorted_train_batch(data_dir, batch_size): if not data_dir: raise ValueError('please supply a data_dir') data_dir = os.path.join(data_dir, 'cifar-10-batches-bin') images, labels = cifar10_input.distorted_inputs(data_dir=data_dir, batch_size=batch_size) return images, labels 得到每一批次的测试数据123456def get_undistorted_eval_batch(data_dir, eval_data, batch_size): if not data_dir: raise ValueError('please supply a data_dir') data_dir = os.path.join(data_dir, 'cifar-10-batches-bin') images, labels = cifar10_input.inputs(eval_data=eval_data, data_dir=data_dir, batch_size=batch_size) return images, labels 根据指定的维数返回初始化好的指定名称的权重 Variable12345678def WeightsVariable(shape, name_str='weights', stddev=0.1): # 单cpu initial = tf.truncated_normal(shape=shape, stddev=stddev, dtype=tf.float32) return tf.Variable(initial, dtype=tf.float32, name=name_str) # 多gpu # weights = tf.get_variable(name_str, shape=shape, dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer_conv2d()) # return weights 根据指定的维数返回初始化好的指定名称的权重 Variable123def BiasesVariable(shape, name_str='biases', init_value=0.0): initial = tf.constant(init_value, shape=shape) return tf.Variable(initial, dtype=tf.float32, name=name_str) 2维卷积层的封装（包含激活函数）1234567def Conv2d(x, W, b, stride=1, padding='SAME', activation=tf.nn.relu, act_name='relu'): with tf.name_scope('conv2d_bias'): y = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=padding) y = tf.nn.bias_add(y, b) with tf.name_scope(act_name): y = activation(y) return y 2维池化层pool的封装12def Pool2d(x, pool=tf.nn.max_pool, k=2, stride=2, padding='SAME'): return pool(x, ksize=[1, k, k, 1], strides=[1, stride, stride, 1], padding=padding) 全连接层的封装1234567def FullyConnected(x, W, b, activation=tf.nn.relu, act_name='relu'): with tf.name_scope('Wx_b'): y = tf.matmul(x, W) y = tf.add(y, b) with tf.name_scope(act_name): y = activation(y) return y 为每一层的激活输出添加汇总节点123def AddActivationSummary(x): tf.summary.histogram('/activations', x) tf.summary.scalar('/sparsity', tf.nn.zero_fraction(x)) # 稀疏性 为所有损失节点添加标量汇总操作12345678910def AddLossesSummary(losses): # 计算所有损失的滑动平均 loss_averages = tf.train.ExponentialMovingAverage(decay=0.9, name='avg') loss_averages_op = loss_averages.apply(losses) # 为所有损失及平滑处理的损失绑定标量汇总节点 for loss in losses: tf.summary.scalar(loss.op.name + '(raw)', loss) tf.summary.scalar(loss.op.name + '(avg)', loss_averages.average(loss)) return loss_averages_op 打印每一层输出张量的shape12def print_layers_shape(t): print(t.op.name, ' ', t.get_shape().as_list()) 前向推断过程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263def Inference(images_holder): # 第一个卷积层 with tf.name_scope('Conv2d_1'): weights = WeightsVariable(shape=[5, 5, image_channel, conv1_kernel_num], stddev=5e-2) biases = BiasesVariable(shape=[conv1_kernel_num]) conv1_out = Conv2d(images_holder, weights, biases) AddActivationSummary(conv1_out) print_layers_shape(conv1_out) # 第一个池化层 with tf.name_scope('Pool2d_1'): pool1_out = Pool2d(conv1_out, k=3, stride=2) # 第二个卷积层 with tf.name_scope('Conv2d_2'): weights = WeightsVariable(shape=[5, 5, conv1_kernel_num, conv2_kernel_num], stddev=5e-2) biases = BiasesVariable(shape=[conv2_kernel_num]) conv2_out = Conv2d(pool1_out, weights, biases) AddActivationSummary(conv2_out) # 第二个池化层 with tf.name_scope('Pool2d_2'): pool2_out = Pool2d(conv2_out, k=3, stride=2) # 将二维特征图变为一维特征向量 with tf.name_scope('FeatsReshape'): features = tf.reshape(pool2_out, [batch_size, -1]) feats_dim = features.get_shape()[1].value # 得到上一行 -1 所指代的值 # 第一个全连接层 with tf.name_scope('FC1_nonlinear'): weights = WeightsVariable(shape=[feats_dim, fc1_units_num], stddev=4e-2) biases = BiasesVariable(shape=[fc1_units_num], init_value=0.1) fc1_out = FullyConnected(features, weights, biases) AddActivationSummary(fc1_out) # 加入L2损失 with tf.name_scope('L2_loss'): weight_loss = tf.multiply(tf.nn.l2_loss(weights), l2loss_ratio, name='fc1_weight_loss') tf.add_to_collection('losses', weight_loss) # Dropout # with tf.name_scope('dropout_1'): # fc1_dropout = tf.nn.dropout(fc1_out, keep_prob=keep_prob) # 第二个全连接层 with tf.name_scope('FC2_nonlinear'): weights = WeightsVariable(shape=[fc1_units_num, fc2_units_num], stddev=4e-2) biases = BiasesVariable(shape=[fc2_units_num], init_value=0.1) fc2_out = FullyConnected(fc1_out, weights, biases) AddActivationSummary(fc2_out) # 加入L2损失 with tf.name_scope('L2_loss'): weight_loss = tf.multiply(tf.nn.l2_loss(weights), l2loss_ratio, name='fc2_weight_loss') tf.add_to_collection('losses', weight_loss) # 第三个全连接层 with tf.name_scope('FC3_linear'): weights = WeightsVariable(shape=[fc2_units_num, fc3_units_num], stddev=1.0/fc2_units_num) biases = BiasesVariable(shape=[fc3_units_num]) logits = FullyConnected(fc2_out, weights, biases, activation=tf.identity, act_name='linear') AddActivationSummary(logits) return logits 调用上面写的函数构造计算图，并设计会话流程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111def TrainModel(): with tf.Graph().as_default(): # 计算图输入 with tf.name_scope('Inputs'): images_holder = tf.placeholder(tf.float32, [batch_size, image_size, image_size, image_channel], name='images') labels_holder = tf.placeholder(tf.int32, [batch_size], name='labels') # 计算图前向推断过程 with tf.name_scope('Inference'): logits = Inference(images_holder) # 定义损失层 with tf.name_scope('Loss'): cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_holder, logits=logits) cross_entropy_loss = tf.reduce_mean(cross_entropy, name='xentropy_loss') tf.add_to_collection('losses', cross_entropy_loss) # 总损失 = 交叉熵损失 + L2损失 total_loss = tf.add_n(tf.get_collection('losses'), name='total_loss') average_losses = AddLossesSummary(tf.get_collection('losses') + [total_loss]) # 定义优化训练层 with tf.name_scope('Train'): learning_rate = tf.placeholder(tf.float32) global_step = tf.Variable(0, name='global_step', trainable=False, dtype=tf.int64) optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate) train_op = optimizer.minimize(total_loss, global_step=global_step) # 定义模型评估层 with tf.name_scope('Evaluate'): top_K_op = tf.nn.in_top_k(predictions=logits, targets=labels_holder, k=1) # 定义获取训练样本批次的节点 with tf.name_scope('GetTrainBatch'): images_train, labels_train = get_distorted_train_batch(data_dir=dataset_dir, batch_size=batch_size) # 定义获取测试样本批次的节点 with tf.name_scope('GetTestBatch'): images_test, labels_test = get_undistorted_eval_batch(eval_data=True, data_dir=dataset_dir, batch_size=batch_size) # 收集所有汇总节点 merged_summaries = tf.summary.merge_all() # 添加所有变量的初始化节点 init_op = tf.global_variables_initializer() print("把计算图写入事件文件...") # graph_writer = tf.summary.FileWriter(logdir='events/', graph=tf.get_default_graph()) # graph_writer.close() summary_writer = tf.summary.FileWriter(logdir='events/') summary_writer.add_graph(graph=tf.get_default_graph()) summary_writer.flush() with tf.Session() as sess: sess.run(init_op) print('==&gt;&gt;&gt;&gt;&gt;&gt;&gt;==开始在训练集上训练模型==&lt;&lt;&lt;&lt;&lt;&lt;&lt;==') total_batches = int(num_examples_per_epoch_for_train / batch_size) print("per batch size: ", batch_size) print("train sample count per epoch:", num_examples_per_epoch_for_train) print("total batch count per epoch:", total_batches) # 启动数据读取队列 tf.train.start_queue_runners() # 记录模型被训练的步数 training_step = 0 # 训练指定轮数，每一轮的训练样本总数为：num_examples_per_epoch_for_train for epoch in range(training_epochs): # 每一轮都要把所有的batch跑一遍 for batch_idx in range(total_batches): # 运行获取批次训练数据的计算图，取出一个批次数据 images_batch, labels_batch = sess.run([images_train, labels_train]) # 运行优化器训练节点 _, loss_value, avg_losses= sess.run([train_op, total_loss, average_losses], feed_dict=&#123;images_holder:images_batch, labels_holder:labels_batch, learning_rate:learning_rate_init&#125;) # 每调用一次训练节点，training_step就加1，最终 == training_epochs * total_batch training_step = sess.run(global_step) # 每训练display_step次，计算当前模型的损失和分类准确率 if training_step % display_step == 0: # 运行Evaluate节点，计算当前批次的训练样本的准确率 predictions = sess.run([top_K_op], feed_dict=&#123;images_holder:images_batch, labels_holder:labels_batch&#125;) # 计算当前批次的预测正确样本量 batch_accuracy = np.sum(predictions) / batch_size print("train step: " + str(training_step) + ", train loss= " + "&#123;:.6f&#125;".format(loss_value) + ", train accuracy=" + "&#123;:.5f&#125;".format(batch_accuracy)) # 运行汇总节点 summaries_str = sess.run(merged_summaries, feed_dict= &#123;images_holder: images_batch, labels_holder: labels_batch&#125;) summary_writer.add_summary(summary=summaries_str, global_step=training_step) summary_writer.flush() summary_writer.close() print("训练完毕！") print('==&gt;&gt;&gt;&gt;&gt;&gt;&gt;==开始在测试集上评估模型==&lt;&lt;&lt;&lt;&lt;&lt;&lt;==') total_batches = int(num_examples_per_epoch_for_eval / batch_size) total_examples = total_batches * batch_size # 当除不尽batch_size时，num_examples_per_epoch_for_evalv ！= total_examples print("per batch size: ", batch_size) print("test sample count per epoch:", total_examples) print("total batch count per epoch:", total_batches) correc_predicted = 0 for test_step in range(total_batches): # 运行获取批次测试数据的计算图，取出一个批次数据 images_batch, labels_batch = sess.run([images_test, labels_test]) # 运行Evaluate节点，计算当前批次的训练样本的准确率 predictions = sess.run([top_K_op], feed_dict=&#123;images_holder:images_batch, labels_holder:labels_batch&#125;) # 累计每个批次的预测正确样本量 correc_predicted += np.sum(predictions) accuracy_score = correc_predicted / total_examples print("--------&gt;accuracy on test examples: ",accuracy_score) 123456def main(argv=None): train_dir = './events/' if tf.gfile.Exists(train_dir): tf.gfile.DeleteRecursively(train_dir) tf.gfile.MakeDirs(train_dir) TrainModel() 12if __name__ == '__main__': tf.app.run() 结果 训练结果 测试结果 Tensorboard 中查看 代码地址github中没有上传cifar10数据集，需要的话请从百度云下载，或自行下载，按照如下解压 github 百度云 提取码：xw3x]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>图像分类</tag>
        <tag>tensorflow</tag>
        <tag>cifar10</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-004-图像像素读写操作]]></title>
    <url>%2F2019%2F03%2F21%2Fopencv-004%2F</url>
    <content type="text"><![CDATA[知识点 C++中的像素遍历与访问 数组遍历 指针方式遍历 Python中的像素遍历与访问 数组遍历 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace std;using namespace cv;/** * 读取图像，实现像素反转 */int main() &#123; Mat src = imread("../images/liuyifei_1.png"); Mat src_copy = src.clone(); int height = src.rows; int width = src.cols; int ch = src.channels(); imshow("input", src); // 直接读取图像像素 for (int row = 0; row &lt; height; ++row) &#123; for (int col = 0; col &lt; width; ++col) &#123; if (ch == 3) &#123; Vec3b bgr = src.at&lt;Vec3b&gt;(row, col); bgr[0] = 255 - bgr[0]; bgr[1] = 255 - bgr[1]; bgr[2] = 255 - bgr[2]; src.at&lt;Vec3b&gt;(row, col) = bgr; &#125; else if (ch == 1) &#123; int gray = src.at&lt;uchar&gt;(row, col); src.at&lt;uchar&gt;(row, col) = 255 - gray; &#125; &#125; &#125; imshow("output1", src); // 指针读取 Mat result = Mat::zeros(src_copy.size(), src_copy.type()); int blue = 0, green = 0, red = 0; int gray; for (int row = 0; row &lt; height; ++row) &#123; // curr_row为第row行的首地址，遍历时，前三个字节表示的是第一个像素的BGR值， // 注意BGR值顺序，接下来三个字节是第二个像素的值。 uchar *curr_row = src_copy.ptr&lt;uchar&gt;(row); uchar *result_row = result.ptr&lt;uchar&gt;(row); for (int col = 0; col &lt; width; ++col) &#123; if (ch == 3) &#123; blue = *curr_row++; green = *curr_row++; red = *curr_row++; *result_row++ = 255 - blue; *result_row++ = 255 - green; *result_row++ = 255 - red; &#125; else if (ch == 1) &#123; gray = *curr_row++; *result_row++ = gray; &#125; &#125; &#125; imshow("output2", result); waitKey(0); return 0;&#125; 123456789101112131415161718import cv2 as cvsrc = cv.imread("../images/liuyifei_1.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)h, w, ch = src.shapeprint("h , w, ch", h, w, ch)for row in range(h): for col in range(w): b, g, r = src[row, col] b = 255 - b g = 255 - g r = 255 - r src[row, col] = [b, g, r]cv.imshow("output", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像像素读写</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-003-图像对象(Mat)创建与赋值]]></title>
    <url>%2F2019%2F03%2F21%2Fopencv-003%2F</url>
    <content type="text"><![CDATA[知识点 C++中Mat对象与创建 Python中Numpy数组对象 代码（c++,python）123456789101112131415161718192021222324252627282930#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace std;using namespace cv;int main()&#123; Mat src = imread("../images/liuyifei_1.png"); // 通过克隆或复制创建图像对象，m1和src指向不同内存块 Mat m1 = src.clone(); Mat m2; src.copyTo(m2); // 赋值法，m3和src指向同一内存块 Mat m3 = src; // 创建空白图像 Mat m4 = Mat::zeros(src.size(),src.type()); Mat m5 = Mat::zeros(Size(512,512),CV_8UC3); Mat m6 = Mat::ones(Size(512,512),CV_8UC3); // kernel: [0, -1, 0 // -1, 5, -1 // 0, -1, 0] Mat kernel = (Mat_&lt;char&gt;(3,3)&lt;&lt;0,-1,0,-1,5,-1,0,-1,0); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425262728import cv2 as cvimport numpy as npsrc = cv.imread("../images/liuyifei_1.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# 克隆图像m1 = np.copy(src)# 赋值m2 = srcsrc[100:200,200:300,:] = 255 # 第三维代表图像通道cv.imshow("m2",m2)m3 = np.zeros(src.shape, src.dtype)cv.imshow("m3", m3)m4 = np.zeros([512,512], np.uint8)# m4[:,:] =127 try to give gray value 127cv.imshow("m4", m4)m5 = np.ones(shape=[512,512,3], dtype=np.uint8)m5[:,:,0] = 255cv.imshow("m5", m5)cv.waitKey(0)cv.destroyAllWindows() 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>Mat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-002-色彩空间转换(cvtcolor)与图像保存(imwrite)]]></title>
    <url>%2F2019%2F03%2F20%2Fopencv-002%2F</url>
    <content type="text"><![CDATA[知识点 色彩空间转换函数- cvtColor COLOR_BGR2GRAY = 6 彩色到灰度 COLOR_GRAY2BGR = 8 灰度到彩色 COLOR_BGR2HSV = 40 BGR到HSV COLOR_HSV2BGR = 54 HSV到 BGR 图像保存 - imwrite 第一个参数是图像保存路径 第二个参数是图像内存对象 代码（c++,python）1234567891011121314151617181920212223242526#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace std;using namespace cv;int main()&#123; Mat src = imread("../images/liuyifei_1.png"); if (src.empty())&#123; cout &lt;&lt; "could not load image..." &lt;&lt; endl; return -1; &#125; namedWindow("input"); imshow("input",src); Mat dst; cvtColor(src,dst,COLOR_BGR2GRAY); imwrite("../images/result1.png",dst); namedWindow("output gray"); imshow("output gray",dst); waitKey(0); return 0;&#125; 123456789import cv2 as cvsrc = cv.imread("../images/liuyifei_1.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)cv.imshow("gray", gray)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>色彩空间转换(cvtcolor)</tag>
        <tag>图像保存(imwrite)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Clion无法读取相对路径文件或图像的解决方法]]></title>
    <url>%2F2019%2F03%2F20%2FClion_path_problem%2F</url>
    <content type="text"><![CDATA[项目目录 相对路径错误写法12// opencv读取图像，此时无法读取Mat image = imread("images/liuyifei_1.png") 解决方案 1 - 使用绝对路径1Mat image = imread("D:\\code-workspace\\Clion-workspace\\learnOpencv\\images\\liuyifei_1.png") 解决方案 2 - 返回根目录1Mat image = imread("../images/liuyifei_1.png") 解决方案 3 - 设置项目工作目录 设置项目工作目录 代码如下 12// 此时读取成功Mat image = imread("images/liuyifei_1.png")]]></content>
      <tags>
        <tag>Clion</tag>
        <tag>相对路径问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-001-读取(imread)与显示(imshow)图像]]></title>
    <url>%2F2019%2F03%2F20%2Fopencv-001%2F</url>
    <content type="text"><![CDATA[知识点 读取图像 - imread() 显示图像 - imshow() 代码（c++,python）123456789101112131415161718192021#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace std;using namespace cv;int main() &#123; // Mat image = imread("../images/liuyifei_1.png"); // 读取的时候加参数，使读取后为灰度图像 Mat image = imread("../images/liuyifei_1.png",IMREAD_GRAYSCALE); if (image.empty()) &#123; cout &lt;&lt; "could not load image..." &lt;&lt; endl; return -1; &#125; namedWindow("input"); imshow("input",image); waitKey(0); return 0;&#125; 1234567import cv2 as cvsrc = cv.imread("../images/liuyifei_1.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>读取并显示图像</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pip配置阿里云镜像]]></title>
    <url>%2F2019%2F03%2F19%2Fpip_windows_aliyun%2F</url>
    <content type="text"><![CDATA[windows新建pip配置文件夹 在windows “文件资源管理器” 地址栏输入%APPDATA% 按回车，创建pip文件夹，用于存放pip配置文件 在pip文件夹中新建名为：pip.ini 的配置文件 在pip.ini中输入以下内容 123[global]trusted-host = mirrors.aliyun.comindex-url = https://mirrors.aliyun.com/pypi/simple linux新建.pip文件夹 1mkdir .pip 新建pip.conf文件 12cd .piptouch pip.conf 在pip.conf中输入以下内容 1vim pip.conf 123[global]trusted-host = mirrors.aliyun.comindex-url = https://mirrors.aliyun.com/pypi/simple]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>pip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python安装opencv]]></title>
    <url>%2F2019%2F03%2F19%2Fopencv_python%2F</url>
    <content type="text"><![CDATA[安装opencv123456# opencv-python 和 opencv-contrib-python只能安装一个，后者带有扩展包，建议直接安后者pip install opencv-python# 安装opencv-contrib-python前，要先卸载opencv-pythonpip uninstall opencv-pythonpip install opencv-contrib-python 更新opencv1pip install --upgrade opencv-python]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VS2017配置opencv]]></title>
    <url>%2F2019%2F03%2F19%2Fopencv_vs2017%2F</url>
    <content type="text"><![CDATA[安装 opencv 下载地址 ：opencv download 解压到 opencv4文件夹中 解压后： 配置环境变量： VS2017中配置opencv 新建一个工程 依次点击：视图 ==&gt; 其他窗口 ==&gt; 属性管理器 添加包含目录 添加库目录 添加附加依赖项 重启VS2017 测试 测试代码 1234567891011121314#include &lt;opencv2\opencv.hpp&gt;using namespace cv;int main()&#123; Mat img = imread("1.png"); namedWindow("hahaha"); imshow("hahaha", img); waitKey(0); return 0;&#125; 测试结果]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>VS2017</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows下Clion配置opencv]]></title>
    <url>%2F2019%2F03%2F19%2Fopencv_Clion%2F</url>
    <content type="text"><![CDATA[所需环境MinGw + Cmake + Clion + opencv 安装MinGw参考：install MinGw 安装Cmake参考：install Cmake Cmake下载网址：Cmake download 注：Cmake最好安装跟Clion中配置一样的版本，省的麻烦 安装 opencv 下载地址 ：opencv download 解压到 opencv4文件夹中 解压后： 配置环境变量： Clion 配置 编译opencv源码 打开Cmake-GUI，选择源码路径和输出路径 点击Configure，选择MinGW Makefiles，点击Finish，开始编译 等待一段时间后，会有很多报红，再次点击Configure，红色消失，点击Generate 进入输出目录，在cmd 运行下面代码，等待完成 1mingw32-make -j8 运行mingw32-make install，等待片刻，输出目录下会多出install文件夹 添加…\install\x64\mingw\bin 添加到path系统环境变量环境变量 编辑CMakeLists.txt1234567891011121314151617cmake_minimum_required(VERSION 3.13)project(learnOpencv)set(CMAKE_CXX_STANDARD 11)# Where to find CMake modules and OpenCVset(OpenCV_DIR "D:\\software\\opencv4\\MinGW64_build\\install")set(CMAKE_MODULE_PATH $&#123;CMAKE_MODULE_PATH&#125; "$&#123;CMAKE_SOURCE_DIR&#125;/cmake/")find_package(OpenCV REQUIRED)include_directories($&#123;OpenCV_INCLUDE_DIRS&#125;)add_executable(learnOpencv test.cpp)# add libs you needset(OpenCV_LIBS opencv_core opencv_imgproc opencv_highgui opencv_imgcodecs)# linkingtarget_link_libraries(learnOpencv $&#123;OpenCV_LIBS&#125;) 注意：opencv4必须要c++11支持 测试12345678910111213#include &lt;opencv2\opencv.hpp&gt;using namespace cv;int main()&#123; Mat img = imread("D:\\code-workspace\\Clion-workspace\\learnOpencv\\images\\1.png",WINDOW_AUTOSIZE); namedWindow("刘亦菲"); imshow("刘亦菲", img); waitKey(0); return 0;&#125;]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>Clion</tag>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装jupyter notebook插件]]></title>
    <url>%2F2019%2F03%2F02%2FjupyterPlugin%2F</url>
    <content type="text"><![CDATA[步骤12python -m pip install jupyter_contrib_nbextensionsjupyter contrib nbextension install --user --skip-running-check Autopep8 –&gt; 格式化代码]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>jupyter notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用keras用常规神经网络训练MNIST数据集]]></title>
    <url>%2F2019%2F03%2F02%2FmnistLinearNN%2F</url>
    <content type="text"><![CDATA[加载mnist数据集123456from keras.datasets import mnist(x_train,y_train),(x_test,y_test) = mnist.load_data() # 将下载好的mnist.npz方在 ~/.keras/datasets/ 目录下print(x_train.shape,type(x_train))print(y_train.shape,type(y_train))print(x_test.shape,type(x_test))print(y_test.shape,type(y_test)) (60000, 28, 28) &lt;class &apos;numpy.ndarray&apos;&gt; (60000,) &lt;class &apos;numpy.ndarray&apos;&gt; (10000, 28, 28) &lt;class &apos;numpy.ndarray&apos;&gt; (10000,) &lt;class &apos;numpy.ndarray&apos;&gt; 数据处理：规范化1234# 将图形从[28,28]变为[784,]X_train = x_train.reshape(60000,784)X_test = x_test.reshape(10000,784)print(X_train.shape,X_test.shape) (60000, 784) (10000, 784) 123456# 将数据转换为float32，为了进行归一化，不然/255得到全部是0X_train = X_train.astype('float32')X_test = X_test.astype('float32')# 数据归一化X_train /= 255X_test /= 255 统计训练数据中个标签数量12345import numpy as npimport matplotlib.pyplot as pltlabel, count = np.unique(y_train, return_counts=True)print(label, count) [0 1 2 3 4 5 6 7 8 9] [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949] 123456789101112fig = plt.figure(figsize=(8, 5))plt.bar(label, count, width=0.7, align='center')plt.title("Label Distribution")plt.xlabel('Label')plt.ylabel('Count')plt.xticks(label)plt.ylim(0, 7500)for a, b in zip(label, count): plt.text(a, b, '%d' % b, ha='center', va='bottom', fontsize=10)plt.show() 对标签进行one-hot编码123456789101112131415161718192021# import tensorflow as tf# n_classes = 10# Y_train = tf.one_hot(y_train, n_classes)# Y_test = tf.one_hot(y_test, n_classes)# with tf.Session() as sess:# sess.run(tf.global_variables_initializer())# Y_train=sess.run(Y_train)# Y_test=sess.run(Y_test)# print(Y_train.shape)# 下面代码同上，使用tensorflow需要建立会话，简单转换keras更方便from keras.utils import np_utilsn_classes = 10Y_train = np_utils.to_categorical(y_train,n_classes)Y_test = np_utils.to_categorical(y_test,n_classes)print(Y_train.shape) (60000, 10) 12print(y_train[0])print(Y_train[0]) 5 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] 使用Keras sequential model 定义神经网络1234567891011121314# 使用keras定义线性网络很方便from keras.models import Sequentialfrom keras.layers.core import Dense, Activationmodel = Sequential()# 第一隐藏层model.add(Dense(512, input_shape=(784,)))model.add(Activation('relu'))# 第二隐藏层model.add(Dense(512))model.add(Activation('relu'))# 输出层model.add(Dense(10))model.add(Activation('softmax')) 编译模型1model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy']) 训练模型，并将指标保存到history中12history = model.fit(X_train, Y_train, batch_size=128, epochs=5, verbose=2, validation_data=(X_test, Y_test)) Train on 60000 samples, validate on 10000 samples Epoch 1/5 - 7s - loss: 0.2156 - acc: 0.9373 - val_loss: 0.0970 - val_acc: 0.9710 Epoch 2/5 - 7s - loss: 0.0804 - acc: 0.9758 - val_loss: 0.0769 - val_acc: 0.9770 Epoch 3/5 - 7s - loss: 0.0504 - acc: 0.9838 - val_loss: 0.0791 - val_acc: 0.9746 Epoch 4/5 - 7s - loss: 0.0350 - acc: 0.9891 - val_loss: 0.0659 - val_acc: 0.9804 Epoch 5/5 - 8s - loss: 0.0264 - acc: 0.9913 - val_loss: 0.0734 - val_acc: 0.9794 可视化指标12345678910111213141516171819fig = plt.figure()plt.subplot(211)plt.plot(history.history['acc'])plt.plot(history.history['val_acc'])plt.title('Model Accuracy')plt.xlabel('epoch')plt.ylabel('accuracy')plt.legend(['train','test'])plt.subplot(212)plt.plot(history.history['loss'])plt.plot(history.history['val_loss'])plt.title('Model Loss')plt.xlabel('epoch')plt.ylabel('loss')plt.legend(['train','test'])plt.tight_layout()plt.show() 保存模型123456789101112import osimport tensorflow.gfile as gfilesave_dir = '.\model'if gfile.Exists(save_dir): gfile.DeleteRecursively(save_dir)gfile.MakeDirs(save_dir)model_name = 'keras_mnist.h5'model_path = os.path.join(save_dir,model_name)model.save(model_path)print('Saved trained model at %s' % model_path) Saved trained model at .\model\keras_mnist.h5 加载模型123from keras.models import load_modelmnist_model = load_model(model_path) 统计模型在测试集上的分类结果123456789loss_and_metrics = mnist_model.evaluate(X_test, Y_test, verbose=2)print("Test Loss: &#123;&#125;".format(loss_and_metrics[0]))print("Test Accuracy: &#123;&#125;%".format(loss_and_metrics[1]*100))predicted_classes = mnist_model.predict_classes(X_test)correct_indices = np.nonzero(predicted_classes == y_test)[0]incorrect_indices = np.nonzero(predicted_classes != y_test)[0]print("Classified correctly count: &#123;&#125;".format(len(correct_indices)))print("Classified incorrectly count: &#123;&#125;".format(len(incorrect_indices))) Test Loss: 0.07340353026344673 Test Accuracy: 97.94% Classified correctly count: 9794 Classified incorrectly count: 206 代码地址github]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>mnist</tag>
        <tag>keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用keras用卷积神经网络训练MNIST数据集]]></title>
    <url>%2F2019%2F03%2F02%2FmnistCNN%2F</url>
    <content type="text"><![CDATA[加载mnist数据集123456from keras.datasets import mnist(x_train,y_train),(x_test,y_test) = mnist.load_data() # 将下载好的mnist.npz方在 ~/.keras/datasets/ 目录下print(x_train.shape,type(x_train))print(y_train.shape,type(y_train))print(x_test.shape,type(x_test))print(y_test.shape,type(y_test)) (60000, 28, 28) &lt;class &apos;numpy.ndarray&apos;&gt; (60000,) &lt;class &apos;numpy.ndarray&apos;&gt; (10000, 28, 28) &lt;class &apos;numpy.ndarray&apos;&gt; (10000,) &lt;class &apos;numpy.ndarray&apos;&gt; 数据处理：规范化channels_last对应的输入：(batch,height,width,channels) channels_first对应的输入：(batch,channels,height,width) 默认channels_last，修改：~/.keras/keras.json 123456789101112131415from keras import backend as Kimg_rows, img_cols = 28, 28if K.image_data_format() == 'channels_first': x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols) x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols) input_shape = (1, img_rows, img_cols)else: x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1) x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1) input_shape = (img_rows, img_cols, 1)print(x_train.shape, type(x_train))print(x_test.shape, type(x_test)) (60000, 28, 28, 1) &lt;class &apos;numpy.ndarray&apos;&gt; (10000, 28, 28, 1) &lt;class &apos;numpy.ndarray&apos;&gt; 123456# 将数据转换为float32，为了进行归一化，不然/255得到全部是0X_train = x_train.astype('float32')X_test = x_test.astype('float32')# 数据归一化X_train /= 255X_test /= 255 统计训练数据中个标签数量12345import numpy as npimport matplotlib.pyplot as pltlabel, count = np.unique(y_train, return_counts=True)print(label, count) [0 1 2 3 4 5 6 7 8 9] [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949] 123456789101112fig = plt.figure(figsize=(8, 5))plt.bar(label, count, width=0.7, align='center')plt.title("Label Distribution")plt.xlabel('Label')plt.ylabel('Count')plt.xticks(label)plt.ylim(0, 7500)for a, b in zip(label, count): plt.text(a, b, '%d' % b, ha='center', va='bottom', fontsize=10)plt.show() 对标签进行one-hot编码1234567from keras.utils import np_utilsn_classes = 10Y_train = np_utils.to_categorical(y_train,n_classes)Y_test = np_utils.to_categorical(y_test,n_classes)print(Y_train.shape) (60000, 10) 12print(y_train[0])print(Y_train[0]) 5 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] 使用Keras sequential model 定义MNIST CNN网络123456789101112131415161718192021222324from keras.models import Sequentialfrom keras.layers import Dense, Dropout, Flattenfrom keras.layers import Conv2D, MaxPooling2Dmodel = Sequential()## Feature Extraction# 第一层卷积，32个3*3的卷积核，激活函数使用relumodel.add(Conv2D(filters=32,kernel_size=(3,3),activation='relu',input_shape=input_shape))# 第二层卷积，64个3*3的卷积核，激活函数使用relumodel.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu'))# 最大池化层model.add(MaxPooling2D(pool_size=(2,2)))# Dropout 25% 的输入神经元model.add(Dropout(0.25))# 将Pooled feature map 摊平后输入全连接网络model.add(Flatten())## Classification# 全连接层model.add(Dense(128,activation='relu'))# Dropout 50% 的输入神经元model.add(Dropout(0.5))# 使用softmax 激活函数做多分类，输出各数字的概率model.add(Dense(10, activation='softmax')) 查看 MNIST CNN 模型网络结构1model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 26, 26, 32) 320 _________________________________________________________________ conv2d_2 (Conv2D) (None, 24, 24, 64) 18496 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 12, 12, 64) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 9216) 0 _________________________________________________________________ dense_1 (Dense) (None, 128) 1179776 _________________________________________________________________ dropout_2 (Dropout) (None, 128) 0 _________________________________________________________________ dense_2 (Dense) (None, 10) 1290 ================================================================= Total params: 1,199,882 Trainable params: 1,199,882 Non-trainable params: 0 _________________________________________________________________ 12for layer in model.layers: print(layer.get_output_at(0).get_shape().as_list()) [None, 26, 26, 32] [None, 24, 24, 64] [None, 12, 12, 64] [None, 12, 12, 64] [None, None] [None, 128] [None, 128] [None, 10] 编译模型1model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy']) 训练模型，并将指标保存到history中1history = model.fit(X_train, Y_train, batch_size=128, epochs=5,verbose=2, validation_data=(X_test, Y_test)) Train on 60000 samples, validate on 10000 samples Epoch 1/5 - 131s - loss: 0.2330 - acc: 0.9290 - val_loss: 0.0540 - val_acc: 0.9817 Epoch 2/5 - 146s - loss: 0.0853 - acc: 0.9747 - val_loss: 0.0372 - val_acc: 0.9882 Epoch 3/5 - 136s - loss: 0.0605 - acc: 0.9812 - val_loss: 0.0315 - val_acc: 0.9898 Epoch 4/5 - 129s - loss: 0.0514 - acc: 0.9843 - val_loss: 0.0283 - val_acc: 0.9913 Epoch 5/5 - 130s - loss: 0.0416 - acc: 0.9873 - val_loss: 0.0272 - val_acc: 0.9911 可视化指标12345678910111213141516171819fig = plt.figure()plt.subplot(211)plt.plot(history.history['acc'])plt.plot(history.history['val_acc'])plt.title('Model Accuracy')plt.xlabel('epoch')plt.ylabel('accuracy')plt.legend(['train','test'])plt.subplot(212)plt.plot(history.history['loss'])plt.plot(history.history['val_loss'])plt.title('Model Loss')plt.xlabel('epoch')plt.ylabel('loss')plt.legend(['train','test'])plt.tight_layout()plt.show() 保存模型123456789101112import osimport tensorflow.gfile as gfilesave_dir = '.\model'if gfile.Exists(save_dir): gfile.DeleteRecursively(save_dir)gfile.MakeDirs(save_dir)model_name = 'keras_mnist.h5'model_path = os.path.join(save_dir,model_name)model.save(model_path)print('Saved trained model at %s' % model_path) Saved trained model at .\model\keras_mnist.h5 加载模型123from keras.models import load_modelmnist_model = load_model(model_path) 统计模型在测试集上的分类结果123456789loss_and_metrics = mnist_model.evaluate(X_test, Y_test, verbose=2)print("Test Loss: &#123;&#125;".format(loss_and_metrics[0]))print("Test Accuracy: &#123;&#125;%".format(loss_and_metrics[1]*100))predicted_classes = mnist_model.predict_classes(X_test)correct_indices = np.nonzero(predicted_classes == y_test)[0]incorrect_indices = np.nonzero(predicted_classes != y_test)[0]print("Classified correctly count: &#123;&#125;".format(len(correct_indices)))print("Classified incorrectly count: &#123;&#125;".format(len(incorrect_indices))) Test Loss: 0.027159390095694836 Test Accuracy: 99.11% Classified correctly count: 9911 Classified incorrectly count: 89 代码地址github]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>mnist</tag>
        <tag>keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv+tensorflow实时检测人脸]]></title>
    <url>%2F2018%2F12%2F29%2FfaceDetection%2F</url>
    <content type="text"><![CDATA[关于人脸检测的说明本文代码使用了opencv自带的人脸检测算法和mtcnn算法，mtcnn有明显的优势，检测成功率基本维持在100%，而且人脸各角度都可以检测成功，所以建议使用mtcnn来进行人脸检测，电脑cpu也可以流畅运行。 需要提前配置的环境：python + opencv + tensorflow 关于mtcnn的介绍，请参见压缩包中的电子书 代码结构说明 detect_face.py定义了mtcnn模型 det 1-3.npy是预训练好的模型，所以不用再对mtcnn进行训练 detect 1-3.py是三种实现方式，下面一一介绍 代码演示detect1.py使用mtcnn对一张图片进行检测，效果如下： detect2.py使用opencv自带的HAAR进行实时人脸检测，当人脸倾斜时无法检测到，效果如下： detect3.py使用MTCNN进行实时人脸检测，无论人脸各个角度，都可以检测到，效果如下： 代码地址github地址 百度云地址 注意：github地址中没有mtcnn的预训练模型，需要自己下载，百度云是完整的]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>人脸检测</tag>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下关于screen命令的使用]]></title>
    <url>%2F2018%2F12%2F08%2Flinux-screen%2F</url>
    <content type="text"><![CDATA[因为进入服务器只有一个窗口，当我们用这个窗口跑代码时，就没有办法同时用命令编辑一些文件。为了解决这个问题，我们可以使用screen开启多个进程，用一个进程跑代码，然后将这个窗口折叠到后台，创建新的进程来编辑代码。 当我们想要断开服务器连接仍然让一些程序运行的时候，可以使用screen让程序在后台一直运行。 安装screen (ubuntu系统)1sudo apt-get install screen 创建进程1screen -S 进程名 之后，会进入一个干净的窗口，可以执行相应操作，连续按Ctrl+A、Ctrl+D回到主线程，之前执行的操作会一直在后台运行，直到杀死该进程。 这条命令可以多次使用，创建多个进程。 查看当前screen进程1screen -ls 进入某一进程123#两条命令选其一screen -r 进程名screen -r 进程pid号 终止进程12345#方法一screen -X -S 进程名 quit#方法二先进入要杀死的进程，然后输入exit]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux下screen的使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下查看端口占用进程及杀死进程]]></title>
    <url>%2F2018%2F12%2F08%2Flinux-kill-process%2F</url>
    <content type="text"><![CDATA[直接查看进程1ps 通过端口查看进程1lsof –i:端口号 杀死进程1kill -9 pid号]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>查看linux进程并杀死</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux添加用户并赋予sudo权限]]></title>
    <url>%2F2018%2F11%2F29%2FaddLinuxUser%2F</url>
    <content type="text"><![CDATA[创建用户123# 在root用户下不用写sudosudo adduser fanfan # 在/home 下会自动创建同名文件夹passwd fanfan # 设置密码，上个命令有时会直接让输入密码，就不需要执行这一步了 删除用户1sudo userdel fanfan 添加sudo权限 su -切换到root vim /etc/sudoers ，在root ALL=(ALL) ALL的下一行添加： 12345# sudo时需要输入密码fanfan ALL=(ALL) ALL# sudo时不需要输入密码fanfan ALL=(ALL) NOPASSWD: ALL 按Esc，再输入:wq!保存文件，要加!，不然保存会出问题]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux添加用户</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[本地访问服务器端jupyter notebook]]></title>
    <url>%2F2018%2F11%2F29%2FremoteJupyter%2F</url>
    <content type="text"><![CDATA[1 登陆远程服务器2 生成配置文件1$ jupyter notebook --generate-config 3 生成密码打开ipython，创建一个密文的密码12345In [1]: from notebook.auth import passwdIn [2]: passwd()Enter password: Verify password: Out[2]: 'sha1:ce23d945972f:34769685a7ccd3d08c84a18c63968a41f1140274' 把生成的密文‘sha:ce…’复制下来 4 修改默认配置文件1$ vim ~/.jupyter/jupyter_notebook_config.py 进行如下修改：1234c.NotebookApp.ip='*'c.NotebookApp.password = u'sha:ce...刚才复制的那个密文'c.NotebookApp.open_browser = Falsec.NotebookApp.port =8888 #随便指定一个端口 5 启动jupyter notebook1$ jupyter notebook 6 远程访问此时应该可以直接从本地浏览器直接访问http://address_of_remote:8888就可以看到jupyter的登陆界面，输入第三步中设置的密码。 7 建立SSH通道如果登陆失败，则有可能是服务器防火墙设置的问题，此时最简单的方法是在本地建立一个ssh通道：在本地终端中输入：12ssh fanfan@222.92.146.251 -L127.0.0.1:1234:127.0.0.1:6666ssh fanfan@47.106.208.254 -L127.0.0.1:1234:127.0.0.1:8888 便可以在localhost:1234直接访问远程的jupyter了。]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>jupyter notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器端安装Anaconda]]></title>
    <url>%2F2018%2F11%2F29%2FinstallAnaconda%2F</url>
    <content type="text"><![CDATA[步骤打开网址：Anaconda清华镜像，复制要下载的文件地址，执行以下命令：12345678910wget 复制的网址（会下载一个sh文件）sh sh文件名 #执行后，会显示使用条款，按enter继续阅读，会让回答几个问题，全部yesrm -rf sh文件名source ~/.bashrc （使conda生效）#设置清华conda镜像conda config --prepend channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ 注意事项 Anaconda3-5.2.0.Linux-x86_64.sh ==&gt; python3.6 Anaconda3-5.3.1.Linux-x86_64.sh ==&gt; python3.7 若wget显示网络不可达，执行以下操作： 123456#centossudo yum -y install wget#ubuntusudo apt-get updatesudo apt-get install wget 若不能运行jupyter notebook，进行如下配置：jupyter notebook配置]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>Anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux终端bash美化教程]]></title>
    <url>%2F2018%2F11%2F29%2FbeautifyBash%2F</url>
    <content type="text"><![CDATA[美化步骤12345vim .bashrc添加下行export PS1="Time:\[\033[1;35m\]\T \[\033[0m\]User:\[\033[1;33m\]\u \[\033[0m\]Dir:\[\033[1;32m\]\w\[\033[0m\]\n\$"退出vimsource .bashrc 美化效果 PS1中参数的具体含义参考链接]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo写博客步骤]]></title>
    <url>%2F2018%2F11%2F29%2FwriteArtical%2F</url>
    <content type="text"><![CDATA[博客编写步骤1 进入D:\Blog文件夹下，打开终端 2 输入：hexo new &quot;文件名&quot;，在D:\Blog\source\\_posts目录下创建了文件名.md文件 3 打开文件名.md，编写博客 4 终端输入：hexo d -g提交博客 md文件编写注意事项1234567---title: 博客名categories: 分类名tags: - 标签1 - 标签2--- 更新博客分类与标签页面12hexo cleanhexo d -g]]></content>
      <tags>
        <tag>Hexo发送文章</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简易安卓聊天软件之思路架构及源码]]></title>
    <url>%2F2018%2F11%2F04%2FweChat%2F</url>
    <content type="text"><![CDATA[安卓聊天软件完成的功能罗列1 登陆2 动态显示好友列表3 服务端程序4 客户端程序5 安全退出6 与多好友聊天，屏幕切换，可以保存原信息，每次新登陆，可以读取历史记录7 缓存消息，及离线完成 软件架构图 本地Sqlite数据库设计只有一张表，存储聊天消息，表中有三个属性，分别为：发送者 接收者 消息内容 客户端与服务端传输消息协议约定： 客户端新上线的时候，向服务端发送用户名，服务端向客户端发送好友列表与离线消息 客户端 ==&gt; 服务端：发送者：接收者：消息 服务端 ==&gt; 客户端：发送者：接收者：消息 服务端向客户端发送的是消息还是好友列表，以开头是否是”&amp;”符号区分 客户端目录结构（Android Studio） 客户端的基本思路Service负责与服务器进行网络连接与IO读写，无论是发送消息还是接受消息，Service都先把消息存到本地数据库，FriendListActivity与ChatActivity中ListView的显示，都是直接从数据库读取数据。Service与Activity的通信主要使用Intent和广播来进行。 服务端程序服务端基本思路（具体代码见文末源码地址）： 使用一个List存储所有好友 使用一个Map存储在线好友及对应Socket 使用一个Map存储离线消息 软件开发经验总结这次软件开发是以小组形式进行的，最后算是完成了聊天软件的基本功能，这次开发做的好的地方在于一开始小组就先把真个架构图设计好了，包括数据库，后面写代码基本很顺畅，得到的经验就是开发一个软件，做一个项目，写代码真的是很靠后的事情了，前期一定是先通过写用例，梳理好逻辑，画好架构图，后期按照梳理好的逻辑来写代码。后期还需要努力的地方在于UML类图，希望下次开发前期能把UML类图画出来，这样前期工作会更完善，加油，希望可以成为一个专业的程序员。 源码地址源码：github地址 注意：源码中的ImServeFinal.java文件时服务端程序，应该拿出来用java的IDE运行，记得更改ip与端口]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>java</tag>
      </tags>
  </entry>
</search>
