<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[使用tensorflow对cifar10数据集进行图像分类]]></title>
    <url>%2F2019%2F03%2F22%2Fcifar10%2F</url>
    <content type="text"><![CDATA[神经网络结构 导包12345import tensorflow as tfimport osimport cifar10_inputimport numpy as npos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' 设置算法超参数12345678910learning_rate_init = 0.001l2loss_ratio = 0.001training_epochs = 5batch_size = 100display_step = 100conv1_kernel_num = 64conv2_kernel_num = 64fc1_units_num = 256fc2_units_num = 128fc3_units_num = cifar10_input.NUM_CLASSES 数据集中输入图像的参数123456dataset_dir = './cifar10_data/'image_size = cifar10_input.IMAGE_SIZEimage_channel = 3n_classes = cifar10_input.NUM_CLASSESnum_examples_per_epoch_for_train = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAINnum_examples_per_epoch_for_eval = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL 得到每一批次的训练数据123456def get_distorted_train_batch(data_dir, batch_size): if not data_dir: raise ValueError('please supply a data_dir') data_dir = os.path.join(data_dir, 'cifar-10-batches-bin') images, labels = cifar10_input.distorted_inputs(data_dir=data_dir, batch_size=batch_size) return images, labels 得到每一批次的测试数据123456def get_undistorted_eval_batch(data_dir, eval_data, batch_size): if not data_dir: raise ValueError('please supply a data_dir') data_dir = os.path.join(data_dir, 'cifar-10-batches-bin') images, labels = cifar10_input.inputs(eval_data=eval_data, data_dir=data_dir, batch_size=batch_size) return images, labels 根据指定的维数返回初始化好的指定名称的权重 Variable123def WeightsVariable(shape, name_str='weights', stddev=0.1): initial = tf.truncated_normal(shape=shape, stddev=stddev, dtype=tf.float32) return tf.Variable(initial, dtype=tf.float32, name=name_str) 根据指定的维数返回初始化好的指定名称的权重 Variable123def BiasesVariable(shape, name_str='biases', init_value=0.0): initial = tf.constant(init_value, shape=shape) return tf.Variable(initial, dtype=tf.float32, name=name_str) 2维卷积层的封装（包含激活函数）1234567def Conv2d(x, W, b, stride=1, padding='SAME', activation=tf.nn.relu, act_name='relu'): with tf.name_scope('conv2d_bias'): y = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=padding) y = tf.nn.bias_add(y, b) with tf.name_scope(act_name): y = activation(y) return y 2维池化层pool的封装12def Pool2d(x, pool=tf.nn.max_pool, k=2, stride=2, padding='SAME'): return pool(x, ksize=[1, k, k, 1], strides=[1, stride, stride, 1], padding=padding) 全连接层的封装1234567def FullyConnected(x, W, b, activation=tf.nn.relu, act_name='relu'): with tf.name_scope('Wx_b'): y = tf.matmul(x, W) y = tf.add(y, b) with tf.name_scope(act_name): y = activation(y) return y 为每一层的激活输出添加汇总节点123def AddActivationSummary(x): tf.summary.histogram('/activations', x) tf.summary.scalar('/sparsity', tf.nn.zero_fraction(x)) # 稀疏性 为所有损失节点添加标量汇总操作12345678910def AddLossesSummary(losses): # 计算所有损失的滑动平均 loss_averages = tf.train.ExponentialMovingAverage(decay=0.9, name='avg') loss_averages_op = loss_averages.apply(losses) # 为所有损失及平滑处理的损失绑定标量汇总节点 for loss in losses: tf.summary.scalar(loss.op.name + '(raw)', loss) tf.summary.scalar(loss.op.name + '(avg)', loss_averages.average(loss)) return loss_averages_op 前向推断过程12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758def Inference(images_holder): # 第一个卷积层 with tf.name_scope('Conv2d_1'): weights = WeightsVariable(shape=[5, 5, image_channel, conv1_kernel_num], stddev=5e-2) biases = BiasesVariable(shape=[conv1_kernel_num]) conv1_out = Conv2d(images_holder, weights, biases) AddActivationSummary(conv1_out) # 第一个池化层 with tf.name_scope('Pool2d_1'): pool1_out = Pool2d(conv1_out, k=3, stride=2) # 第二个卷积层 with tf.name_scope('Conv2d_2'): weights = WeightsVariable(shape=[5, 5, conv1_kernel_num, conv2_kernel_num], stddev=5e-2) biases = BiasesVariable(shape=[conv2_kernel_num]) conv2_out = Conv2d(pool1_out, weights, biases) AddActivationSummary(conv2_out) # 第二个池化层 with tf.name_scope('Pool2d_2'): pool2_out = Pool2d(conv2_out, k=3, stride=2) # 将二维特征图变为一维特征向量 with tf.name_scope('FeatsReshape'): features = tf.reshape(pool2_out, [batch_size, -1]) feats_dim = features.get_shape()[1].value # 得到上一行 -1 所指代的值 # 第一个全连接层 with tf.name_scope('FC1_nonlinear'): weights = WeightsVariable(shape=[feats_dim, fc1_units_num], stddev=4e-2) biases = BiasesVariable(shape=[fc1_units_num], init_value=0.1) fc1_out = FullyConnected(features, weights, biases) AddActivationSummary(fc1_out) # 加入L2损失 with tf.name_scope('L2_loss'): weight_loss = tf.multiply(tf.nn.l2_loss(weights), l2loss_ratio, name='fc1_weight_loss') tf.add_to_collection('losses', weight_loss) # 第二个全连接层 with tf.name_scope('FC2_nonlinear'): weights = WeightsVariable(shape=[fc1_units_num, fc2_units_num], stddev=4e-2) biases = BiasesVariable(shape=[fc2_units_num], init_value=0.1) fc2_out = FullyConnected(fc1_out, weights, biases) AddActivationSummary(fc2_out) # 加入L2损失 with tf.name_scope('L2_loss'): weight_loss = tf.multiply(tf.nn.l2_loss(weights), l2loss_ratio, name='fc2_weight_loss') tf.add_to_collection('losses', weight_loss) # 第三个全连接层 with tf.name_scope('FC3_linear'): weights = WeightsVariable(shape=[fc2_units_num, fc3_units_num], stddev=1.0/fc2_units_num) biases = BiasesVariable(shape=[fc3_units_num]) logits = FullyConnected(fc2_out, weights, biases, activation=tf.identity, act_name='linear') AddActivationSummary(logits) return logits 调用上面写的函数构造计算图，并设计会话流程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111def TrainModel(): with tf.Graph().as_default(): # 计算图输入 with tf.name_scope('Inputs'): images_holder = tf.placeholder(tf.float32, [batch_size, image_size, image_size, image_channel], name='images') labels_holder = tf.placeholder(tf.int32, [batch_size], name='labels') # 计算图前向推断过程 with tf.name_scope('Inference'): logits = Inference(images_holder) # 定义损失层 with tf.name_scope('Loss'): cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_holder, logits=logits) cross_entropy_loss = tf.reduce_mean(cross_entropy, name='xentropy_loss') tf.add_to_collection('losses', cross_entropy_loss) # 总损失 = 交叉熵损失 + L2损失 total_loss = tf.add_n(tf.get_collection('losses'), name='total_loss') average_losses = AddLossesSummary(tf.get_collection('losses') + [total_loss]) # 定义优化训练层 with tf.name_scope('Train'): learning_rate = tf.placeholder(tf.float32) global_step = tf.Variable(0, name='global_step', trainable=False, dtype=tf.int64) optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate) train_op = optimizer.minimize(total_loss, global_step=global_step) # 定义模型评估层 with tf.name_scope('Evaluate'): top_K_op = tf.nn.in_top_k(predictions=logits, targets=labels_holder, k=1) # 定义获取训练样本批次的节点 with tf.name_scope('GetTrainBatch'): images_train, labels_train = get_distorted_train_batch(data_dir=dataset_dir, batch_size=batch_size) # 定义获取测试样本批次的节点 with tf.name_scope('GetTestBatch'): images_test, labels_test = get_undistorted_eval_batch(eval_data=True, data_dir=dataset_dir, batch_size=batch_size) # 收集所有汇总节点 merged_summaries = tf.summary.merge_all() # 添加所有变量的初始化节点 init_op = tf.global_variables_initializer() print("把计算图写入事件文件...") # graph_writer = tf.summary.FileWriter(logdir='events/', graph=tf.get_default_graph()) # graph_writer.close() summary_writer = tf.summary.FileWriter(logdir='events/') summary_writer.add_graph(graph=tf.get_default_graph()) summary_writer.flush() with tf.Session() as sess: sess.run(init_op) print('==&gt;&gt;&gt;&gt;&gt;&gt;&gt;==开始在训练集上训练模型==&lt;&lt;&lt;&lt;&lt;&lt;&lt;==') total_batches = int(num_examples_per_epoch_for_train / batch_size) print("per batch size: ", batch_size) print("train sample count per epoch:", num_examples_per_epoch_for_train) print("total batch count per epoch:", total_batches) # 启动数据读取队列 tf.train.start_queue_runners() # 记录模型被训练的步数 training_step = 0 # 训练指定轮数，每一轮的训练样本总数为：num_examples_per_epoch_for_train for epoch in range(training_epochs): # 每一轮都要把所有的batch跑一遍 for batch_idx in range(total_batches): # 运行获取批次训练数据的计算图，取出一个批次数据 images_batch, labels_batch = sess.run([images_train, labels_train]) # 运行优化器训练节点 _, loss_value, avg_losses= sess.run([train_op, total_loss, average_losses], feed_dict=&#123;images_holder:images_batch, labels_holder:labels_batch, learning_rate:learning_rate_init&#125;) # 每调用一次训练节点，training_step就加1，最终 == training_epochs * total_batch training_step = sess.run(global_step) # 每训练display_step次，计算当前模型的损失和分类准确率 if training_step % display_step == 0: # 运行Evaluate节点，计算当前批次的训练样本的准确率 predictions = sess.run([top_K_op], feed_dict=&#123;images_holder:images_batch, labels_holder:labels_batch&#125;) # 计算当前批次的预测正确样本量 batch_accuracy = np.sum(predictions) / batch_size print("train step: " + str(training_step) + ", train loss= " + "&#123;:.6f&#125;".format(loss_value) + ", train accuracy=" + "&#123;:.5f&#125;".format(batch_accuracy)) # 运行汇总节点 summaries_str = sess.run(merged_summaries, feed_dict= &#123;images_holder: images_batch, labels_holder: labels_batch&#125;) summary_writer.add_summary(summary=summaries_str, global_step=training_step) summary_writer.flush() summary_writer.close() print("训练完毕！") print('==&gt;&gt;&gt;&gt;&gt;&gt;&gt;==开始在测试集上评估模型==&lt;&lt;&lt;&lt;&lt;&lt;&lt;==') total_batches = int(num_examples_per_epoch_for_eval / batch_size) total_examples = total_batches * batch_size # 当除不尽batch_size时，num_examples_per_epoch_for_evalv ！= total_examples print("per batch size: ", batch_size) print("test sample count per epoch:", total_examples) print("total batch count per epoch:", total_batches) correc_predicted = 0 for test_step in range(total_batches): # 运行获取批次测试数据的计算图，取出一个批次数据 images_batch, labels_batch = sess.run([images_test, labels_test]) # 运行Evaluate节点，计算当前批次的训练样本的准确率 predictions = sess.run([top_K_op], feed_dict=&#123;images_holder:images_batch, labels_holder:labels_batch&#125;) # 累计每个批次的预测正确样本量 correc_predicted += np.sum(predictions) accuracy_score = correc_predicted / total_examples print("--------&gt;accuracy on test examples: ",accuracy_score) 123456def main(argv=None): train_dir = './events/' if tf.gfile.Exists(train_dir): tf.gfile.DeleteRecursively(train_dir) tf.gfile.MakeDirs(train_dir) TrainModel() 12if __name__ == '__main__': tf.app.run() 结果 训练结果 测试结果 Tensorboard 中查看 代码地址github中没有上传cifar10数据集，需要的话请从百度云下载，或自行下载，按照如下解压 github 百度云 提取码：xw3x]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>图像分类</tag>
        <tag>tensorflow</tag>
        <tag>cifar10</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Clion无法读取相对路径文件或图像的解决方法]]></title>
    <url>%2F2019%2F03%2F20%2FClion_path_problem%2F</url>
    <content type="text"><![CDATA[项目目录 相对路径错误写法12// opencv读取图像，此时无法读取Mat image = imread("images/liuyifei_1.png") 解决方案 1 - 使用绝对路径1Mat image = imread("D:\\code-workspace\\Clion-workspace\\learnOpencv\\images\\liuyifei_1.png") 解决方案 2 - 返回根目录1Mat image = imread("../images/liuyifei_1.png") 解决方案 3 - 设置项目工作目录 设置项目工作目录 代码如下 12// 此时读取成功Mat image = imread("images/liuyifei_1.png")]]></content>
      <tags>
        <tag>Clion</tag>
        <tag>相对路径问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pip配置阿里云镜像]]></title>
    <url>%2F2019%2F03%2F19%2Fpip_windows_aliyun%2F</url>
    <content type="text"><![CDATA[windows新建pip配置文件夹 在windows “文件资源管理器” 地址栏输入%APPDATA% 按回车，创建pip文件夹，用于存放pip配置文件 在pip文件夹中新建名为：pip.ini 的配置文件 在pip.ini中输入以下内容 123[global]trusted-host = mirrors.aliyun.comindex-url = https://mirrors.aliyun.com/pypi/simple linux新建.pip文件夹 1mkdir .pip 新建pip.conf文件 12cd .piptouch pip.conf 在pip.conf中输入以下内容 1vim pip.conf 123[global]trusted-host = mirrors.aliyun.comindex-url = https://mirrors.aliyun.com/pypi/simple]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>pip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VS2017配置opencv]]></title>
    <url>%2F2019%2F03%2F19%2Fopencv_vs2017%2F</url>
    <content type="text"><![CDATA[安装 opencv 下载地址 ：opencv download 解压到 opencv4文件夹中 解压后： 配置环境变量： VS2017中配置opencv 新建一个工程 依次点击：视图 ==&gt; 其他窗口 ==&gt; 属性管理器 添加包含目录 添加库目录 添加附加依赖项 重启VS2017 测试 测试代码 1234567891011121314#include &lt;opencv2\opencv.hpp&gt;using namespace cv;int main()&#123; Mat img = imread("1.png"); namedWindow("hahaha"); imshow("hahaha", img); waitKey(0); return 0;&#125; 测试结果]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>VS2017</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows下Clion配置opencv]]></title>
    <url>%2F2019%2F03%2F19%2Fopencv_Clion%2F</url>
    <content type="text"><![CDATA[所需环境MinGw + Cmake + Clion + opencv 安装MinGw参考：install MinGw 安装Cmake参考：install Cmake Cmake下载网址：Cmake download 注：Cmake最好安装跟Clion中配置一样的版本，省的麻烦 安装 opencv 下载地址 ：opencv download 解压到 opencv4文件夹中 解压后： 配置环境变量： Clion 配置 编译opencv源码 打开Cmake-GUI，选择源码路径和输出路径 点击Configure，选择MinGW Makefiles，点击Finish，开始编译 等待一段时间后，会有很多报红，再次点击Configure，红色消失，点击Generate 进入输出目录，在cmd 运行下面代码，等待完成 1mingw32-make -j8 运行mingw32-make install，等待片刻，输出目录下会多出install文件夹 添加…\install\x64\mingw\bin 添加到path系统环境变量环境变量 编辑CMakeLists.txt1234567891011121314151617cmake_minimum_required(VERSION 3.13)project(learnOpencv)set(CMAKE_CXX_STANDARD 11)# Where to find CMake modules and OpenCVset(OpenCV_DIR "D:\\software\\opencv4\\MinGW64_build\\install")set(CMAKE_MODULE_PATH $&#123;CMAKE_MODULE_PATH&#125; "$&#123;CMAKE_SOURCE_DIR&#125;/cmake/")find_package(OpenCV REQUIRED)include_directories($&#123;OpenCV_INCLUDE_DIRS&#125;)add_executable(learnOpencv test.cpp)# add libs you needset(OpenCV_LIBS opencv_core opencv_imgproc opencv_highgui opencv_imgcodecs)# linkingtarget_link_libraries(learnOpencv $&#123;OpenCV_LIBS&#125;) 注意：opencv4必须要c++11支持 测试12345678910111213#include &lt;opencv2\opencv.hpp&gt;using namespace cv;int main()&#123; Mat img = imread("D:\\code-workspace\\Clion-workspace\\learnOpencv\\images\\1.png",WINDOW_AUTOSIZE); namedWindow("刘亦菲"); imshow("刘亦菲", img); waitKey(0); return 0;&#125;]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>Clion</tag>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python安装opencv]]></title>
    <url>%2F2019%2F03%2F19%2Fopencv_python%2F</url>
    <content type="text"><![CDATA[安装opencv123456# opencv-python 和 opencv-contrib-python只能安装一个，后者带有扩展包，建议直接安后者pip install opencv-python# 安装opencv-contrib-python前，要先卸载opencv-pythonpip uninstall opencv-pythonpip install opencv-contrib-python 更新opencv1pip install --upgrade opencv-python]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装jupyter notebook插件]]></title>
    <url>%2F2019%2F03%2F02%2FjupyterPlugin%2F</url>
    <content type="text"><![CDATA[步骤12python -m pip install jupyter_contrib_nbextensionsjupyter contrib nbextension install --user --skip-running-check Autopep8 –&gt; 格式化代码]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>jupyter notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用keras用常规神经网络训练MNIST数据集]]></title>
    <url>%2F2019%2F03%2F02%2FmnistLinearNN%2F</url>
    <content type="text"><![CDATA[加载mnist数据集123456from keras.datasets import mnist(x_train,y_train),(x_test,y_test) = mnist.load_data() # 将下载好的mnist.npz方在 ~/.keras/datasets/ 目录下print(x_train.shape,type(x_train))print(y_train.shape,type(y_train))print(x_test.shape,type(x_test))print(y_test.shape,type(y_test)) (60000, 28, 28) &lt;class &apos;numpy.ndarray&apos;&gt; (60000,) &lt;class &apos;numpy.ndarray&apos;&gt; (10000, 28, 28) &lt;class &apos;numpy.ndarray&apos;&gt; (10000,) &lt;class &apos;numpy.ndarray&apos;&gt; 数据处理：规范化1234# 将图形从[28,28]变为[784,]X_train = x_train.reshape(60000,784)X_test = x_test.reshape(10000,784)print(X_train.shape,X_test.shape) (60000, 784) (10000, 784) 123456# 将数据转换为float32，为了进行归一化，不然/255得到全部是0X_train = X_train.astype('float32')X_test = X_test.astype('float32')# 数据归一化X_train /= 255X_test /= 255 统计训练数据中个标签数量12345import numpy as npimport matplotlib.pyplot as pltlabel, count = np.unique(y_train, return_counts=True)print(label, count) [0 1 2 3 4 5 6 7 8 9] [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949] 123456789101112fig = plt.figure(figsize=(8, 5))plt.bar(label, count, width=0.7, align='center')plt.title("Label Distribution")plt.xlabel('Label')plt.ylabel('Count')plt.xticks(label)plt.ylim(0, 7500)for a, b in zip(label, count): plt.text(a, b, '%d' % b, ha='center', va='bottom', fontsize=10)plt.show() 对标签进行one-hot编码123456789101112131415161718192021# import tensorflow as tf# n_classes = 10# Y_train = tf.one_hot(y_train, n_classes)# Y_test = tf.one_hot(y_test, n_classes)# with tf.Session() as sess:# sess.run(tf.global_variables_initializer())# Y_train=sess.run(Y_train)# Y_test=sess.run(Y_test)# print(Y_train.shape)# 下面代码同上，使用tensorflow需要建立会话，简单转换keras更方便from keras.utils import np_utilsn_classes = 10Y_train = np_utils.to_categorical(y_train,n_classes)Y_test = np_utils.to_categorical(y_test,n_classes)print(Y_train.shape) (60000, 10) 12print(y_train[0])print(Y_train[0]) 5 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] 使用Keras sequential model 定义神经网络1234567891011121314# 使用keras定义线性网络很方便from keras.models import Sequentialfrom keras.layers.core import Dense, Activationmodel = Sequential()# 第一隐藏层model.add(Dense(512, input_shape=(784,)))model.add(Activation('relu'))# 第二隐藏层model.add(Dense(512))model.add(Activation('relu'))# 输出层model.add(Dense(10))model.add(Activation('softmax')) 编译模型1model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy']) 训练模型，并将指标保存到history中12history = model.fit(X_train, Y_train, batch_size=128, epochs=5, verbose=2, validation_data=(X_test, Y_test)) Train on 60000 samples, validate on 10000 samples Epoch 1/5 - 7s - loss: 0.2156 - acc: 0.9373 - val_loss: 0.0970 - val_acc: 0.9710 Epoch 2/5 - 7s - loss: 0.0804 - acc: 0.9758 - val_loss: 0.0769 - val_acc: 0.9770 Epoch 3/5 - 7s - loss: 0.0504 - acc: 0.9838 - val_loss: 0.0791 - val_acc: 0.9746 Epoch 4/5 - 7s - loss: 0.0350 - acc: 0.9891 - val_loss: 0.0659 - val_acc: 0.9804 Epoch 5/5 - 8s - loss: 0.0264 - acc: 0.9913 - val_loss: 0.0734 - val_acc: 0.9794 可视化指标12345678910111213141516171819fig = plt.figure()plt.subplot(211)plt.plot(history.history['acc'])plt.plot(history.history['val_acc'])plt.title('Model Accuracy')plt.xlabel('epoch')plt.ylabel('accuracy')plt.legend(['train','test'])plt.subplot(212)plt.plot(history.history['loss'])plt.plot(history.history['val_loss'])plt.title('Model Loss')plt.xlabel('epoch')plt.ylabel('loss')plt.legend(['train','test'])plt.tight_layout()plt.show() 保存模型123456789101112import osimport tensorflow.gfile as gfilesave_dir = '.\model'if gfile.Exists(save_dir): gfile.DeleteRecursively(save_dir)gfile.MakeDirs(save_dir)model_name = 'keras_mnist.h5'model_path = os.path.join(save_dir,model_name)model.save(model_path)print('Saved trained model at %s' % model_path) Saved trained model at .\model\keras_mnist.h5 加载模型123from keras.models import load_modelmnist_model = load_model(model_path) 统计模型在测试集上的分类结果123456789loss_and_metrics = mnist_model.evaluate(X_test, Y_test, verbose=2)print("Test Loss: &#123;&#125;".format(loss_and_metrics[0]))print("Test Accuracy: &#123;&#125;%".format(loss_and_metrics[1]*100))predicted_classes = mnist_model.predict_classes(X_test)correct_indices = np.nonzero(predicted_classes == y_test)[0]incorrect_indices = np.nonzero(predicted_classes != y_test)[0]print("Classified correctly count: &#123;&#125;".format(len(correct_indices)))print("Classified incorrectly count: &#123;&#125;".format(len(incorrect_indices))) Test Loss: 0.07340353026344673 Test Accuracy: 97.94% Classified correctly count: 9794 Classified incorrectly count: 206 代码地址github]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>mnist</tag>
        <tag>keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用keras用卷积神经网络训练MNIST数据集]]></title>
    <url>%2F2019%2F03%2F02%2FmnistCNN%2F</url>
    <content type="text"><![CDATA[加载mnist数据集123456from keras.datasets import mnist(x_train,y_train),(x_test,y_test) = mnist.load_data() # 将下载好的mnist.npz方在 ~/.keras/datasets/ 目录下print(x_train.shape,type(x_train))print(y_train.shape,type(y_train))print(x_test.shape,type(x_test))print(y_test.shape,type(y_test)) (60000, 28, 28) &lt;class &apos;numpy.ndarray&apos;&gt; (60000,) &lt;class &apos;numpy.ndarray&apos;&gt; (10000, 28, 28) &lt;class &apos;numpy.ndarray&apos;&gt; (10000,) &lt;class &apos;numpy.ndarray&apos;&gt; 数据处理：规范化channels_last对应的输入：(batch,height,width,channels) channels_first对应的输入：(batch,channels,height,width) 默认channels_last，修改：~/.keras/keras.json 123456789101112131415from keras import backend as Kimg_rows, img_cols = 28, 28if K.image_data_format() == 'channels_first': x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols) x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols) input_shape = (1, img_rows, img_cols)else: x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1) x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1) input_shape = (img_rows, img_cols, 1)print(x_train.shape, type(x_train))print(x_test.shape, type(x_test)) (60000, 28, 28, 1) &lt;class &apos;numpy.ndarray&apos;&gt; (10000, 28, 28, 1) &lt;class &apos;numpy.ndarray&apos;&gt; 123456# 将数据转换为float32，为了进行归一化，不然/255得到全部是0X_train = x_train.astype('float32')X_test = x_test.astype('float32')# 数据归一化X_train /= 255X_test /= 255 统计训练数据中个标签数量12345import numpy as npimport matplotlib.pyplot as pltlabel, count = np.unique(y_train, return_counts=True)print(label, count) [0 1 2 3 4 5 6 7 8 9] [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949] 123456789101112fig = plt.figure(figsize=(8, 5))plt.bar(label, count, width=0.7, align='center')plt.title("Label Distribution")plt.xlabel('Label')plt.ylabel('Count')plt.xticks(label)plt.ylim(0, 7500)for a, b in zip(label, count): plt.text(a, b, '%d' % b, ha='center', va='bottom', fontsize=10)plt.show() 对标签进行one-hot编码1234567from keras.utils import np_utilsn_classes = 10Y_train = np_utils.to_categorical(y_train,n_classes)Y_test = np_utils.to_categorical(y_test,n_classes)print(Y_train.shape) (60000, 10) 12print(y_train[0])print(Y_train[0]) 5 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] 使用Keras sequential model 定义MNIST CNN网络123456789101112131415161718192021222324from keras.models import Sequentialfrom keras.layers import Dense, Dropout, Flattenfrom keras.layers import Conv2D, MaxPooling2Dmodel = Sequential()## Feature Extraction# 第一层卷积，32个3*3的卷积核，激活函数使用relumodel.add(Conv2D(filters=32,kernel_size=(3,3),activation='relu',input_shape=input_shape))# 第二层卷积，64个3*3的卷积核，激活函数使用relumodel.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu'))# 最大池化层model.add(MaxPooling2D(pool_size=(2,2)))# Dropout 25% 的输入神经元model.add(Dropout(0.25))# 将Pooled feature map 摊平后输入全连接网络model.add(Flatten())## Classification# 全连接层model.add(Dense(128,activation='relu'))# Dropout 50% 的输入神经元model.add(Dropout(0.5))# 使用softmax 激活函数做多分类，输出各数字的概率model.add(Dense(10, activation='softmax')) 查看 MNIST CNN 模型网络结构1model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 26, 26, 32) 320 _________________________________________________________________ conv2d_2 (Conv2D) (None, 24, 24, 64) 18496 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 12, 12, 64) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 9216) 0 _________________________________________________________________ dense_1 (Dense) (None, 128) 1179776 _________________________________________________________________ dropout_2 (Dropout) (None, 128) 0 _________________________________________________________________ dense_2 (Dense) (None, 10) 1290 ================================================================= Total params: 1,199,882 Trainable params: 1,199,882 Non-trainable params: 0 _________________________________________________________________ 12for layer in model.layers: print(layer.get_output_at(0).get_shape().as_list()) [None, 26, 26, 32] [None, 24, 24, 64] [None, 12, 12, 64] [None, 12, 12, 64] [None, None] [None, 128] [None, 128] [None, 10] 编译模型1model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy']) 训练模型，并将指标保存到history中1history = model.fit(X_train, Y_train, batch_size=128, epochs=5,verbose=2, validation_data=(X_test, Y_test)) Train on 60000 samples, validate on 10000 samples Epoch 1/5 - 131s - loss: 0.2330 - acc: 0.9290 - val_loss: 0.0540 - val_acc: 0.9817 Epoch 2/5 - 146s - loss: 0.0853 - acc: 0.9747 - val_loss: 0.0372 - val_acc: 0.9882 Epoch 3/5 - 136s - loss: 0.0605 - acc: 0.9812 - val_loss: 0.0315 - val_acc: 0.9898 Epoch 4/5 - 129s - loss: 0.0514 - acc: 0.9843 - val_loss: 0.0283 - val_acc: 0.9913 Epoch 5/5 - 130s - loss: 0.0416 - acc: 0.9873 - val_loss: 0.0272 - val_acc: 0.9911 可视化指标12345678910111213141516171819fig = plt.figure()plt.subplot(211)plt.plot(history.history['acc'])plt.plot(history.history['val_acc'])plt.title('Model Accuracy')plt.xlabel('epoch')plt.ylabel('accuracy')plt.legend(['train','test'])plt.subplot(212)plt.plot(history.history['loss'])plt.plot(history.history['val_loss'])plt.title('Model Loss')plt.xlabel('epoch')plt.ylabel('loss')plt.legend(['train','test'])plt.tight_layout()plt.show() 保存模型123456789101112import osimport tensorflow.gfile as gfilesave_dir = '.\model'if gfile.Exists(save_dir): gfile.DeleteRecursively(save_dir)gfile.MakeDirs(save_dir)model_name = 'keras_mnist.h5'model_path = os.path.join(save_dir,model_name)model.save(model_path)print('Saved trained model at %s' % model_path) Saved trained model at .\model\keras_mnist.h5 加载模型123from keras.models import load_modelmnist_model = load_model(model_path) 统计模型在测试集上的分类结果123456789loss_and_metrics = mnist_model.evaluate(X_test, Y_test, verbose=2)print("Test Loss: &#123;&#125;".format(loss_and_metrics[0]))print("Test Accuracy: &#123;&#125;%".format(loss_and_metrics[1]*100))predicted_classes = mnist_model.predict_classes(X_test)correct_indices = np.nonzero(predicted_classes == y_test)[0]incorrect_indices = np.nonzero(predicted_classes != y_test)[0]print("Classified correctly count: &#123;&#125;".format(len(correct_indices)))print("Classified incorrectly count: &#123;&#125;".format(len(incorrect_indices))) Test Loss: 0.027159390095694836 Test Accuracy: 99.11% Classified correctly count: 9911 Classified incorrectly count: 89 代码地址github]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>mnist</tag>
        <tag>keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv+tensorflow实时检测人脸]]></title>
    <url>%2F2018%2F12%2F29%2FfaceDetection%2F</url>
    <content type="text"><![CDATA[关于人脸检测的说明本文代码使用了opencv自带的人脸检测算法和mtcnn算法，mtcnn有明显的优势，检测成功率基本维持在100%，而且人脸各角度都可以检测成功，所以建议使用mtcnn来进行人脸检测，电脑cpu也可以流畅运行。 需要提前配置的环境：python + opencv + tensorflow 关于mtcnn的介绍，请参见压缩包中的电子书 代码结构说明 detect_face.py定义了mtcnn模型 det 1-3.npy是预训练好的模型，所以不用再对mtcnn进行训练 detect 1-3.py是三种实现方式，下面一一介绍 代码演示detect1.py使用mtcnn对一张图片进行检测，效果如下： detect2.py使用opencv自带的HAAR进行实时人脸检测，当人脸倾斜时无法检测到，效果如下： detect3.py使用MTCNN进行实时人脸检测，无论人脸各个角度，都可以检测到，效果如下： 代码地址github地址 百度云地址 注意：github地址中没有mtcnn的预训练模型，需要自己下载，百度云是完整的]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>人脸检测</tag>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下关于screen命令的使用]]></title>
    <url>%2F2018%2F12%2F08%2Flinux-screen%2F</url>
    <content type="text"><![CDATA[因为进入服务器只有一个窗口，当我们用这个窗口跑代码时，就没有办法同时用命令编辑一些文件。为了解决这个问题，我们可以使用screen开启多个进程，用一个进程跑代码，然后将这个窗口折叠到后台，创建新的进程来编辑代码。 当我们想要断开服务器连接仍然让一些程序运行的时候，可以使用screen让程序在后台一直运行。 安装screen (ubuntu系统)1sudo apt-get install screen 创建进程1screen -S 进程名 之后，会进入一个干净的窗口，可以执行相应操作，连续按Ctrl+A、Ctrl+D回到主线程，之前执行的操作会一直在后台运行，直到杀死该进程。 这条命令可以多次使用，创建多个进程。 查看当前screen进程1screen -ls 进入某一进程123#两条命令选其一screen -r 进程名screen -r 进程pid号 终止进程12345#方法一screen -X -S 进程名 quit#方法二先进入要杀死的进程，然后输入exit]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux下screen的使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下查看端口占用进程及杀死进程]]></title>
    <url>%2F2018%2F12%2F08%2Flinux-kill-process%2F</url>
    <content type="text"><![CDATA[直接查看进程1ps 通过端口查看进程1lsof –i:端口号 杀死进程1kill -9 pid号]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>查看linux进程并杀死</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux添加用户并赋予sudo权限]]></title>
    <url>%2F2018%2F11%2F29%2FaddLinuxUser%2F</url>
    <content type="text"><![CDATA[创建用户123# 在root用户下不用写sudosudo adduser fanfan # 在/home 下会自动创建同名文件夹passwd fanfan # 设置密码，上个命令有时会直接让输入密码，就不需要执行这一步了 删除用户1sudo userdel fanfan 添加sudo权限 su -切换到root vim /etc/sudoers ，在root ALL=(ALL) ALL的下一行添加： 12345# sudo时需要输入密码fanfan ALL=(ALL) ALL# sudo时不需要输入密码fanfan ALL=(ALL) NOPASSWD: ALL 按Esc，再输入:wq!保存文件，要加!，不然保存会出问题]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux添加用户</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[本地访问服务器端jupyter notebook]]></title>
    <url>%2F2018%2F11%2F29%2FremoteJupyter%2F</url>
    <content type="text"><![CDATA[1 登陆远程服务器2 生成配置文件1$ jupyter notebook --generate-config 3 生成密码打开ipython，创建一个密文的密码12345In [1]: from notebook.auth import passwdIn [2]: passwd()Enter password: Verify password: Out[2]: 'sha1:ce23d945972f:34769685a7ccd3d08c84a18c63968a41f1140274' 把生成的密文‘sha:ce…’复制下来 4 修改默认配置文件1$ vim ~/.jupyter/jupyter_notebook_config.py 进行如下修改：1234c.NotebookApp.ip='*'c.NotebookApp.password = u'sha:ce...刚才复制的那个密文'c.NotebookApp.open_browser = Falsec.NotebookApp.port =8888 #随便指定一个端口 5 启动jupyter notebook1$ jupyter notebook 6 远程访问此时应该可以直接从本地浏览器直接访问http://address_of_remote:8888就可以看到jupyter的登陆界面，输入第三步中设置的密码。 7 建立SSH通道如果登陆失败，则有可能是服务器防火墙设置的问题，此时最简单的方法是在本地建立一个ssh通道：在本地终端中输入：12ssh fanfan@222.92.146.251 -L127.0.0.1:1234:127.0.0.1:6666ssh fanfan@47.106.208.254 -L127.0.0.1:1234:127.0.0.1:8888 便可以在localhost:1234直接访问远程的jupyter了。]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>jupyter notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器端安装Anaconda]]></title>
    <url>%2F2018%2F11%2F29%2FinstallAnaconda%2F</url>
    <content type="text"><![CDATA[步骤打开网址：Anaconda清华镜像，复制要下载的文件地址，执行以下命令：12345678910wget 复制的网址（会下载一个sh文件）sh sh文件名 #执行后，会显示使用条款，按enter继续阅读，会让回答几个问题，全部yesrm -rf sh文件名source ~/.bashrc （使conda生效）#设置清华conda镜像conda config --prepend channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ 注意事项 Anaconda3-5.2.0.Linux-x86_64.sh ==&gt; python3.6 Anaconda3-5.3.1.Linux-x86_64.sh ==&gt; python3.7 若wget显示网络不可达，执行以下操作： 123456#centossudo yum -y install wget#ubuntusudo apt-get updatesudo apt-get install wget 若不能运行jupyter notebook，进行如下配置：jupyter notebook配置]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>Anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux终端bash美化教程]]></title>
    <url>%2F2018%2F11%2F29%2FbeautifyBash%2F</url>
    <content type="text"><![CDATA[美化步骤12345vim .bashrc添加下行export PS1="Time:\[\033[1;35m\]\T \[\033[0m\]User:\[\033[1;33m\]\u \[\033[0m\]Dir:\[\033[1;32m\]\w\[\033[0m\]\n\$"退出vimsource .bashrc 美化效果 PS1中参数的具体含义参考链接]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo写博客步骤]]></title>
    <url>%2F2018%2F11%2F29%2FwriteArtical%2F</url>
    <content type="text"><![CDATA[博客编写步骤1 进入D:\Blog文件夹下，打开终端 2 输入：hexo new &quot;文件名&quot;，在D:\Blog\source\\_posts目录下创建了文件名.md文件 3 打开文件名.md，编写博客 4 终端输入：hexo d -g提交博客 md文件编写注意事项1234567---title: 博客名categories: 分类名tags: - 标签1 - 标签2--- 更新博客分类与标签页面12hexo cleanhexo d -g]]></content>
      <tags>
        <tag>Hexo发送文章</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简易安卓聊天软件之思路架构及源码]]></title>
    <url>%2F2018%2F11%2F04%2FweChat%2F</url>
    <content type="text"><![CDATA[安卓聊天软件完成的功能罗列1 登陆2 动态显示好友列表3 服务端程序4 客户端程序5 安全退出6 与多好友聊天，屏幕切换，可以保存原信息，每次新登陆，可以读取历史记录7 缓存消息，及离线完成 软件架构图 本地Sqlite数据库设计只有一张表，存储聊天消息，表中有三个属性，分别为：发送者 接收者 消息内容 客户端与服务端传输消息协议约定： 客户端新上线的时候，向服务端发送用户名，服务端向客户端发送好友列表与离线消息 客户端 ==&gt; 服务端：发送者：接收者：消息 服务端 ==&gt; 客户端：发送者：接收者：消息 服务端向客户端发送的是消息还是好友列表，以开头是否是”&amp;”符号区分 客户端目录结构（Android Studio） 客户端的基本思路Service负责与服务器进行网络连接与IO读写，无论是发送消息还是接受消息，Service都先把消息存到本地数据库，FriendListActivity与ChatActivity中ListView的显示，都是直接从数据库读取数据。Service与Activity的通信主要使用Intent和广播来进行。 服务端程序服务端基本思路（具体代码见文末源码地址）： 使用一个List存储所有好友 使用一个Map存储在线好友及对应Socket 使用一个Map存储离线消息 软件开发经验总结这次软件开发是以小组形式进行的，最后算是完成了聊天软件的基本功能，这次开发做的好的地方在于一开始小组就先把真个架构图设计好了，包括数据库，后面写代码基本很顺畅，得到的经验就是开发一个软件，做一个项目，写代码真的是很靠后的事情了，前期一定是先通过写用例，梳理好逻辑，画好架构图，后期按照梳理好的逻辑来写代码。后期还需要努力的地方在于UML类图，希望下次开发前期能把UML类图画出来，这样前期工作会更完善，加油，希望可以成为一个专业的程序员。 源码地址源码：github地址 注意：源码中的ImServeFinal.java文件时服务端程序，应该拿出来用java的IDE运行，记得更改ip与端口]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>java</tag>
      </tags>
  </entry>
</search>
