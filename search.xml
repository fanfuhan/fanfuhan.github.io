<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[贪吃蛇小项目(c++实现)]]></title>
    <url>%2F2019%2F04%2F05%2Ftanchishe%2F</url>
    <content type="text"><![CDATA[模块结构 墙模块 食物模块 蛇模块 主程序 代码墙模块(wall.h) 123456789101112131415161718192021#ifndef TANCHISHE_WALL_H#define TANCHISHE_WALL_H#include &lt;iostream&gt;using namespace std;class Wall&#123;public: enum &#123;ROW = 20, COL = 30&#125;; // 初始化墙壁 void initWall(); // 画出墙壁 void drawWall(); // 根据索引设置二维数据里的内容 void setWall(int x, int y, char c); // 根据索引获取当前位置的符号 char getWall(int x, int y);private: char gameArray[ROW][COL];&#125;;#endif //TANCHISHE_WALL_H 食物模块(food.h) 123456789101112131415161718#ifndef TANCHISHE_FOOD_H#define TANCHISHE_FOOD_H#include &lt;iostream&gt;#include "wall.h"using namespace std;class Food&#123;public: Food(Wall &amp;tempWall); // 设置食物 void setFood();private: int foodX; int foodY; Wall &amp;wall;&#125;;#endif //TANCHISHE_FOOD_H 蛇模块(snake.h) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#ifndef TANCHISHE_SNAKE_H#define TANCHISHE_SNAKE_H#include &lt;iostream&gt;#include "wall.h"#include "food.h"using namespace std;class Snake &#123;public: Snake(Wall &amp;tempWall, Food &amp;tempFood); enum &#123; UP = 'w', DOWN = 's', LEFT = 'a', RIGHT = 'd' &#125;; struct Point &#123; int x; int y; Point *next; &#125;; // 初始化 void initSnake(); // 销毁节点 void destroyPoint(); // 添加节点 void addPoint(int x, int y); // 删除节点 void delPoint(); // 移动蛇 bool move(char key); // 设定难度 // 获取刷屏时间 int getSleepTime(); // 获取蛇身段 int countList(); // 获取分数 int getScore();private: Point *pHead; Wall &amp;wall; Food &amp;food; bool isRool; // 判断循环表示&#125;;#endif //TANCHISHE_SNAKE_H 主程序(game.cpp) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#include &lt;iostream&gt;#include "wall.h"#include "snake.h"#include "food.h"#include &lt;ctime&gt;#include &lt;conio.h&gt;#include &lt;windows.h&gt;// 定位光标void gotoxy(HANDLE hOut1, int x, int y) &#123; COORD pos; pos.X = x; pos.Y = y; SetConsoleCursorPosition(hOut1, pos);&#125;HANDLE hOut = GetStdHandle(STD_OUTPUT_HANDLE); // 定义显示器句柄变量int main() &#123; // 添加随机数种子 srand((unsigned int) time(NULL)); // 是否死亡的标识 bool isDead = false; // 上一次按键标识 bool preKey = true; Wall wall; wall.initWall(); wall.drawWall(); Food food(wall); food.setFood(); Snake snake(wall, food); snake.initSnake(); gotoxy(hOut, 0, Wall::ROW); cout &lt;&lt; "score：" &lt;&lt; snake.getScore() &lt;&lt; " points" &lt;&lt; endl; while (!isDead) &#123; // 接受用户输入 char key = _getch(); // 第一次按左键，不激活游戏 if (preKey &amp;&amp; key == snake.LEFT) &#123; continue; &#125; preKey = false; do &#123; if (key == snake.UP || key == snake.DOWN || key == snake.LEFT || key == snake.RIGHT) &#123; if (snake.move(key)) &#123; //system("cls"); //wall.drawWall(); gotoxy(hOut, 0, Wall::ROW); cout &lt;&lt; "score：" &lt;&lt; snake.getScore() &lt;&lt; " points" &lt;&lt; endl; Sleep(snake.getSleepTime()); &#125; else &#123; isDead = true; break; &#125; &#125; else &#123; cout &lt;&lt; "input error, please input again" &lt;&lt; endl; break; &#125; &#125; while (!_kbhit()); // 没有键盘输入的时候，返回0 &#125; system("pause"); return 0;&#125; 结果 代码地址github]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>贪吃蛇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动编码器(含两个隐藏层)]]></title>
    <url>%2F2019%2F04%2F03%2FAutoEncoder%2F</url>
    <content type="text"><![CDATA[知识点 自动编码器分类 降噪自动编码器代码 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_data# 控制训练过程的参数learning_rate = 0.01training_epochs = 20batch_size = 256display_step = 5examples_to_show = 10# w网络模型参数n_input_units = 784 # 输入神经元数量 MNIST data input (img shape : 28*28)n_hidden1_units = 256 # 编码起第一隐藏层神经元数量（让编码器和解码器都有同样规模的隐藏层n_hidden2_units = 128 # 编码起第二隐藏层神经元数量（让编码器和解码器都有同样规模的隐藏层n_output_units = n_input_units # 解码器输出层神经元数量必须等于输入数据的units数量# 对一个张量进行全面汇总(均值，标准差，最大最小值，直方图)def varible_summaries(var): with tf.name_scope('summaries'): mean = tf.reduce_mean(var) stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean))) tf.summary.scalar('mean', mean) tf.summary.scalar('stddev', stddev) tf.summary.scalar('max', tf.reduce_max(var)) tf.summary.scalar('min', tf.reduce_min(var)) tf.summary.histogram('histogram', var)# 根据输入输出节点数量返回权重def WeightsVarible(n_in, n_out, name_str='weights'): return tf.Variable(tf.random_normal([n_in, n_out]), dtype=tf.float32, name=name_str)# 根据输出节点数量返回偏置def BiasesVarible(n_out, name_str='biases'): return tf.Variable(tf.random_normal([n_out]), dtype=tf.float32, name=name_str)# 构建编码器def Encoder(x_origin, activate_func=tf.nn.sigmoid): # 编码器第一隐藏层 with tf.name_scope('Layer1'): weights = WeightsVarible(n_input_units, n_hidden1_units) biases = BiasesVarible(n_hidden1_units) x_code1 = activate_func(tf.nn.xw_plus_b(x_origin, weights, biases)) varible_summaries(weights) # 编码器第二隐藏层 with tf.name_scope('Layer2'): weights = WeightsVarible(n_hidden1_units, n_hidden2_units) biases = BiasesVarible(n_hidden2_units) x_code = activate_func(tf.nn.xw_plus_b(x_code1, weights, biases)) varible_summaries(weights) return x_code# 构建解吗器def Decoder(x_code, activate_func=tf.nn.sigmoid): # 解码器第一隐藏层 with tf.name_scope('Layer'): weights = WeightsVarible(n_hidden2_units, n_hidden1_units) biases = BiasesVarible(n_hidden1_units) x_decode1 = activate_func(tf.nn.xw_plus_b(x_code, weights, biases)) varible_summaries(weights) # 解码器第二隐藏层 with tf.name_scope('Layer'): weights = WeightsVarible(n_hidden1_units, n_output_units) biases = BiasesVarible(n_output_units) x_decode = activate_func(tf.nn.xw_plus_b(x_decode1, weights, biases)) varible_summaries(weights) return x_decode# 调用上面写的函数构造计算图with tf.Graph().as_default(): # 计算图输入 with tf.name_scope('X_origin'): X_origin = tf.placeholder(tf.float32, [None, n_input_units]) # 构建编码器 with tf.name_scope('Encoder'): X_code = Encoder(X_origin) # 构建解吗器 with tf.name_scope('Decoder'): X_decode = Decoder(X_code) # 定义损失节点 with tf.name_scope('Loss'): Loss = tf.reduce_mean(tf.pow(X_origin - X_decode, 2)) # 定义优化器 with tf.name_scope('Train'): Optimizer = tf.train.RMSPropOptimizer(learning_rate) Train = Optimizer.minimize(Loss) # 为计算图添加损失节点的标量汇总(scalar summary) with tf.name_scope('LossSummary'): tf.summary.scalar('loss', Loss) tf.summary.scalar('learning_rate', learning_rate) # 为计算图添加图像汇总 with tf.name_scope('ImageSummary'): image_origin = tf.reshape(X_origin, [-1, 28, 28, 1]) image_reconstructed = tf.reshape(X_decode, [-1, 28, 28, 1]) tf.summary.image('image_origin', image_origin, 10) tf.summary.image('image_reconstructed', image_reconstructed, 10) # 聚合所有汇总节点 merged_summary = tf.summary.merge_all() init = tf.global_variables_initializer() print("把计算图写入事件文件，在TensorBoard里面查看") writer = tf.summary.FileWriter(logdir='logs', graph=tf.get_default_graph()) writer.flush() # 读取数据集 mnist = input_data.read_data_sets('mnist_data/', one_hot=True) with tf.Session() as sess: sess.run(init) total_batch = int(mnist.train.num_examples / batch_size) for epoch in range(training_epochs): for i in range(total_batch): batch_xs, batch_ys = mnist.train.next_batch(batch_size) _, loss = sess.run([Train, Loss], feed_dict=&#123;X_origin: batch_xs&#125;) if epoch % display_step == 0: print("epoch : %03d, loss = %.3f" % (epoch + 1, loss)) # 运行汇总节点，更新事件文件 summary_str = sess.run(merged_summary, feed_dict=&#123;X_origin: batch_xs&#125;) writer.add_summary(summary_str, epoch) writer.flush() writer.close() print("训练完毕！") # 把训练好的编码器-解码器模型用在测试集上，输出重建后的样本数据 reconstructions = sess.run(X_decode, feed_dict=&#123;X_origin: mnist.test.images[:examples_to_show]&#125;) # 比较原始图像与重建后的图像 f, a = plt.subplots(2, 10, figsize=(10, 2)) for i in range(examples_to_show): a[0][i].imshow(np.reshape(mnist.test.images[i], (28, 28))) a[1][i].imshow(np.reshape(reconstructions[i], (28, 28))) f.show() plt.draw() 结果计算图 测试结果 代码地址github]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>自动编码器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[降噪自动编码器]]></title>
    <url>%2F2019%2F04%2F03%2FDenoiseAutoEncoder%2F</url>
    <content type="text"><![CDATA[知识点 参数初始化问题 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126import numpy as npimport sklearn.preprocessing as prepimport tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_data# Xavier均匀初始化def xavier_init(fan_in, fan_out, constant = 1): low = -constant * np.sqrt(6.0 / (fan_in + fan_out)) high = constant * np.sqrt(6.0 / (fan_in + fan_out)) return tf.random_uniform((fan_in, fan_out), minval=low, maxval=high, dtype=tf.float32)# 加性高斯噪声的自动编码器class AdditiveGaussianNoiseAutoencoder(object): def __init__(self, n_input, n_hidden, transfer_function=tf.nn.softplus, optimizer=tf.train.AdamOptimizer(), scale=0.1): self.n_input = n_input self.n_hidden = n_hidden self.transfer = transfer_function self.training_scale = scale self.weights = dict() # 构建计算图 with tf.name_scope('raw_input'): self.x = tf.placeholder(tf.float32, [None, self.n_input]) with tf.name_scope('NoiseAdder'): self.scale = tf.placeholder(tf.float32) self.noise_x = self.x + self.scale * tf.random_normal((n_input,)) with tf.name_scope('encoder'): self.weights['w1'] = tf.Variable(xavier_init(self.n_input, self.n_hidden), name='weight1') self.weights['b1'] = tf.Variable(tf.zeros([self.n_hidden], dtype=tf.float32), name='bias1') self.hidden = self.transfer(tf.add(tf.matmul(self.noise_x, self.weights['w1']), self.weights['b1'])) with tf.name_scope('reconstruction'): self.weights['w2'] = tf.Variable(tf.zeros([self.n_hidden, self.n_input], dtype=tf.float32), name='weight2') self.weights['b2'] = tf.Variable(tf.zeros([self.n_input], dtype=tf.float32), name='bias2') self.reconstruction = tf.nn.xw_plus_b(self.hidden, self.weights['w2'], self.weights['b2']) # hidden * w2 + b2 with tf.name_scope('loss'): self.cost = 0.5 * tf.reduce_sum(tf.pow(tf.subtract(self.reconstruction, self.x), 2)) with tf.name_scope('train'): self.optimizer = optimizer.minimize(self.cost) init = tf.global_variables_initializer() self.sess = tf.Session() self.sess.run(init) print("begin to run session...") # 在一个批次上训练模型 def partial_fit(self, X): cost, opt = self.sess.run((self.cost, self.optimizer), feed_dict=&#123;self.x: X, self.scale: self.training_scale&#125;) return cost # 在给定样本集合上计算损失（用于测试阶段） def calc_total_cost(self, X): return self.sess.run(self.cost, feed_dict=&#123;self.x: X, self.scale: self.training_scale&#125;) # 返回自编码器隐含层的输出结果，获得抽象后的高阶特征表示 def transform(self, X): return self.sess.run(self.hidden, feed_dict=&#123;self.x: X, self.scale: self.training_scale&#125;) # 将隐藏层的高阶特征作为输入，将其重建为原始输入数据 def generate(self, hidden = None): if hidden == None: hidden = np.random.normal(size=self.weights['b1']) return self.sess.run(self.reconstruction, feed_dict=&#123;self.hidden: hidden&#125;) # 整体运行一遍复原过程，包括提取高阶特征以及重建原始数据，输入原始数据，输出复原后的数据 def reconstruction(self, X): return self.sess.run(self.reconstruction, feed_dict=&#123;self.x: X, self.scale: self.training_scale&#125;) # 获取隐含层的权重 def getWeights(self): return self.sess.run(self.weights['w1']) # 获取隐含层的偏置 def getBiases(self): return self.sess.run(self.weights['b1'])AGN_AutoEncoder = AdditiveGaussianNoiseAutoencoder(n_input=784, n_hidden=200, optimizer=tf.train.AdamOptimizer(learning_rate=0.01), scale=0.01)print("把计算图写入事件文件，在TensorBoard里面查看")writer = tf.summary.FileWriter(logdir='logs', graph=AGN_AutoEncoder.sess.graph)writer.close()# 读取数据集mnist = input_data.read_data_sets('../mnist_data/', one_hot=True)# 使用sklearn.preprocessing 的数据标准化操作(0均值标准差为1) 预处理数据# 首先在训练集上估计均值与方差，然后将其作用到训练集和测试集def standard_scale(x_train, x_test): preprocesser = prep.StandardScaler().fit(x_train) x_train = preprocesser.transform(x_train) x_test = preprocesser.transform(x_test) return x_train, x_test# 获取随机block数据的函数：取一个从0到len(data) - batch_size的随机整数# 以这个随机整数为起始索引，抽出一个batch_size的批次样本def get_random_block_from_data(data, batch_size): start_index = np.random.randint(0, len(data) - batch_size) return data[start_index: start_index + batch_size]# 使用标准化操作变换数据集X_train, X_test = standard_scale(mnist.train.images, mnist.test.images)# 定义训练参数n_samples = int(mnist.train.num_examples)training_epochs = 20batch_size = 128display_step = 1 # 输出训练结果的间隔# 开始训练，每次epoch开始时将avg_cost设为0，计算总共需要的batch数量，# 这里使用的是有放回抽样，所以不能保证每个样本被抽到并参与训练for epoch in range(training_epochs): avg_cost = 0 total_batch = int(n_samples / batch_size) for i in range(total_batch): batch_xs = get_random_block_from_data(X_train, batch_size) cost = AGN_AutoEncoder.partial_fit(batch_xs) avg_cost += cost / batch_size avg_cost /= total_batch if epoch % display_step == 0: print("epoch : %03d, cost = %.3f" % (epoch + 1, avg_cost))# 计算测试集上的costprint("total cost :", str(AGN_AutoEncoder.calc_total_cost(X_test))) 计算图 代码地址github]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>降噪自动编码器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy[..., None]的理解]]></title>
    <url>%2F2019%2F04%2F01%2Fnumpy_None%2F</url>
    <content type="text"><![CDATA[numpy数组维度1import numpy as np 12arr_1 = np.array([1, 2, 3, 4])print(arr_1, '\n' , 'shape of arr_1:', arr_1.shape, '， dimension of arr_1:',np.ndim(arr_1)) output: arr_1 = [1 2 3 4] shape of arr_1: (4,) ， dimension of arr_1: 1 12arr_2 = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])print(arr_2, '\n' , 'shape of arr_2:', arr_2.shape, '， dimension of arr_2:',np.ndim(arr_2)) output: arr_2 = [[1 2 3 4] [5 6 7 8]] shape of arr_2: (2, 4) ， dimension of arr_2: 2 12arr_3 = np.array([[[1, 2, 3, 4], [5, 6, 7, 8]], [[1, 2, 3, 4], [5, 6, 7, 8]], [[1, 2, 3, 4], [5, 6, 7, 8]]])print(arr_3, '\n' , 'shape of arr_3:', arr_3.shape, '， dimension of arr_3:',np.ndim(arr_3)) output: arr_3 = [[[1 2 3 4] [5 6 7 8]] [[1 2 3 4] [5 6 7 8]] [[1 2 3 4] [5 6 7 8]]] shape of arr_3: (3, 2, 4) ， dimension of arr_3: 3 numpy数组切片中[…]的理解假设 x 是一个数组，np.ndim(x) == 5 x[1,2,...] == x[1,2,:,:,:] x[...,3] == x[:,:,:,:,3] x[4,...,5,:] == x[4,:,:,5,:] numpy数组切片中None的理解None 的作用就是在相应的位置上增加了一个维度，在这个维度上只有一个元素 假设 x.shape == (a, b)，则 (a, b) ==&gt; [None, :, :] ==&gt; (1, a, b) (a, b) ==&gt; [:, None, :] ==&gt; (a, 1, b) (a, b) ==&gt; [:, :, None] ==&gt; (a, b, 1) 123import numpy as nparr = np.array([[1,2,3],[4,5,6]])print(arr, '\n' , 'shape of arr:', arr.shape, '， dimension of arr:',np.ndim(arr)) output: arr = [[1 2 3] [4 5 6]] shape of arr: (2, 3) ， dimension of arr: 2 12None_1 = arr[None, :, :]print(None_1, '\n' , 'shape of None_1:', None_1.shape, '， dimension of None_1:',np.ndim(None_1)) output: None_1 = [[[1 2 3] [4 5 6]]] shape of None_1: (1, 2, 3) ， dimension of None_1: 3 12None_2 = arr[:, None, :]print(None_2, '\n' , 'shape of None_2:', None_2.shape, '， dimension of None_2:',np.ndim(None_2)) output: None_2 = [[[1 2 3]] [[4 5 6]]] shape of None_2: (2, 1, 3) ， dimension of None_2: 3 12None_3 = arr[:, :, None]print(None_3, '\n' , 'shape of None_3:', None_3.shape, '， dimension of None_3:',np.ndim(None_3)) output: None_3 = [[[1] [2] [3]] [[4] [5] [6]]] shape of None_3: (2, 3, 1) ， dimension of None_3: 3 numpy[…, None]的理解12None_3 = arr[..., None] # 等价于 None_3 = arr[:, :, None]print(None_3, '\n' , 'shape of None_3:', None_3.shape, '， dimension of None_3:',np.ndim(None_3)) output: None_3 = [[[1] [2] [3]] [[4] [5] [6]]] shape of None_3: (2, 3, 1) ， dimension of None_3: 3 12y = np.arange(12).reshape((2,2,3))print(y, '\n' , 'shape of y:', y.shape, '， dimension of y:',np.ndim(y)) output: y = [[[ 0 1 2] [ 3 4 5]] [[ 6 7 8] [ 9 10 11]]] shape of y: (2, 2, 3) ， dimension of y: 3 12y = y[..., None]print(y, '\n' , 'shape of y:', y.shape, '， dimension of y:',np.ndim(y)) output: y = [[[[ 0] [ 1] [ 2]] [[ 3] [ 4] [ 5]]] [[[ 6] [ 7] [ 8]] [[ 9] [10] [11]]]] shape of y: (2, 2, 3, 1) ， dimension of y: 4]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-020-图像直方图反向投影]]></title>
    <url>%2F2019%2F03%2F30%2Fopencv-020%2F</url>
    <content type="text"><![CDATA[知识点文字解释图像直方图反向投影是通过构建指定模板图像的二维直方图空间与目标的二维直方图空间，进行直方图数据归一化之后， 进行比率操作，对所有得到非零数值，生成查找表对原图像进行像素映射之后，再进行图像模糊输出的结果。 直方图反向投影流程 计算直方图 计算比率R LUT查找表 卷积模糊 归一化输出 API 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;void backProjection_demo(Mat &amp;mat, Mat &amp;model);/* * 图像直方图反向投影 */int main() &#123; Mat src = imread("../images/target.png"); Mat model = imread("../images/sample.png"); if (src.empty() || model.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; namedWindow("model", WINDOW_NORMAL); imshow("input", src); imshow("model", model); backProjection_demo(src, model); waitKey(0); return 0;&#125;void backProjection_demo(Mat &amp;image, Mat &amp;model) &#123; Mat image_hsv, model_hsv; cvtColor(image, image_hsv, COLOR_BGR2HSV); cvtColor(model, model_hsv, COLOR_BGR2HSV); // 定义直方图参数与属性 int h_bins = 32, s_bins = 32; int histSize[] = &#123;h_bins, s_bins&#125;; float h_ranges[] = &#123;0, 180&#125;, s_ranges[] = &#123;0, 256&#125;; const float* ranges[] = &#123;h_ranges, s_ranges&#125;; int channels[] = &#123;0, 1&#125;; Mat roiHist; calcHist(&amp;model_hsv, 1, channels, Mat(), roiHist, 2, histSize, ranges); normalize(roiHist, roiHist, 0, 255, NORM_MINMAX, -1, Mat()); MatND backproj; calcBackProject(&amp;image_hsv, 1, channels, roiHist, backproj, ranges); imshow("BackProj", backproj);&#125; 1234567891011121314151617181920212223242526272829303132333435363738import cv2 as cvimport numpy as npfrom matplotlib import pyplot as pltdef back_projection_demo(): sample = cv.imread("D:/javaopencv/sample.png") # hist2d_demo(sample) target = cv.imread("D:/javaopencv/target.png") # hist2d_demo(target) roi_hsv = cv.cvtColor(sample, cv.COLOR_BGR2HSV) target_hsv = cv.cvtColor(target, cv.COLOR_BGR2HSV) # show images cv.imshow("sample", sample) cv.imshow("target", target) roiHist = cv.calcHist([roi_hsv], [0, 1], None, [32, 32], [0, 180, 0, 256]) cv.normalize(roiHist, roiHist, 0, 255, cv.NORM_MINMAX) dst = cv.calcBackProject([target_hsv], [0, 1], roiHist, [0, 180, 0, 256], 1) cv.imshow("backProjectionDemo", dst)def hist2d_demo(image): hsv = cv.cvtColor(image, cv.COLOR_BGR2HSV) hist = cv.calcHist([hsv], [0, 1], None, [32, 32], [0, 180, 0, 256]) dst = cv.resize(hist, (400, 400)) cv.imshow("image", image) cv.imshow("hist", dst) plt.imshow(hist, interpolation='nearest') plt.title("2D Histogram") plt.show()back_projection_demo()cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像直方图反向投影</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-019-图像直方图比较]]></title>
    <url>%2F2019%2F03%2F30%2Fopencv-019%2F</url>
    <content type="text"><![CDATA[知识点图像直方图比较，就是计算两幅图像的直方图数据，比较两组数据的相似性，从而得到两幅图像之间的相似程度，直方图比较在早期的CBIR(以图搜图)中是应用很常见的技术手段，通常会结合边缘处理、词袋等技术一起使用。APIcompareHist(hist1, hist2, method)常见比较方法有 相关性(常用) 卡方 交叉 巴氏(常用) 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像直方图比较 */int main() &#123; Mat src1 = imread("../images/left01.jpg"); Mat src2 = imread("../images/left13.jpg"); if (src1.empty() || src2.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input1", src1); imshow("input2", src2); // 一般在HSV色彩空间进行计算 Mat hsv1, hsv2; cvtColor(src1, hsv1, COLOR_BGR2HSV); cvtColor(src2, hsv2, COLOR_BGR2HSV); int h_bins = 60, s_bins = 64; int histSize[] = &#123;h_bins, s_bins&#125;; float h_ranges[] = &#123;0, 180&#125;; float s_ranges[] = &#123;0, 256&#125;; const float* ranges[] = &#123;h_ranges, s_ranges&#125;; int channels[] = &#123;0, 1&#125;; Mat hist1, hist2; calcHist(&amp;hsv1, 1, channels, Mat(), hist1, 2, histSize, ranges); calcHist(&amp;hsv2, 1, channels, Mat(), hist2, 2, histSize, ranges); normalize(hist1, hist1, 0, 1, NORM_MINMAX, -1, Mat()); normalize(hist2, hist2, 0, 1, NORM_MINMAX, -1, Mat()); // 比较 double src1_src2_1 = compareHist(hist1, hist2, HISTCMP_CORREL); double src1_src2_2 = compareHist(hist1, hist2, HISTCMP_BHATTACHARYYA); printf("HISTCMP_CORREL : %.2f\n", src1_src2_1); printf("HISTCMP_BHATTACHARYYA : %.2f\n", src1_src2_1); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import cv2 as cvimport numpy as npsrc1 = cv.imread("D:/vcprojects/images/m1.png")src2 = cv.imread("D:/vcprojects/images/m2.png")src3 = cv.imread("D:/vcprojects/images/flower.png")src4 = cv.imread("D:/vcprojects/images/wm_test.png")cv.imshow("input1", src1)cv.imshow("input2", src2)cv.imshow("input3", src3)cv.imshow("input4", src4)hsv1 = cv.cvtColor(src1, cv.COLOR_BGR2HSV)hsv2 = cv.cvtColor(src2, cv.COLOR_BGR2HSV)hsv3 = cv.cvtColor(src3, cv.COLOR_BGR2HSV)hsv4 = cv.cvtColor(src4, cv.COLOR_BGR2HSV)hist1 = cv.calcHist([hsv1], [0, 1], None, [60, 64], [0, 180, 0, 256])hist2 = cv.calcHist([hsv2], [0, 1], None, [60, 64], [0, 180, 0, 256])hist3 = cv.calcHist([hsv3], [0, 1], None, [60, 64], [0, 180, 0, 256])hist4 = cv.calcHist([hsv4], [0, 1], None, [60, 64], [0, 180, 0, 256])cv.normalize(hist1, hist1, 0, 1.0, cv.NORM_MINMAX, dtype=np.float32)cv.normalize(hist2, hist2, 0, 1.0, cv.NORM_MINMAX)cv.normalize(hist3, hist3, 0, 1.0, cv.NORM_MINMAX)cv.normalize(hist4, hist4, 0, 1.0, cv.NORM_MINMAX)methods = [cv.HISTCMP_CORREL, cv.HISTCMP_CHISQR, cv.HISTCMP_INTERSECT, cv.HISTCMP_BHATTACHARYYA]str_method = ""for method in methods: src1_src2 = cv.compareHist(hist1, hist2, method) src3_src4 = cv.compareHist(hist3, hist4, method) if method == cv.HISTCMP_CORREL: str_method = "Correlation" if method == cv.HISTCMP_CHISQR: str_method = "Chi-square" if method == cv.HISTCMP_INTERSECT: str_method = "Intersection" if method == cv.HISTCMP_BHATTACHARYYA: str_method = "Bhattacharyya" print("%s src1_src2 = %.2f, src3_src4 = %.2f"%(str_method, src1_src2, src3_src4))cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像直方图比较</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-018-图像直方图均衡化]]></title>
    <url>%2F2019%2F03%2F29%2Fopencv-018%2F</url>
    <content type="text"><![CDATA[知识点图像直方图均衡化可以用于图像增强、对输入图像进行直方图均衡化处理，提升后续对象检测的准确率，在OpenCV人脸检测的代码演示中已经很常见。此外对医学影像图像与卫星遥感图像也经常通过直方图均衡化来提升图像质量。 API equalizeHist(src, dst) 代码（c++,python）123456789101112131415161718192021222324#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像直方图均衡化 */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; Mat gray, dst; cvtColor(src, gray, COLOR_BGR2GRAY); equalizeHist(gray, dst); imshow("input", gray); imshow("eq", dst); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324252627282930313233import cv2 as cvimport numpy as npfrom matplotlib import pyplot as pltdef custom_hist(gray): h, w = gray.shape hist = np.zeros([256], dtype=np.int32) for row in range(h): for col in range(w): pv = gray[row, col] hist[pv] += 1 y_pos = np.arange(0, 256, 1, dtype=np.int32) plt.bar(y_pos, hist, align='center', color='r', alpha=0.5) plt.xticks(y_pos, y_pos) plt.ylabel('Frequency') plt.title('Histogram') plt.show()src = cv.imread("../images/test.png")gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", gray)dst = cv.equalizeHist(gray)cv.imshow("eh", dst)custom_hist(gray)custom_hist(dst)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像直方图均衡化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-017-图像直方图]]></title>
    <url>%2F2019%2F03%2F28%2Fopencv-017%2F</url>
    <content type="text"><![CDATA[知识点图像直方图的解释图像直方图是图像像素值的统计学特征、计算代价较小，具有图像平移、旋转、缩放不变性等众多优点，广泛地应用于图像处理的各个领域，特别是灰度图像的阈值分割、基于颜色的图像检索以及图像分类、反向投影跟踪。常见的分为 灰度直方图 颜色直方图 Bins是指直方图的大小范围， 对于像素值取值在0～255之间的，最少有256个bin，此外还可以有16、32、48、128等，256除以bin的大小应该是整数倍。 OpenCV中相关APIcalcHist(&amp;bgr_plane[0], 1, 0, Mat(), b_hist, 1, bins, ranges);cv.calcHist([image], [i], None, [256], [0, 256]) 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;#include &lt;vector&gt;using namespace std;using namespace cv;const int bins = 256;Mat src;const char *winTitle = "input image";void showHistogram();/* * 图像直方图 */int main() &#123; src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow(winTitle, src); showHistogram(); waitKey(0); return 0;&#125;void showHistogram() &#123; // 三通道分离 vector&lt;Mat&gt; bgr_plane; split(src, bgr_plane); // 定义参数变量 const int channels[1] = &#123;0&#125;; const int bins[1] = &#123;256&#125;; float hranges[2] = &#123;0, 255&#125;; const float *ranges[1] = &#123;hranges&#125;; Mat b_hist, g_hist, r_hist; // 计算三通道直方图 calcHist(&amp;bgr_plane[0], 1, 0, Mat(), b_hist, 1, bins, ranges); calcHist(&amp;bgr_plane[1], 1, 0, Mat(), g_hist, 1, bins, ranges); calcHist(&amp;bgr_plane[2], 1, 0, Mat(), r_hist, 1, bins, ranges); /* * 显示直方图 */ int hist_w = 512; int hist_h = 400; int bin_w = cvRound((double) hist_w / bins[0]); Mat histImage = Mat::zeros(hist_h, hist_w, CV_8UC3); // 归一化直方图数据 normalize(b_hist, b_hist, 0, histImage.rows, NORM_MINMAX, -1); normalize(g_hist, g_hist, 0, histImage.rows, NORM_MINMAX, -1); normalize(r_hist, r_hist, 0, histImage.rows, NORM_MINMAX, -1); // 绘制直方图曲线 for (int i = 1; i &lt; bins[0]; ++i) &#123; line(histImage, Point(bin_w * (i - 1), hist_h - cvRound(b_hist.at&lt;float&gt;(i - 1))), Point(bin_w * (i), hist_h - cvRound(b_hist.at&lt;float&gt;(i))), Scalar(255, 0, 0), 2, 8, 0); line(histImage, Point(bin_w * (i - 1), hist_h - cvRound(g_hist.at&lt;float&gt;(i - 1))), Point(bin_w * (i), hist_h - cvRound(g_hist.at&lt;float&gt;(i))), Scalar(0, 255, 0), 2, 8, 0); line(histImage, Point(bin_w * (i - 1), hist_h - cvRound(r_hist.at&lt;float&gt;(i - 1))), Point(bin_w * (i), hist_h - cvRound(r_hist.at&lt;float&gt;(i))), Scalar(0, 0, 255), 2, 8, 0); &#125; imshow("Histogram", histImage);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142import cv2 as cvimport numpy as npfrom matplotlib import pyplot as pltdef custom_hist(gray): h, w = gray.shape hist = np.zeros([256], dtype=np.int32) for row in range(h): for col in range(w): pv = gray[row, col] hist[pv] += 1 y_pos = np.arange(0, 256, 1, dtype=np.int32) plt.bar(y_pos, hist, align='center', color='r', alpha=0.5) plt.xticks(y_pos, y_pos) plt.ylabel('Frequency') plt.title('Histogram') # plt.plot(hist, color='r') # plt.xlim([0, 256]) plt.show()def image_hist(image): cv.imshow("input", image) color = ('blue', 'green', 'red') for i, color in enumerate(color): hist = cv.calcHist([image], [i], None, [256], [0, 256]) plt.plot(hist, color=color) plt.xlim([0, 256]) plt.show()src = cv.imread("../images/test.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)cv.imshow("input", gray)#custom_hist(gray)image_hist(src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像直方图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-016-图像ROI与ROI操作]]></title>
    <url>%2F2019%2F03%2F28%2Fopencv-016%2F</url>
    <content type="text"><![CDATA[知识点图像的ROI(region of interest)是指图像中感兴趣区域、在OpenCV中图像设置图像ROI区域，实现只对ROI区域操作。 矩形ROI区域提取 矩形ROI区域copy 不规则ROI区域 ROI区域mask生成 像素位 and操作 提取到ROI区域 加背景or操作 add 背景与ROI区域 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * ROI及相关操作 */int main() &#123; Mat src = imread("../images/test.png"); imshow("input", src); int h = src.rows; int w = src.cols; // 获取ROI int cy = h / 2; int cx = w / 2; Rect rect(cx - 100, cy - 100, 200, 200); // 注意：roi 与 src指向同一块内存区域，改变roi,src也会改变 Mat roi = src(rect); imshow("roi", roi); // 人物背景图，换背景 // load image Mat image = imread("../images/boy.jpg"); imshow("input", image); // generate mask Mat hsv, mask, mask_not; cvtColor(image, hsv, COLOR_BGR2HSV); inRange(hsv, Scalar(35, 43, 46), Scalar(99, 255, 255), mask); imshow("mask", mask); // extract person Mat person; bitwise_not(mask, mask_not); imshow("mask_not", mask_not); bitwise_and(image, image, person, mask_not); imshow("person", person); // gengerate background Mat background = Mat::zeros(image.size(), image.type()); background.setTo(Scalar(255, 0 ,0)); imshow("background", background); // combine background + person Mat dst; bitwise_or(person, background, dst, mask); add(dst, person, dst); imshow("dst", dst); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import cv2 as cvimport numpy as npsrc = cv.imread("D:/javaopencv/dahlia_4.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)h, w = src.shape[:2]# 获取ROIcy = h//2cx = w//2roi = src[cy-100:cy+100,cx-100:cx+100,:]cv.imshow("roi", roi)# copy ROIimage = np.copy(roi)# modify ROIroi[:, :, 0] = 0cv.imshow("result", src)# modify copy roiimage[:, :, 2] = 0cv.imshow("result", src)cv.imshow("copy roi", image)# example with ROI - generate masksrc2 = cv.imread("D:/javaopencv/tinygreen.png");cv.imshow("src2", src2)hsv = cv.cvtColor(src2, cv.COLOR_BGR2HSV)mask = cv.inRange(hsv, (35, 43, 46), (99, 255, 255))# extract person ROImask = cv.bitwise_not(mask)person = cv.bitwise_and(src2, src2, mask=mask);# generate backgroundresult = np.zeros(src2.shape, src2.dtype)result[:,:,0] = 255# combine background + personmask = cv.bitwise_not(mask)dst = cv.bitwise_or(person, result, mask=mask)dst = cv.add(dst, person)cv.imshow("dst", dst)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像ROI与ROI操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-015-绘制几何形状及随机数的生成]]></title>
    <url>%2F2019%2F03%2F28%2Fopencv-015%2F</url>
    <content type="text"><![CDATA[知识点绘制几何形状 绘制直线 绘制圆 绘制矩形 绘制椭圆 填充几何形状 OpenCV没有专门的填充方法，只是把绘制几何形状时候的线宽thickness参数值设置为负数即表示填充该几何形状或者使用参数CV_FILLED 随机数方法：RNG 表示OpenCV C++版本中的随机数对象，rng.uniform(a, b)生成[a, b)之间的随机数，包含a，但是不包含b。 np.random.rand() 表示numpy中随机数生成，生成浮点数0～1的随机数, 包含0，不包含1。 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 绘制几何形状及随机数 */int main() &#123; Mat image = Mat::zeros(Size(512, 512), CV_8UC3); Rect rect(100, 100, 200, 200); // 绘制 rectangle(image, rect, Scalar(255, 0, 0), 2, LINE_8, 0); circle(image, Point(256, 256), 50, Scalar(0, 255, 0), 2, LINE_8, 0); ellipse(image, Point(256, 256), Size(150, 50), 360, 0, 360, Scalar(0, 0, 255), 2, LINE_8, 0); imshow("image_draw", image); // 填充 thickness=-1 or FILLED rectangle(image, rect, Scalar(255, 0, 0), FILLED, LINE_8, 0); ellipse(image, Point(256, 256), Size(150, 50), 360, 0, 360, Scalar(0, 0, 255), FILLED, LINE_8, 0); circle(image, Point(256, 256), 50, Scalar(0, 255, 0), -1, LINE_8, 0); imshow("image_fill", image); // 随机数 RNG rng(0xFFFFFF); image.setTo(Scalar(0, 0, 0)); Mat image_copy = image.clone(); for (int i = 0; i &lt; 100000; ++i) &#123; int x1 = rng.uniform(0, 512); int y1 = rng.uniform(0, 512); int x2 = rng.uniform(0, 512); int y2 = rng.uniform(0, 512); int b = rng.uniform(0, 256); int g = rng.uniform(0, 256); int r = rng.uniform(0, 256); rect.x = x1; rect.y = y1; rect.width = x2 - x1; rect.height = y2 - y1; // LINE_AA 反锯齿 line(image, Point(x1, y1), Point(x2, y2), Scalar(b, g, r), 1, LINE_AA, 0); rectangle(image_copy, rect, Scalar(b, g, r), 1, LINE_AA, 0); imshow("image_line", image); imshow("image_rect", image_copy); char c = waitKey(20); if (c == 27)&#123; // ESC break; &#125; &#125; waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526272829303132import cv2 as cvimport numpy as npimage = np.zeros((512, 512, 3), dtype=np.uint8)cv.rectangle(image, (100, 100), (300, 300), (255, 0, 0), 2, cv.LINE_8, 0)cv.circle(image, (256, 256), 50, (0, 0, 255), 2, cv.LINE_8, 0)cv.ellipse(image, (256, 256), (150, 50), 360, 0, 360, (0, 255, 0), 2, cv.LINE_8, 0)cv.imshow("image", image)cv.waitKey(0)for i in range(100000): image[:,:,:]= 0 x1 = np.random.rand() * 512 y1 = np.random.rand() * 512 x2 = np.random.rand() * 512 y2 = np.random.rand() * 512 b = np.random.randint(0, 256) g = np.random.randint(0, 256) r = np.random.randint(0, 256) # cv.line(image, (np.int(x1), np.int(y1)), (np.int(x2), np.int(y2)), (b, g, r), 4, cv.LINE_8, 0) cv.rectangle(image, (np.int(x1), np.int(y1)), (np.int(x2), np.int(y2)), (b, g, r), 1, cv.LINE_8, 0) cv.imshow("image", image) c = cv.waitKey(20) if c == 27: break # ESCä¸cv.imshow("image", image)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>绘制几何形状</tag>
        <tag>随机数生成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-014-使用resize进行图像插值(Image Interpolation)]]></title>
    <url>%2F2019%2F03%2F27%2Fopencv-014%2F</url>
    <content type="text"><![CDATA[知识点最常见四种插值算法 INTER_NEAREST = 0 #最近邻插值，速度快，没考虑周围像素影响 INTER_LINEAR = 1 #双线性插值 INTER_CUBIC = 2 #双立方插值，高质量 INTER_LANCZOS4 = 4 #高质量 关于这四种插值算法的详细代码实现与解释 三种常见双立方插值算法-CSDN 图像放缩之双立方插值 图像放缩之双线性内插值 Lanczos采样放缩算法 相关的应用场景几何变换、透视变换、插值计算新像素 API resize(InputArray src, OutputArray dst, Size dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR ) 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像插值 */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); int h = src.rows; int w = src.cols; float fx = 0.0, fy = 0.0; Mat dst = Mat::zeros(src.size(), src.type()); Size S(w * 2, h * 2); resize(src, dst, S, fx, fy, INTER_NEAREST); imshow("INTER_NEAREST", dst); resize(src, dst, S, fx, fy, INTER_LINEAR); imshow("INTER_LINEAR", dst); resize(src, dst, S, fx, fy, INTER_CUBIC); imshow("INTER_CUBIC", dst); resize(src, dst, S, fx, fy, INTER_LANCZOS4); imshow("INTER_LANCZOS4", dst); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324import cv2 as cvsrc = cv.imread("D:/vcprojects/images/test.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)h, w = src.shape[:2]print(h, w)dst = cv.resize(src, (w*2, h*2), fx=0.75, fy=0.75, interpolation=cv.INTER_NEAREST)cv.imshow("INTER_NEAREST", dst)dst = cv.resize(src, (w*2, h*2), interpolation=cv.INTER_LINEAR)cv.imshow("INTER_LINEAR", dst)dst = cv.resize(src, (w*2, h*2), interpolation=cv.INTER_CUBIC)cv.imshow("INTER_CUBIC", dst)dst = cv.resize(src, (w*2, h*2), interpolation=cv.INTER_LANCZOS4)cv.imshow("INTER_LANCZOS4", dst)cv.warpAffine()cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像插值</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-013-图像翻转(Image Flip)]]></title>
    <url>%2F2019%2F03%2F27%2Fopencv-013%2F</url>
    <content type="text"><![CDATA[知识点图像翻转的本质像素映射，OpenCV支持三种图像翻转方式 X轴翻转，flipcode = 0 Y轴翻转, flipcode = 1 XY轴翻转, flipcode = -1 相关的APIflip(src, dst, flipcode) src输入参数 dst 翻转后图像 flipcode 应用：摄像头拍摄后经常需要翻转 代码（c++,python）1234567891011121314151617181920212223242526272829303132#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像翻转 */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); Mat dst; // X轴 倒影 flip(src, dst, 0); imshow("x_flip", dst); // Y轴 镜像 flip(src, dst, 1); imshow("y_flip", dst); // XY轴 对角 flip(src, dst, -1); imshow("xy_flip", dst); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324252627282930import cv2 as cvimport numpy as npsrc = cv.imread("D:/vcprojects/images/test.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# X Flip 倒影dst1 = cv.flip(src, 0);cv.imshow("x-flip", dst1);# Y Flip 镜像dst2 = cv.flip(src, 1);cv.imshow("y-flip", dst2);# XY Flip 对角dst3 = cv.flip(src, -1);cv.imshow("xy-flip", dst3);# custom y-fliph, w, ch = src.shapedst = np.zeros(src.shape, src.dtype)for row in range(h): for col in range(w): b, g, r = src[row, col] dst[row, w - col - 1] = [b, g, r]cv.imshow("custom-y-flip", dst)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像翻转</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-012-视频读写]]></title>
    <url>%2F2019%2F03%2F26%2Fopencv-012%2F</url>
    <content type="text"><![CDATA[知识点VideoCapture 视频文件读取、摄像头读取、视频流读取VideoWriter 视频写出、文件保存、 CAP_PROP_FRAME_HEIGHT #高度 CAP_PROP_FRAME_WIDTH #宽度 CAP_PROP_FRAME_COUNT #数量 CAP_PROP_FPS #帧率 不支持音频编码与解码保存，不是一个音视频处理的库！主要是分析与解析视频内容。保存文件最大支持单个文件为2G。 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 视频读写 */int main() &#123; // 打开摄像头 // VideoCapture capture(0); // 打开视频文件 VideoCapture capture; capture.open("../images/vtest.avi"); if (!capture.isOpened()) &#123; cout &lt;&lt; "could not load video.." &lt;&lt; endl; return -1; &#125; Size S = Size((int) capture.get(CAP_PROP_FRAME_WIDTH), (int) capture.get(CAP_PROP_FRAME_HEIGHT)); int fps = capture.get(CAP_PROP_FPS); cout &lt;&lt; "capture fps: " &lt;&lt; fps &lt;&lt; endl; VideoWriter writer("D:/test.mp4", cv::VideoWriter::fourcc('D', 'I','V','X'), fps, S, true); Mat frame; while(capture.read(frame))&#123; imshow("input", frame); writer.write(frame); char c = waitKey(50); if(c == 27)&#123; break; &#125; &#125; capture.release(); writer.release(); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526import cv2 as cvimport numpy as npcapture = cv.VideoCapture("D:/vcprojects/images/768x576.avi")# capture = cv.VideoCapture(0) 打开摄像头height = capture.get(cv.CAP_PROP_FRAME_HEIGHT)width = capture.get(cv.CAP_PROP_FRAME_WIDTH)count = capture.get(cv.CAP_PROP_FRAME_COUNT)fps = capture.get(cv.CAP_PROP_FPS)print(height, width, count, fps)out = cv.VideoWriter("D:/test.mp4", cv.VideoWriter_fourcc('D', 'I', 'V', 'X'), 15, (np.int(width), np.int(height)), True)while True: ret, frame = capture.read() if ret is True: cv.imshow("video-input", frame) out.write(frame) c = cv.waitKey(50) if c == 27: # ESC break else: breakcapture.release()out.release() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>视频读写</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-011-图像像素归一化]]></title>
    <url>%2F2019%2F03%2F25%2Fopencv-011%2F</url>
    <content type="text"><![CDATA[知识点OpenCV中提供了四种归一化的方法 NORM_MINMAX NORM_INF NORM_L1 NORM_L2 最常用的就是NORM_MINMAX归一化方法. 四种归一化方法示例 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像像素归一化 */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; // imshow("input", src); Mat gray; cvtColor(src, gray, COLOR_BGR2GRAY); imshow("input", gray); // 显示图像用uchar类型，计算时转为float类型 gray.convertTo(gray, CV_32F); // NORM_MINMAX Mat dst = Mat::zeros(gray.size(), CV_32FC1); normalize(gray, dst, 1.0, 0, NORM_MINMAX); Mat res = dst * 255; res.convertTo(dst, CV_8UC1); // 显示图像用uchar类型 imshow("NORM_MINMAX", dst); // scale and shift by NORM_INF normalize(gray, dst, 1.0, 0, NORM_INF); res = dst * 255; res.convertTo(dst, CV_8UC1); imshow("NORM_INF", dst); // scale and shift by NORM_L1 normalize(gray, dst, 1.0, 0, NORM_L1); res = dst * 10000000; res.convertTo(dst, CV_8UC1); imshow("NORM_L1", dst); // scale and shift by NORM_L2 normalize(gray, dst, 1.0, 0, NORM_L2); res = dst * 10000; res.convertTo(dst, CV_8UC1); imshow("NORM_L2", dst); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526272829303132333435363738import cv2 as cvimport numpy as npsrc = cv.imread("D:/vcprojects/images/test.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)# 转换为浮点数类型数组gray = np.float32(gray)print(gray)# scale and shift by NORM_MINMAXdst = np.zeros(gray.shape, dtype=np.float32)cv.normalize(gray, dst=dst, alpha=0, beta=1.0, norm_type=cv.NORM_MINMAX)print(dst)cv.imshow("NORM_MINMAX", np.uint8(dst*255))# scale and shift by NORM_INFdst = np.zeros(gray.shape, dtype=np.float32)cv.normalize(gray, dst=dst, alpha=1.0, beta=0, norm_type=cv.NORM_INF)print(dst)cv.imshow("NORM_INF", np.uint8(dst*255))# scale and shift by NORM_L1dst = np.zeros(gray.shape, dtype=np.float32)cv.normalize(gray, dst=dst, alpha=1.0, beta=0, norm_type=cv.NORM_L1)print(dst)cv.imshow("NORM_L1", np.uint8(dst*10000000))# scale and shift by NORM_L2dst = np.zeros(gray.shape, dtype=np.float32)cv.normalize(gray, dst=dst, alpha=1.0, beta=0, norm_type=cv.NORM_L2)print(dst)cv.imshow("NORM_L2", np.uint8(dst*10000))cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像像素归一化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-010-图像像素值统计及应用（普通图像转化为二值图像）]]></title>
    <url>%2F2019%2F03%2F25%2Fopencv-010%2F</url>
    <content type="text"><![CDATA[知识点 最小(min) 最大(max) 均值(mean) 标准方差(standard deviation) API知识点 最大最小值minMaxLoc 计算均值与标准方差meanStdDev 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像像素值统计及应用（普通图像转化为二值图像） */int main() &#123; Mat src_bgr = imread("../images/test.png"); Mat src_gray; cvtColor(src_bgr, src_gray, COLOR_BGR2GRAY); if (src_bgr.empty() || src_gray.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input_bgr", src_bgr); // 计算灰度图像的最大最小值 double minVal, maxVal; Point minLoc, maxLoc; minMaxLoc(src_gray, &amp;minVal, &amp;maxVal, &amp;minLoc, &amp;maxLoc); cout &lt;&lt; "paramenters of src_gray:" &lt;&lt; endl; printf("min:%.2f, max:%.2f \n", minVal, maxVal); printf("min loc: (%d, %d) \n", minLoc.x, minLoc.y); printf("max loc: (%d, %d) \n", maxLoc.x, maxLoc.y); // 普通图像转二值图像 Mat mean, stddev; meanStdDev(src_bgr, mean, stddev); cout &lt;&lt; "paramenters of src_bgr:" &lt;&lt; endl; printf("blue channel mean:%.2f, stddev: %.2f \n", mean.at&lt;double&gt;(0, 0), stddev.at&lt;double&gt;(0, 0)); printf("green channel mean:%.2f, stddev: %.2f \n", mean.at&lt;double&gt;(1, 0), stddev.at&lt;double&gt;(1, 0)); printf("red channel mean:%.2f, stddev: %.2f \n", mean.at&lt;double&gt;(2, 0), stddev.at&lt;double&gt;(2, 0)); for (int row = 0; row &lt; src_bgr.rows; ++row) &#123; for (int col = 0; col &lt; src_bgr.cols; ++col) &#123; Vec3b bgr = src_bgr.at&lt;Vec3b&gt;(row, col); bgr[0] = bgr[0] &lt; mean.at&lt;double&gt;(0, 0) ? 0 : 255; bgr[1] = bgr[1] &lt; mean.at&lt;double&gt;(1, 0) ? 0 : 255; bgr[2] = bgr[2] &lt; mean.at&lt;double&gt;(2, 0) ? 0 : 255; src_bgr.at&lt;Vec3b&gt;(row, col) = bgr; &#125; &#125; imshow("binary", src_bgr); waitKey(0); return 0;&#125; 1234567891011121314151617181920import cv2 as cvimport numpy as npsrc = cv.imread("../images/test.png", cv.IMREAD_GRAYSCALE)cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)min, max, minLoc, maxLoc = cv.minMaxLoc(src)print("min: %.2f, max: %.2f"% (min, max))print("min loc: ", minLoc)print("max loc: ", maxLoc)means, stddev = cv.meanStdDev(src)print("mean: %.2f, stddev: %.2f"% (means, stddev))src[np.where(src &lt; means)] = 0src[np.where(src &gt; means)] = 255cv.imshow("binary", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像像素值统计</tag>
        <tag>普通图像转化为二值图像</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-009-色彩空间及其应用（提取图像的前景和背景）]]></title>
    <url>%2F2019%2F03%2F25%2Fopencv-009%2F</url>
    <content type="text"><![CDATA[知识点 RGB色彩空间 HSV色彩空间 -维基百科 ### 直方图算法中常用 YUV色彩空间 YCrCb色彩空间 # 皮肤检测常用 API知识点 色彩空间转换cvtColor 提取指定色彩范围区域inRange 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 色彩空间及其应用 */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // RGB ==&gt; HSV YUV YCrCb Mat hsv, yuv, ycrcb; cvtColor(src, hsv, COLOR_BGR2HSV); cvtColor(src, yuv, COLOR_BGR2YUV); cvtColor(src, ycrcb, COLOR_BGR2YCrCb); imshow("hsv", hsv); imshow("yuv", yuv); imshow("ycrcb", ycrcb); /* * 提取图像前景和背景 */ Mat src2 = imread("../images/boy.jpg"); imshow("input boy", src2); cvtColor(src2, hsv, COLOR_BGR2HSV); // 从HSV表中查到绿色的最低值和最高值，建立掩模 Mat mask, mask_not; inRange(hsv, Scalar(35, 43, 46), Scalar(77, 255, 255), mask); imshow("mask", mask); Mat fg, bg; // 提取背景 bitwise_and(src2, src2, bg, mask); // 提取前景 bitwise_not(mask, mask_not); imshow("mask_not", mask_not); bitwise_and(src2, src2, fg, mask_not); imshow("background", bg); imshow("foreground" ,fg); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425262728import cv2 as cvsrc = cv.imread("../images/test.png")cv.namedWindow("rgb", cv.WINDOW_AUTOSIZE)cv.imshow("rgb", src)# RGB to HSVhsv = cv.cvtColor(src, cv.COLOR_BGR2HSV)cv.imshow("hsv", hsv)# RGB to YUVyuv = cv.cvtColor(src, cv.COLOR_BGR2YUV)cv.imshow("yuv", yuv)# RGB to YUVycrcb = cv.cvtColor(src, cv.COLOR_BGR2YCrCb)cv.imshow("ycrcb", ycrcb)src2 = cv.imread("../images/boy.jpg");cv.imshow("src2", src2)hsv = cv.cvtColor(src2, cv.COLOR_BGR2HSV)mask = cv.inRange(hsv, (35, 43, 46), (99, 255, 255))dst = cv.bitwise_and(src2, src2, mask=mask)cv.imshow("mask", mask)cv.imshow("dst", dst)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>色彩空间</tag>
        <tag>提取图像前景和背景</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-008-图像通道的分离与合并]]></title>
    <url>%2F2019%2F03%2F24%2Fopencv-008%2F</url>
    <content type="text"><![CDATA[知识点OpenCV中默认imread函数加载图像文件，加载进来的是三通道彩色图像，色彩空间是RGB色彩空间、通道顺序是BGR（蓝色、绿色、红色）、对于三通道的图像OpenCV中提供了两个API函数用以实现通道分离与合并。 split // 通道分类 merge // 通道合并 扩展在很多CNN的卷积神经网络中输入的图像一般会要求[h, w, ch]其中h是高度、w是指宽度、ch是指通道数数目、OpenCV DNN模块中关于图像分类的googlenet模型输入[224,224,3]表示的就是224x224大小的三通道的彩色图像输入。 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像通道分离与合并 */int main() &#123; Mat src = imread("../images/baboon.jpg"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); vector&lt;Mat&gt; mv; // mv用于存储图像分离后各通道像素 Mat dst1, dst2, dst3; // 令蓝色通道为0 split(src, mv); mv[0] = Scalar(0); merge(mv, dst1); imshow("blue == 0", dst1); // 令绿色通道为0 split(src, mv); mv[1] = Scalar(0); merge(mv, dst2); imshow("green == 0", dst2); // 令红色通道为0 split(src, mv); mv[2] = Scalar(0); merge(mv, dst3); imshow("red == 0", dst3); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526import cv2 as cvsrc = cv.imread("../images/baboon.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# 蓝色通道为零mv = cv.split(src)mv[0][:, :] = 0dst1 = cv.merge(mv)cv.imshow("output1", dst1)# 绿色通道为零mv = cv.split(src)mv[1][:, :] = 0dst2 = cv.merge(mv)cv.imshow("output2", dst2)# 红色通道为零mv = cv.split(src)mv[2][:, :] = 0dst3 = cv.merge(mv)cv.imshow("output3", dst3)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像通道的分离与合并</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-007-图像像素之逻辑操作]]></title>
    <url>%2F2019%2F03%2F24%2Fopencv-007%2F</url>
    <content type="text"><![CDATA[知识点下面三个操作类似，都是针对两张图像的位操作 bitwise_and bitwise_xor bitwise_or 针对输入图像, 图像取反操作，二值图像分析中经常用 bitwise_not 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像像素的逻辑操作 */int main() &#123; // create image one, CV_8UC3创建三通道图像 Mat src1 = Mat::zeros(Size(400, 400), CV_8UC3); Rect rect(100,100,100,100); // Scalar() 参数为BGR三通道值，绿色和红色加起来是黄色 src1(rect) = Scalar(0, 255, 255); imshow("input1", src1); // create image two Mat src2 = Mat::zeros(Size(400, 400), CV_8UC3); rect.x = 150; rect.y = 150; src2(rect) = Scalar(0, 0, 255); imshow("input2", src2); // 逻辑操作 Mat dst1, dst2, dst3; bitwise_and(src1, src2, dst1); bitwise_xor(src1, src2, dst2); bitwise_or(src1, src2, dst3); imshow("and", dst1); imshow("xor", dst2); imshow("or", dst3); // 演示取反操作 Mat src = imread("../images/test1.jpg"); Mat dst; imshow("input", src); bitwise_not(src,dst); imshow("not", dst); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526272829import cv2 as cvimport numpy as np# create image onesrc1 = np.zeros(shape=[400, 400, 3], dtype=np.uint8)src1[100:200, 100:200, 1] = 255src1[100:200, 100:200, 2] = 255cv.imshow("input1", src1)# create image twosrc2 = np.zeros(shape=[400, 400, 3], dtype=np.uint8)src2[150:250, 150:250, 2] = 255cv.imshow("input2", src2)dst1 = cv.bitwise_and(src1, src2)dst2 = cv.bitwise_xor(src1, src2)dst3 = cv.bitwise_or(src1, src2)cv.imshow("dst1", dst1)cv.imshow("dst2", dst2)cv.imshow("dst3", dst3)src = cv.imread("../images/test1.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)dst = cv.bitwise_not(src)cv.imshow("dst", dst)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像像素逻辑操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生成假数据用于卷积神经网络模型训练]]></title>
    <url>%2F2019%2F03%2F24%2Fget_faked_data%2F</url>
    <content type="text"><![CDATA[背景在设计神经网络时，用于测试的imageNet等数据集太大，所以生成假数据用来测试神经网络能不能正常运行 代码123456789101112131415161718192021import tensorflow as tf# 参数设置batch_size = 32image_size = 24image_channel = 3n_classes = 10# 生成假数据用于训练模型def get_faked_train_batch(batch_size): images = tf.Variable(tf.random_normal(shape=[batch_size, image_size, image_size, image_channel], mean=0.0, stddev=1.0, dtype=tf.float32)) # tf.random_uniform() 标准均匀分布 labels = tf.Variable(tf.random_uniform(shape=[batch_size], minval=0, maxval=n_classes, dtype=tf.int32)) return images, labels # 生成假数据用于测试模型def get_faked_test_batch(batch_size): images = tf.Variable(tf.random_normal(shape=[batch_size, image_size, image_size, image_channel], mean=0.0, stddev=1.0, dtype=tf.float32)) # tf.random_uniform() 标准均匀分布 labels = tf.Variable(tf.random_uniform(shape=[batch_size], minval=0, maxval=n_classes, dtype=tf.int32)) return images, labels]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>卷积神经网络假数据生成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-006-Look Up Table(LUT)查找表的使用]]></title>
    <url>%2F2019%2F03%2F23%2Fopencv-006%2F</url>
    <content type="text"><![CDATA[知识点LUT查找表的简单原理 LUT查找表的作用 颜色匹配，比如讲灰度图像进行伪彩色增强 加快计算速度 API：applyColorMap(src, dst, COLORMAP) src 表示输入图像 dst表示输出图像 匹配到的颜色LUT， OpenCV支持13种颜色风格的查找表映射 COLORMAP ：13种色彩风格 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace std;using namespace cv;// 自定义LUTMat &amp;myColorMap(Mat &amp;image);/* * Look Up Table(LUT)查找表的使用 */int main() &#123; Mat src = imread("../images/LinuxLogo.jpg"); imshow("input", src); // 使用LUT Mat dst; applyColorMap(src, dst, COLORMAP_SUMMER); imshow("colorMap", dst); // 使用自己的LUT Mat my_dst, gray; cvtColor(src, gray, COLOR_BGR2GRAY); my_dst = myColorMap(gray); imshow("my_dst", my_dst); waitKey(0); return 0;&#125;// 自定义LUTMat &amp;myColorMap(Mat &amp;image) &#123; int lut[256]; for (int i = 0; i &lt; 256; ++i) &#123; if (i &lt; 127) lut[i] = 0; else lut[i] = 255; &#125; for (int row = 0; row &lt; image.rows; ++row) &#123; for (int col = 0; col &lt; image.cols; ++col) &#123; int pv = image.at&lt;uchar&gt;(row, col); image.at&lt;uchar&gt;(row, col) = lut[pv]; &#125; &#125; return image;&#125; 12345678910import cv2 as cvsrc = cv.imread("../images/LinuxLogo.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)dst = cv.applyColorMap(src, cv.COLORMAP_COOL)cv.imshow("output", dst)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>查找表（LUT）</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-005-像素算术操作]]></title>
    <url>%2F2019%2F03%2F23%2Fopencv-005%2F</url>
    <content type="text"><![CDATA[知识点像素算术操作 加add、减subtract、乘multiply、除divide saturate_cast&lt;T&gt;(value) # 类型转换注意点：图像的数据类型、通道数目、大小必须相同 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace std;using namespace cv;/* * 图像像素的加减乘除，两张图像大小类型要完全相同 */ int main()&#123; Mat src1 = imread("../images/opencv_images/LinuxLogo.jpg"); Mat src2 = imread("../images/opencv_images/WindowsLogo.jpg"); if(src1.empty() || src2.empty())&#123; cout&lt;&lt;"conld not read image..."&lt;&lt;endl; return -1; &#125; imshow("input1", src1); imshow("input2", src2); // 加法 Mat add_result = Mat::zeros(src1.size(),src1.type()); add(src1, src2, add_result); imshow("add_result", add_result); // 带权重的加法，一般推荐使用这个 Mat add_weight_result = Mat::zeros(src1.size(),src1.type()); addWeighted(src1, 0.5, src2, (1.0 - 0.5), 0.0, add_weight_result); imshow("add_weight_result", add_weight_result); // 减法 Mat sub_result = Mat::zeros(src1.size(),src1.type()); subtract(src1, src2, sub_result); imshow("sub_result", sub_result); // 乘法 Mat mul_result = Mat::zeros(src1.size(),src1.type()); multiply(src1, src2, mul_result); imshow("mul_result", mul_result); // 除法 Mat div_result = Mat::zeros(src1.size(),src1.type()); divide(src1, src2, div_result); imshow("div_result", div_result); // 自己实现加法操作 int b1 = 0, g1 = 0, r1 = 0; int b2 = 0, g2 = 0, r2 = 0; int b = 0, g = 0, r = 0; Mat my_add_result = Mat::zeros(src1.size(), src1.type()); for (int row = 0; row &lt; src1.rows; ++row) &#123; for (int col = 0; col &lt; src1.cols; ++col) &#123; b1 = src1.at&lt;Vec3b&gt;(row, col)[0]; g1 = src1.at&lt;Vec3b&gt;(row, col)[1]; r1 = src1.at&lt;Vec3b&gt;(row, col)[2]; b2 = src2.at&lt;Vec3b&gt;(row, col)[0]; g2 = src2.at&lt;Vec3b&gt;(row, col)[1]; r2 = src2.at&lt;Vec3b&gt;(row, col)[2]; // b1:0~255,b2:0~255, b1+b2可能大于255，所以需要转换，通过saturate_cast&lt;uchar&gt;() my_add_result.at&lt;Vec3b&gt;(row, col)[0] = saturate_cast&lt;uchar&gt;(b1 + b2); my_add_result.at&lt;Vec3b&gt;(row, col)[1] = saturate_cast&lt;uchar&gt;(g1 + g2); my_add_result.at&lt;Vec3b&gt;(row, col)[2] = saturate_cast&lt;uchar&gt;(r1 + r2); &#125; &#125; imshow("my_add_result", my_add_result); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425262728import cv2 as cvimport numpy as npsrc1 = cv.imread("../images/opencv_images/LinuxLogo.jpg");src2 = cv.imread("../images/opencv_images/WindowsLogo.jpg");cv.imshow("input1", src1)cv.imshow("input2", src2)h, w, ch = src1.shapeprint("h , w, ch", h, w, ch)add_result = np.zeros(src1.shape, src1.dtype);cv.add(src1, src2, add_result);cv.imshow("add_result", add_result);sub_result = np.zeros(src1.shape, src1.dtype);cv.subtract(src1, src2, sub_result);cv.imshow("sub_result", sub_result);mul_result = np.zeros(src1.shape, src1.dtype);cv.multiply(src1, src2, mul_result);cv.imshow("mul_result", mul_result);div_result = np.zeros(src1.shape, src1.dtype);cv.divide(src1, src2, div_result);cv.imshow("div_result", div_result);cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像像素算术操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用tensorflow对cifar10数据集进行图像分类]]></title>
    <url>%2F2019%2F03%2F22%2Fcifar10%2F</url>
    <content type="text"><![CDATA[步骤 定义神经网络计算图 运行计算图 导包12345import tensorflow as tfimport osimport cifar10_input # tensorflow/modle模块中自带案例，可以去github下载import numpy as npos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' 设置算法超参数1234567891011learning_rate_init = 0.001l2loss_ratio = 0.001keep_prob = 0.7 #dropouttraining_epochs = 5batch_size = 100display_step = 100conv1_kernel_num = 64conv2_kernel_num = 64fc1_units_num = 256fc2_units_num = 128fc3_units_num = cifar10_input.NUM_CLASSES 数据集中输入图像的参数123456dataset_dir = './cifar10_data/'image_size = cifar10_input.IMAGE_SIZEimage_channel = 3n_classes = cifar10_input.NUM_CLASSESnum_examples_per_epoch_for_train = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAINnum_examples_per_epoch_for_eval = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL 得到每一批次的训练数据123456def get_distorted_train_batch(data_dir, batch_size): if not data_dir: raise ValueError('please supply a data_dir') data_dir = os.path.join(data_dir, 'cifar-10-batches-bin') images, labels = cifar10_input.distorted_inputs(data_dir=data_dir, batch_size=batch_size) return images, labels 得到每一批次的测试数据123456def get_undistorted_eval_batch(data_dir, eval_data, batch_size): if not data_dir: raise ValueError('please supply a data_dir') data_dir = os.path.join(data_dir, 'cifar-10-batches-bin') images, labels = cifar10_input.inputs(eval_data=eval_data, data_dir=data_dir, batch_size=batch_size) return images, labels 根据指定的维数返回初始化好的指定名称的权重 Variable12345678def WeightsVariable(shape, name_str='weights', stddev=0.1): # 单cpu initial = tf.truncated_normal(shape=shape, stddev=stddev, dtype=tf.float32) return tf.Variable(initial, dtype=tf.float32, name=name_str) # 多gpu # weights = tf.get_variable(name_str, shape=shape, dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer_conv2d()) # return weights 根据指定的维数返回初始化好的指定名称的权重 Variable123def BiasesVariable(shape, name_str='biases', init_value=0.0): initial = tf.constant(init_value, shape=shape) return tf.Variable(initial, dtype=tf.float32, name=name_str) 2维卷积层的封装（包含激活函数）1234567def Conv2d(x, W, b, stride=1, padding='SAME', activation=tf.nn.relu, act_name='relu'): with tf.name_scope('conv2d_bias'): y = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=padding) y = tf.nn.bias_add(y, b) with tf.name_scope(act_name): y = activation(y) return y 2维池化层pool的封装12def Pool2d(x, pool=tf.nn.max_pool, k=2, stride=2, padding='SAME'): return pool(x, ksize=[1, k, k, 1], strides=[1, stride, stride, 1], padding=padding) 全连接层的封装1234567def FullyConnected(x, W, b, activation=tf.nn.relu, act_name='relu'): with tf.name_scope('Wx_b'): y = tf.matmul(x, W) y = tf.add(y, b) with tf.name_scope(act_name): y = activation(y) return y 为每一层的激活输出添加汇总节点123def AddActivationSummary(x): tf.summary.histogram('/activations', x) tf.summary.scalar('/sparsity', tf.nn.zero_fraction(x)) # 稀疏性 为所有损失节点添加标量汇总操作12345678910def AddLossesSummary(losses): # 计算所有损失的滑动平均 loss_averages = tf.train.ExponentialMovingAverage(decay=0.9, name='avg') loss_averages_op = loss_averages.apply(losses) # 为所有损失及平滑处理的损失绑定标量汇总节点 for loss in losses: tf.summary.scalar(loss.op.name + '(raw)', loss) tf.summary.scalar(loss.op.name + '(avg)', loss_averages.average(loss)) return loss_averages_op 打印每一层输出张量的shape12def print_layers_shape(t): print(t.op.name, ' ', t.get_shape().as_list()) 前向推断过程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263def Inference(images_holder): # 第一个卷积层 with tf.name_scope('Conv2d_1'): weights = WeightsVariable(shape=[5, 5, image_channel, conv1_kernel_num], stddev=5e-2) biases = BiasesVariable(shape=[conv1_kernel_num]) conv1_out = Conv2d(images_holder, weights, biases) AddActivationSummary(conv1_out) print_layers_shape(conv1_out) # 第一个池化层 with tf.name_scope('Pool2d_1'): pool1_out = Pool2d(conv1_out, k=3, stride=2) # 第二个卷积层 with tf.name_scope('Conv2d_2'): weights = WeightsVariable(shape=[5, 5, conv1_kernel_num, conv2_kernel_num], stddev=5e-2) biases = BiasesVariable(shape=[conv2_kernel_num]) conv2_out = Conv2d(pool1_out, weights, biases) AddActivationSummary(conv2_out) # 第二个池化层 with tf.name_scope('Pool2d_2'): pool2_out = Pool2d(conv2_out, k=3, stride=2) # 将二维特征图变为一维特征向量 with tf.name_scope('FeatsReshape'): features = tf.reshape(pool2_out, [batch_size, -1]) feats_dim = features.get_shape()[1].value # 得到上一行 -1 所指代的值 # 第一个全连接层 with tf.name_scope('FC1_nonlinear'): weights = WeightsVariable(shape=[feats_dim, fc1_units_num], stddev=4e-2) biases = BiasesVariable(shape=[fc1_units_num], init_value=0.1) fc1_out = FullyConnected(features, weights, biases) AddActivationSummary(fc1_out) # 加入L2损失 with tf.name_scope('L2_loss'): weight_loss = tf.multiply(tf.nn.l2_loss(weights), l2loss_ratio, name='fc1_weight_loss') tf.add_to_collection('losses', weight_loss) # Dropout # with tf.name_scope('dropout_1'): # fc1_dropout = tf.nn.dropout(fc1_out, keep_prob=keep_prob) # 第二个全连接层 with tf.name_scope('FC2_nonlinear'): weights = WeightsVariable(shape=[fc1_units_num, fc2_units_num], stddev=4e-2) biases = BiasesVariable(shape=[fc2_units_num], init_value=0.1) fc2_out = FullyConnected(fc1_out, weights, biases) AddActivationSummary(fc2_out) # 加入L2损失 with tf.name_scope('L2_loss'): weight_loss = tf.multiply(tf.nn.l2_loss(weights), l2loss_ratio, name='fc2_weight_loss') tf.add_to_collection('losses', weight_loss) # 第三个全连接层 with tf.name_scope('FC3_linear'): weights = WeightsVariable(shape=[fc2_units_num, fc3_units_num], stddev=1.0/fc2_units_num) biases = BiasesVariable(shape=[fc3_units_num]) logits = FullyConnected(fc2_out, weights, biases, activation=tf.identity, act_name='linear') AddActivationSummary(logits) return logits 调用上面写的函数构造计算图，并设计会话流程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111def TrainModel(): with tf.Graph().as_default(): # 计算图输入 with tf.name_scope('Inputs'): images_holder = tf.placeholder(tf.float32, [batch_size, image_size, image_size, image_channel], name='images') labels_holder = tf.placeholder(tf.int32, [batch_size], name='labels') # 计算图前向推断过程 with tf.name_scope('Inference'): logits = Inference(images_holder) # 定义损失层 with tf.name_scope('Loss'): cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_holder, logits=logits) cross_entropy_loss = tf.reduce_mean(cross_entropy, name='xentropy_loss') tf.add_to_collection('losses', cross_entropy_loss) # 总损失 = 交叉熵损失 + L2损失 total_loss = tf.add_n(tf.get_collection('losses'), name='total_loss') average_losses = AddLossesSummary(tf.get_collection('losses') + [total_loss]) # 定义优化训练层 with tf.name_scope('Train'): learning_rate = tf.placeholder(tf.float32) global_step = tf.Variable(0, name='global_step', trainable=False, dtype=tf.int64) optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate) train_op = optimizer.minimize(total_loss, global_step=global_step) # 定义模型评估层 with tf.name_scope('Evaluate'): top_K_op = tf.nn.in_top_k(predictions=logits, targets=labels_holder, k=1) # 定义获取训练样本批次的节点 with tf.name_scope('GetTrainBatch'): images_train, labels_train = get_distorted_train_batch(data_dir=dataset_dir, batch_size=batch_size) # 定义获取测试样本批次的节点 with tf.name_scope('GetTestBatch'): images_test, labels_test = get_undistorted_eval_batch(eval_data=True, data_dir=dataset_dir, batch_size=batch_size) # 收集所有汇总节点 merged_summaries = tf.summary.merge_all() # 添加所有变量的初始化节点 init_op = tf.global_variables_initializer() print("把计算图写入事件文件...") # graph_writer = tf.summary.FileWriter(logdir='events/', graph=tf.get_default_graph()) # graph_writer.close() summary_writer = tf.summary.FileWriter(logdir='events/') summary_writer.add_graph(graph=tf.get_default_graph()) summary_writer.flush() with tf.Session() as sess: sess.run(init_op) print('==&gt;&gt;&gt;&gt;&gt;&gt;&gt;==开始在训练集上训练模型==&lt;&lt;&lt;&lt;&lt;&lt;&lt;==') total_batches = int(num_examples_per_epoch_for_train / batch_size) print("per batch size: ", batch_size) print("train sample count per epoch:", num_examples_per_epoch_for_train) print("total batch count per epoch:", total_batches) # 启动数据读取队列 tf.train.start_queue_runners() # 记录模型被训练的步数 training_step = 0 # 训练指定轮数，每一轮的训练样本总数为：num_examples_per_epoch_for_train for epoch in range(training_epochs): # 每一轮都要把所有的batch跑一遍 for batch_idx in range(total_batches): # 运行获取批次训练数据的计算图，取出一个批次数据 images_batch, labels_batch = sess.run([images_train, labels_train]) # 运行优化器训练节点 _, loss_value, avg_losses= sess.run([train_op, total_loss, average_losses], feed_dict=&#123;images_holder:images_batch, labels_holder:labels_batch, learning_rate:learning_rate_init&#125;) # 每调用一次训练节点，training_step就加1，最终 == training_epochs * total_batch training_step = sess.run(global_step) # 每训练display_step次，计算当前模型的损失和分类准确率 if training_step % display_step == 0: # 运行Evaluate节点，计算当前批次的训练样本的准确率 predictions = sess.run([top_K_op], feed_dict=&#123;images_holder:images_batch, labels_holder:labels_batch&#125;) # 计算当前批次的预测正确样本量 batch_accuracy = np.sum(predictions) / batch_size print("train step: " + str(training_step) + ", train loss= " + "&#123;:.6f&#125;".format(loss_value) + ", train accuracy=" + "&#123;:.5f&#125;".format(batch_accuracy)) # 运行汇总节点 summaries_str = sess.run(merged_summaries, feed_dict= &#123;images_holder: images_batch, labels_holder: labels_batch&#125;) summary_writer.add_summary(summary=summaries_str, global_step=training_step) summary_writer.flush() summary_writer.close() print("训练完毕！") print('==&gt;&gt;&gt;&gt;&gt;&gt;&gt;==开始在测试集上评估模型==&lt;&lt;&lt;&lt;&lt;&lt;&lt;==') total_batches = int(num_examples_per_epoch_for_eval / batch_size) total_examples = total_batches * batch_size # 当除不尽batch_size时，num_examples_per_epoch_for_evalv ！= total_examples print("per batch size: ", batch_size) print("test sample count per epoch:", total_examples) print("total batch count per epoch:", total_batches) correc_predicted = 0 for test_step in range(total_batches): # 运行获取批次测试数据的计算图，取出一个批次数据 images_batch, labels_batch = sess.run([images_test, labels_test]) # 运行Evaluate节点，计算当前批次的训练样本的准确率 predictions = sess.run([top_K_op], feed_dict=&#123;images_holder:images_batch, labels_holder:labels_batch&#125;) # 累计每个批次的预测正确样本量 correc_predicted += np.sum(predictions) accuracy_score = correc_predicted / total_examples print("--------&gt;accuracy on test examples: ",accuracy_score) 123456def main(argv=None): train_dir = './events/' if tf.gfile.Exists(train_dir): tf.gfile.DeleteRecursively(train_dir) tf.gfile.MakeDirs(train_dir) TrainModel() 12if __name__ == '__main__': tf.app.run() 结果 训练结果 测试结果 Tensorboard 中查看 代码地址github中没有上传cifar10数据集，需要的话请从百度云下载，或自行下载，按照如下解压 github 百度云 提取码：xw3x]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>图像分类</tag>
        <tag>tensorflow</tag>
        <tag>cifar10</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-004-图像像素读写操作]]></title>
    <url>%2F2019%2F03%2F21%2Fopencv-004%2F</url>
    <content type="text"><![CDATA[知识点 C++中的像素遍历与访问 数组遍历 指针方式遍历 Python中的像素遍历与访问 数组遍历 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace std;using namespace cv;/** * 读取图像，实现像素反转 */int main() &#123; Mat src = imread("../images/liuyifei_1.png"); Mat src_copy = src.clone(); int height = src.rows; int width = src.cols; int ch = src.channels(); imshow("input", src); // 直接读取图像像素 for (int row = 0; row &lt; height; ++row) &#123; for (int col = 0; col &lt; width; ++col) &#123; if (ch == 3) &#123; Vec3b bgr = src.at&lt;Vec3b&gt;(row, col); bgr[0] = 255 - bgr[0]; bgr[1] = 255 - bgr[1]; bgr[2] = 255 - bgr[2]; src.at&lt;Vec3b&gt;(row, col) = bgr; &#125; else if (ch == 1) &#123; int gray = src.at&lt;uchar&gt;(row, col); src.at&lt;uchar&gt;(row, col) = 255 - gray; &#125; &#125; &#125; imshow("output1", src); // 指针读取 Mat result = Mat::zeros(src_copy.size(), src_copy.type()); int blue = 0, green = 0, red = 0; int gray; for (int row = 0; row &lt; height; ++row) &#123; // curr_row为第row行的首地址，遍历时，前三个字节表示的是第一个像素的BGR值， // 注意BGR值顺序，接下来三个字节是第二个像素的值。 uchar *curr_row = src_copy.ptr&lt;uchar&gt;(row); uchar *result_row = result.ptr&lt;uchar&gt;(row); for (int col = 0; col &lt; width; ++col) &#123; if (ch == 3) &#123; blue = *curr_row++; green = *curr_row++; red = *curr_row++; *result_row++ = 255 - blue; *result_row++ = 255 - green; *result_row++ = 255 - red; &#125; else if (ch == 1) &#123; gray = *curr_row++; *result_row++ = gray; &#125; &#125; &#125; imshow("output2", result); waitKey(0); return 0;&#125; 123456789101112131415161718import cv2 as cvsrc = cv.imread("../images/liuyifei_1.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)h, w, ch = src.shapeprint("h , w, ch", h, w, ch)for row in range(h): for col in range(w): b, g, r = src[row, col] b = 255 - b g = 255 - g r = 255 - r src[row, col] = [b, g, r]cv.imshow("output", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像像素读写</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-003-图像对象(Mat)创建与赋值]]></title>
    <url>%2F2019%2F03%2F21%2Fopencv-003%2F</url>
    <content type="text"><![CDATA[知识点 C++中Mat对象与创建 Python中Numpy数组对象 代码（c++,python）123456789101112131415161718192021222324252627282930#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace std;using namespace cv;int main()&#123; Mat src = imread("../images/liuyifei_1.png"); // 通过克隆或复制创建图像对象，m1和src指向不同内存块 Mat m1 = src.clone(); Mat m2; src.copyTo(m2); // 赋值法，m3和src指向同一内存块 Mat m3 = src; // 创建空白图像 Mat m4 = Mat::zeros(src.size(),src.type()); Mat m5 = Mat::zeros(Size(512,512),CV_8UC3); Mat m6 = Mat::ones(Size(512,512),CV_8UC3); // kernel: [0, -1, 0 // -1, 5, -1 // 0, -1, 0] Mat kernel = (Mat_&lt;char&gt;(3,3)&lt;&lt;0,-1,0,-1,5,-1,0,-1,0); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425262728import cv2 as cvimport numpy as npsrc = cv.imread("../images/liuyifei_1.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# 克隆图像m1 = np.copy(src)# 赋值m2 = srcsrc[100:200,200:300,:] = 255 # 第三维代表图像通道cv.imshow("m2",m2)m3 = np.zeros(src.shape, src.dtype)cv.imshow("m3", m3)m4 = np.zeros([512,512], np.uint8)# m4[:,:] =127 try to give gray value 127cv.imshow("m4", m4)m5 = np.ones(shape=[512,512,3], dtype=np.uint8)m5[:,:,0] = 255cv.imshow("m5", m5)cv.waitKey(0)cv.destroyAllWindows() 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>Mat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-002-色彩空间转换(cvtcolor)与图像保存(imwrite)]]></title>
    <url>%2F2019%2F03%2F20%2Fopencv-002%2F</url>
    <content type="text"><![CDATA[知识点 色彩空间转换函数- cvtColor COLOR_BGR2GRAY = 6 彩色到灰度 COLOR_GRAY2BGR = 8 灰度到彩色 COLOR_BGR2HSV = 40 BGR到HSV COLOR_HSV2BGR = 54 HSV到 BGR 图像保存 - imwrite 第一个参数是图像保存路径 第二个参数是图像内存对象 代码（c++,python）1234567891011121314151617181920212223242526#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace std;using namespace cv;int main()&#123; Mat src = imread("../images/liuyifei_1.png"); if (src.empty())&#123; cout &lt;&lt; "could not load image..." &lt;&lt; endl; return -1; &#125; namedWindow("input"); imshow("input",src); Mat dst; cvtColor(src,dst,COLOR_BGR2GRAY); imwrite("../images/result1.png",dst); namedWindow("output gray"); imshow("output gray",dst); waitKey(0); return 0;&#125; 123456789import cv2 as cvsrc = cv.imread("../images/liuyifei_1.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)cv.imshow("gray", gray)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>色彩空间转换(cvtcolor)</tag>
        <tag>图像保存(imwrite)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Clion无法读取相对路径文件或图像的解决方法]]></title>
    <url>%2F2019%2F03%2F20%2FClion_path_problem%2F</url>
    <content type="text"><![CDATA[项目目录 相对路径错误写法12// opencv读取图像，此时无法读取Mat image = imread("images/liuyifei_1.png") 解决方案 1 - 使用绝对路径1Mat image = imread("D:\\code-workspace\\Clion-workspace\\learnOpencv\\images\\liuyifei_1.png") 解决方案 2 - 返回根目录1Mat image = imread("../images/liuyifei_1.png") 解决方案 3 - 设置项目工作目录 设置项目工作目录 代码如下 12// 此时读取成功Mat image = imread("images/liuyifei_1.png")]]></content>
      <tags>
        <tag>Clion</tag>
        <tag>相对路径问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-001-读取(imread)与显示(imshow)图像]]></title>
    <url>%2F2019%2F03%2F20%2Fopencv-001%2F</url>
    <content type="text"><![CDATA[知识点 读取图像 - imread() 显示图像 - imshow() 代码（c++,python）123456789101112131415161718192021#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace std;using namespace cv;int main() &#123; // Mat image = imread("../images/liuyifei_1.png"); // 读取的时候加参数，使读取后为灰度图像 Mat image = imread("../images/liuyifei_1.png",IMREAD_GRAYSCALE); if (image.empty()) &#123; cout &lt;&lt; "could not load image..." &lt;&lt; endl; return -1; &#125; namedWindow("input"); imshow("input",image); waitKey(0); return 0;&#125; 1234567import cv2 as cvsrc = cv.imread("../images/liuyifei_1.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>读取并显示图像</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pip配置阿里云镜像]]></title>
    <url>%2F2019%2F03%2F19%2Fpip_windows_aliyun%2F</url>
    <content type="text"><![CDATA[windows新建pip配置文件夹 在windows “文件资源管理器” 地址栏输入%APPDATA% 按回车，创建pip文件夹，用于存放pip配置文件 在pip文件夹中新建名为：pip.ini 的配置文件 在pip.ini中输入以下内容 123[global]trusted-host = mirrors.aliyun.comindex-url = https://mirrors.aliyun.com/pypi/simple linux新建.pip文件夹 1mkdir .pip 新建pip.conf文件 12cd .piptouch pip.conf 在pip.conf中输入以下内容 1vim pip.conf 123[global]trusted-host = mirrors.aliyun.comindex-url = https://mirrors.aliyun.com/pypi/simple]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>pip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VS2017配置opencv]]></title>
    <url>%2F2019%2F03%2F19%2Fopencv_vs2017%2F</url>
    <content type="text"><![CDATA[安装 opencv 下载地址 ：opencv download 解压到 opencv4文件夹中 解压后： 配置环境变量： VS2017中配置opencv 新建一个工程 依次点击：视图 ==&gt; 其他窗口 ==&gt; 属性管理器 添加包含目录 添加库目录 添加附加依赖项 重启VS2017 测试 测试代码 1234567891011121314#include &lt;opencv2\opencv.hpp&gt;using namespace cv;int main()&#123; Mat img = imread("1.png"); namedWindow("hahaha"); imshow("hahaha", img); waitKey(0); return 0;&#125; 测试结果]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>VS2017</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows下Clion配置opencv]]></title>
    <url>%2F2019%2F03%2F19%2Fopencv_Clion%2F</url>
    <content type="text"><![CDATA[所需环境MinGw + Cmake + Clion + opencv 安装MinGw参考：install MinGw 安装Cmake参考：install Cmake Cmake下载网址：Cmake download 注：Cmake最好安装跟Clion中配置一样的版本，省的麻烦 安装 opencv 下载地址 ：opencv download 解压到 opencv4文件夹中 解压后： 配置环境变量： Clion 配置 编译opencv源码 打开Cmake-GUI，选择源码路径和输出路径 点击Configure，选择MinGW Makefiles，点击Finish，开始编译 等待一段时间后，会有很多报红，再次点击Configure，红色消失，点击Generate 进入输出目录，在cmd 运行下面代码，等待完成 1mingw32-make -j8 运行mingw32-make install，等待片刻，输出目录下会多出install文件夹 添加…\install\x64\mingw\bin 添加到path系统环境变量环境变量 编辑CMakeLists.txt1234567891011121314151617cmake_minimum_required(VERSION 3.13)project(learnOpencv)set(CMAKE_CXX_STANDARD 11)# Where to find CMake modules and OpenCVset(OpenCV_DIR "D:\\software\\opencv4\\MinGW64_build\\install")set(CMAKE_MODULE_PATH $&#123;CMAKE_MODULE_PATH&#125; "$&#123;CMAKE_SOURCE_DIR&#125;/cmake/")find_package(OpenCV REQUIRED)include_directories($&#123;OpenCV_INCLUDE_DIRS&#125;)add_executable(learnOpencv test.cpp)# add libs you needset(OpenCV_LIBS opencv_core opencv_imgproc opencv_highgui opencv_imgcodecs)# linkingtarget_link_libraries(learnOpencv $&#123;OpenCV_LIBS&#125;) 注意：opencv4必须要c++11支持 测试12345678910111213#include &lt;opencv2\opencv.hpp&gt;using namespace cv;int main()&#123; Mat img = imread("D:\\code-workspace\\Clion-workspace\\learnOpencv\\images\\1.png",WINDOW_AUTOSIZE); namedWindow("刘亦菲"); imshow("刘亦菲", img); waitKey(0); return 0;&#125;]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>Clion</tag>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python安装opencv]]></title>
    <url>%2F2019%2F03%2F19%2Fopencv_python%2F</url>
    <content type="text"><![CDATA[安装opencv123456# opencv-python 和 opencv-contrib-python只能安装一个，后者带有扩展包，建议直接安后者pip install opencv-python# 安装opencv-contrib-python前，要先卸载opencv-pythonpip uninstall opencv-pythonpip install opencv-contrib-python 更新opencv1pip install --upgrade opencv-python]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装jupyter notebook插件]]></title>
    <url>%2F2019%2F03%2F02%2FjupyterPlugin%2F</url>
    <content type="text"><![CDATA[步骤12python -m pip install jupyter_contrib_nbextensionsjupyter contrib nbextension install --user --skip-running-check Autopep8 –&gt; 格式化代码]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>jupyter notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用keras用常规神经网络训练MNIST数据集]]></title>
    <url>%2F2019%2F03%2F02%2FmnistLinearNN%2F</url>
    <content type="text"><![CDATA[加载mnist数据集123456from keras.datasets import mnist(x_train,y_train),(x_test,y_test) = mnist.load_data() # 将下载好的mnist.npz方在 ~/.keras/datasets/ 目录下print(x_train.shape,type(x_train))print(y_train.shape,type(y_train))print(x_test.shape,type(x_test))print(y_test.shape,type(y_test)) (60000, 28, 28) &lt;class &apos;numpy.ndarray&apos;&gt; (60000,) &lt;class &apos;numpy.ndarray&apos;&gt; (10000, 28, 28) &lt;class &apos;numpy.ndarray&apos;&gt; (10000,) &lt;class &apos;numpy.ndarray&apos;&gt; 数据处理：规范化1234# 将图形从[28,28]变为[784,]X_train = x_train.reshape(60000,784)X_test = x_test.reshape(10000,784)print(X_train.shape,X_test.shape) (60000, 784) (10000, 784) 123456# 将数据转换为float32，为了进行归一化，不然/255得到全部是0X_train = X_train.astype('float32')X_test = X_test.astype('float32')# 数据归一化X_train /= 255X_test /= 255 统计训练数据中个标签数量12345import numpy as npimport matplotlib.pyplot as pltlabel, count = np.unique(y_train, return_counts=True)print(label, count) [0 1 2 3 4 5 6 7 8 9] [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949] 123456789101112fig = plt.figure(figsize=(8, 5))plt.bar(label, count, width=0.7, align='center')plt.title("Label Distribution")plt.xlabel('Label')plt.ylabel('Count')plt.xticks(label)plt.ylim(0, 7500)for a, b in zip(label, count): plt.text(a, b, '%d' % b, ha='center', va='bottom', fontsize=10)plt.show() 对标签进行one-hot编码123456789101112131415161718192021# import tensorflow as tf# n_classes = 10# Y_train = tf.one_hot(y_train, n_classes)# Y_test = tf.one_hot(y_test, n_classes)# with tf.Session() as sess:# sess.run(tf.global_variables_initializer())# Y_train=sess.run(Y_train)# Y_test=sess.run(Y_test)# print(Y_train.shape)# 下面代码同上，使用tensorflow需要建立会话，简单转换keras更方便from keras.utils import np_utilsn_classes = 10Y_train = np_utils.to_categorical(y_train,n_classes)Y_test = np_utils.to_categorical(y_test,n_classes)print(Y_train.shape) (60000, 10) 12print(y_train[0])print(Y_train[0]) 5 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] 使用Keras sequential model 定义神经网络1234567891011121314# 使用keras定义线性网络很方便from keras.models import Sequentialfrom keras.layers.core import Dense, Activationmodel = Sequential()# 第一隐藏层model.add(Dense(512, input_shape=(784,)))model.add(Activation('relu'))# 第二隐藏层model.add(Dense(512))model.add(Activation('relu'))# 输出层model.add(Dense(10))model.add(Activation('softmax')) 编译模型1model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy']) 训练模型，并将指标保存到history中12history = model.fit(X_train, Y_train, batch_size=128, epochs=5, verbose=2, validation_data=(X_test, Y_test)) Train on 60000 samples, validate on 10000 samples Epoch 1/5 - 7s - loss: 0.2156 - acc: 0.9373 - val_loss: 0.0970 - val_acc: 0.9710 Epoch 2/5 - 7s - loss: 0.0804 - acc: 0.9758 - val_loss: 0.0769 - val_acc: 0.9770 Epoch 3/5 - 7s - loss: 0.0504 - acc: 0.9838 - val_loss: 0.0791 - val_acc: 0.9746 Epoch 4/5 - 7s - loss: 0.0350 - acc: 0.9891 - val_loss: 0.0659 - val_acc: 0.9804 Epoch 5/5 - 8s - loss: 0.0264 - acc: 0.9913 - val_loss: 0.0734 - val_acc: 0.9794 可视化指标12345678910111213141516171819fig = plt.figure()plt.subplot(211)plt.plot(history.history['acc'])plt.plot(history.history['val_acc'])plt.title('Model Accuracy')plt.xlabel('epoch')plt.ylabel('accuracy')plt.legend(['train','test'])plt.subplot(212)plt.plot(history.history['loss'])plt.plot(history.history['val_loss'])plt.title('Model Loss')plt.xlabel('epoch')plt.ylabel('loss')plt.legend(['train','test'])plt.tight_layout()plt.show() 保存模型123456789101112import osimport tensorflow.gfile as gfilesave_dir = '.\model'if gfile.Exists(save_dir): gfile.DeleteRecursively(save_dir)gfile.MakeDirs(save_dir)model_name = 'keras_mnist.h5'model_path = os.path.join(save_dir,model_name)model.save(model_path)print('Saved trained model at %s' % model_path) Saved trained model at .\model\keras_mnist.h5 加载模型123from keras.models import load_modelmnist_model = load_model(model_path) 统计模型在测试集上的分类结果123456789loss_and_metrics = mnist_model.evaluate(X_test, Y_test, verbose=2)print("Test Loss: &#123;&#125;".format(loss_and_metrics[0]))print("Test Accuracy: &#123;&#125;%".format(loss_and_metrics[1]*100))predicted_classes = mnist_model.predict_classes(X_test)correct_indices = np.nonzero(predicted_classes == y_test)[0]incorrect_indices = np.nonzero(predicted_classes != y_test)[0]print("Classified correctly count: &#123;&#125;".format(len(correct_indices)))print("Classified incorrectly count: &#123;&#125;".format(len(incorrect_indices))) Test Loss: 0.07340353026344673 Test Accuracy: 97.94% Classified correctly count: 9794 Classified incorrectly count: 206 代码地址github]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>mnist</tag>
        <tag>keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用keras用卷积神经网络训练MNIST数据集]]></title>
    <url>%2F2019%2F03%2F02%2FmnistCNN%2F</url>
    <content type="text"><![CDATA[加载mnist数据集123456from keras.datasets import mnist(x_train,y_train),(x_test,y_test) = mnist.load_data() # 将下载好的mnist.npz方在 ~/.keras/datasets/ 目录下print(x_train.shape,type(x_train))print(y_train.shape,type(y_train))print(x_test.shape,type(x_test))print(y_test.shape,type(y_test)) (60000, 28, 28) &lt;class &apos;numpy.ndarray&apos;&gt; (60000,) &lt;class &apos;numpy.ndarray&apos;&gt; (10000, 28, 28) &lt;class &apos;numpy.ndarray&apos;&gt; (10000,) &lt;class &apos;numpy.ndarray&apos;&gt; 数据处理：规范化channels_last对应的输入：(batch,height,width,channels) channels_first对应的输入：(batch,channels,height,width) 默认channels_last，修改：~/.keras/keras.json 123456789101112131415from keras import backend as Kimg_rows, img_cols = 28, 28if K.image_data_format() == 'channels_first': x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols) x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols) input_shape = (1, img_rows, img_cols)else: x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1) x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1) input_shape = (img_rows, img_cols, 1)print(x_train.shape, type(x_train))print(x_test.shape, type(x_test)) (60000, 28, 28, 1) &lt;class &apos;numpy.ndarray&apos;&gt; (10000, 28, 28, 1) &lt;class &apos;numpy.ndarray&apos;&gt; 123456# 将数据转换为float32，为了进行归一化，不然/255得到全部是0X_train = x_train.astype('float32')X_test = x_test.astype('float32')# 数据归一化X_train /= 255X_test /= 255 统计训练数据中个标签数量12345import numpy as npimport matplotlib.pyplot as pltlabel, count = np.unique(y_train, return_counts=True)print(label, count) [0 1 2 3 4 5 6 7 8 9] [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949] 123456789101112fig = plt.figure(figsize=(8, 5))plt.bar(label, count, width=0.7, align='center')plt.title("Label Distribution")plt.xlabel('Label')plt.ylabel('Count')plt.xticks(label)plt.ylim(0, 7500)for a, b in zip(label, count): plt.text(a, b, '%d' % b, ha='center', va='bottom', fontsize=10)plt.show() 对标签进行one-hot编码1234567from keras.utils import np_utilsn_classes = 10Y_train = np_utils.to_categorical(y_train,n_classes)Y_test = np_utils.to_categorical(y_test,n_classes)print(Y_train.shape) (60000, 10) 12print(y_train[0])print(Y_train[0]) 5 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] 使用Keras sequential model 定义MNIST CNN网络123456789101112131415161718192021222324from keras.models import Sequentialfrom keras.layers import Dense, Dropout, Flattenfrom keras.layers import Conv2D, MaxPooling2Dmodel = Sequential()## Feature Extraction# 第一层卷积，32个3*3的卷积核，激活函数使用relumodel.add(Conv2D(filters=32,kernel_size=(3,3),activation='relu',input_shape=input_shape))# 第二层卷积，64个3*3的卷积核，激活函数使用relumodel.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu'))# 最大池化层model.add(MaxPooling2D(pool_size=(2,2)))# Dropout 25% 的输入神经元model.add(Dropout(0.25))# 将Pooled feature map 摊平后输入全连接网络model.add(Flatten())## Classification# 全连接层model.add(Dense(128,activation='relu'))# Dropout 50% 的输入神经元model.add(Dropout(0.5))# 使用softmax 激活函数做多分类，输出各数字的概率model.add(Dense(10, activation='softmax')) 查看 MNIST CNN 模型网络结构1model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 26, 26, 32) 320 _________________________________________________________________ conv2d_2 (Conv2D) (None, 24, 24, 64) 18496 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 12, 12, 64) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 9216) 0 _________________________________________________________________ dense_1 (Dense) (None, 128) 1179776 _________________________________________________________________ dropout_2 (Dropout) (None, 128) 0 _________________________________________________________________ dense_2 (Dense) (None, 10) 1290 ================================================================= Total params: 1,199,882 Trainable params: 1,199,882 Non-trainable params: 0 _________________________________________________________________ 12for layer in model.layers: print(layer.get_output_at(0).get_shape().as_list()) [None, 26, 26, 32] [None, 24, 24, 64] [None, 12, 12, 64] [None, 12, 12, 64] [None, None] [None, 128] [None, 128] [None, 10] 编译模型1model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy']) 训练模型，并将指标保存到history中1history = model.fit(X_train, Y_train, batch_size=128, epochs=5,verbose=2, validation_data=(X_test, Y_test)) Train on 60000 samples, validate on 10000 samples Epoch 1/5 - 131s - loss: 0.2330 - acc: 0.9290 - val_loss: 0.0540 - val_acc: 0.9817 Epoch 2/5 - 146s - loss: 0.0853 - acc: 0.9747 - val_loss: 0.0372 - val_acc: 0.9882 Epoch 3/5 - 136s - loss: 0.0605 - acc: 0.9812 - val_loss: 0.0315 - val_acc: 0.9898 Epoch 4/5 - 129s - loss: 0.0514 - acc: 0.9843 - val_loss: 0.0283 - val_acc: 0.9913 Epoch 5/5 - 130s - loss: 0.0416 - acc: 0.9873 - val_loss: 0.0272 - val_acc: 0.9911 可视化指标12345678910111213141516171819fig = plt.figure()plt.subplot(211)plt.plot(history.history['acc'])plt.plot(history.history['val_acc'])plt.title('Model Accuracy')plt.xlabel('epoch')plt.ylabel('accuracy')plt.legend(['train','test'])plt.subplot(212)plt.plot(history.history['loss'])plt.plot(history.history['val_loss'])plt.title('Model Loss')plt.xlabel('epoch')plt.ylabel('loss')plt.legend(['train','test'])plt.tight_layout()plt.show() 保存模型123456789101112import osimport tensorflow.gfile as gfilesave_dir = '.\model'if gfile.Exists(save_dir): gfile.DeleteRecursively(save_dir)gfile.MakeDirs(save_dir)model_name = 'keras_mnist.h5'model_path = os.path.join(save_dir,model_name)model.save(model_path)print('Saved trained model at %s' % model_path) Saved trained model at .\model\keras_mnist.h5 加载模型123from keras.models import load_modelmnist_model = load_model(model_path) 统计模型在测试集上的分类结果123456789loss_and_metrics = mnist_model.evaluate(X_test, Y_test, verbose=2)print("Test Loss: &#123;&#125;".format(loss_and_metrics[0]))print("Test Accuracy: &#123;&#125;%".format(loss_and_metrics[1]*100))predicted_classes = mnist_model.predict_classes(X_test)correct_indices = np.nonzero(predicted_classes == y_test)[0]incorrect_indices = np.nonzero(predicted_classes != y_test)[0]print("Classified correctly count: &#123;&#125;".format(len(correct_indices)))print("Classified incorrectly count: &#123;&#125;".format(len(incorrect_indices))) Test Loss: 0.027159390095694836 Test Accuracy: 99.11% Classified correctly count: 9911 Classified incorrectly count: 89 代码地址github]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>mnist</tag>
        <tag>keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv+tensorflow实时检测人脸]]></title>
    <url>%2F2018%2F12%2F29%2FfaceDetection%2F</url>
    <content type="text"><![CDATA[关于人脸检测的说明本文代码使用了opencv自带的人脸检测算法和mtcnn算法，mtcnn有明显的优势，检测成功率基本维持在100%，而且人脸各角度都可以检测成功，所以建议使用mtcnn来进行人脸检测，电脑cpu也可以流畅运行。 需要提前配置的环境：python + opencv + tensorflow 关于mtcnn的介绍，请参见压缩包中的电子书 代码结构说明 detect_face.py定义了mtcnn模型 det 1-3.npy是预训练好的模型，所以不用再对mtcnn进行训练 detect 1-3.py是三种实现方式，下面一一介绍 代码演示detect1.py使用mtcnn对一张图片进行检测，效果如下： detect2.py使用opencv自带的HAAR进行实时人脸检测，当人脸倾斜时无法检测到，效果如下： detect3.py使用MTCNN进行实时人脸检测，无论人脸各个角度，都可以检测到，效果如下： 代码地址github地址 百度云地址 注意：github地址中没有mtcnn的预训练模型，需要自己下载，百度云是完整的]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>人脸检测</tag>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下查看端口占用进程及杀死进程]]></title>
    <url>%2F2018%2F12%2F08%2Flinux-kill-process%2F</url>
    <content type="text"><![CDATA[直接查看进程1ps 通过端口查看进程1lsof –i:端口号 杀死进程1kill -9 pid号]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>查看linux进程并杀死</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下关于screen命令的使用]]></title>
    <url>%2F2018%2F12%2F08%2Flinux-screen%2F</url>
    <content type="text"><![CDATA[因为进入服务器只有一个窗口，当我们用这个窗口跑代码时，就没有办法同时用命令编辑一些文件。为了解决这个问题，我们可以使用screen开启多个进程，用一个进程跑代码，然后将这个窗口折叠到后台，创建新的进程来编辑代码。 当我们想要断开服务器连接仍然让一些程序运行的时候，可以使用screen让程序在后台一直运行。 安装screen (ubuntu系统)1sudo apt-get install screen 创建进程1screen -S 进程名 之后，会进入一个干净的窗口，可以执行相应操作，连续按Ctrl+A、Ctrl+D回到主线程，之前执行的操作会一直在后台运行，直到杀死该进程。 这条命令可以多次使用，创建多个进程。 查看当前screen进程1screen -ls 进入某一进程123#两条命令选其一screen -r 进程名screen -r 进程pid号 终止进程12345#方法一screen -X -S 进程名 quit#方法二先进入要杀死的进程，然后输入exit]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux下screen的使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux添加用户并赋予sudo权限]]></title>
    <url>%2F2018%2F11%2F29%2FaddLinuxUser%2F</url>
    <content type="text"><![CDATA[创建用户123# 在root用户下不用写sudosudo adduser fanfan # 在/home 下会自动创建同名文件夹passwd fanfan # 设置密码，上个命令有时会直接让输入密码，就不需要执行这一步了 删除用户1sudo userdel fanfan 添加sudo权限 su -切换到root vim /etc/sudoers ，在root ALL=(ALL) ALL的下一行添加： 12345# sudo时需要输入密码fanfan ALL=(ALL) ALL# sudo时不需要输入密码fanfan ALL=(ALL) NOPASSWD: ALL 按Esc，再输入:wq!保存文件，要加!，不然保存会出问题]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux添加用户</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[本地访问服务器端jupyter notebook]]></title>
    <url>%2F2018%2F11%2F29%2FremoteJupyter%2F</url>
    <content type="text"><![CDATA[1 登陆远程服务器2 生成配置文件1$ jupyter notebook --generate-config 3 生成密码打开ipython，创建一个密文的密码12345In [1]: from notebook.auth import passwdIn [2]: passwd()Enter password: Verify password: Out[2]: 'sha1:ce23d945972f:34769685a7ccd3d08c84a18c63968a41f1140274' 把生成的密文‘sha:ce…’复制下来 4 修改默认配置文件1$ vim ~/.jupyter/jupyter_notebook_config.py 进行如下修改：1234c.NotebookApp.ip='*'c.NotebookApp.password = u'sha:ce...刚才复制的那个密文'c.NotebookApp.open_browser = Falsec.NotebookApp.port =8888 #随便指定一个端口 5 启动jupyter notebook1$ jupyter notebook 6 远程访问此时应该可以直接从本地浏览器直接访问http://address_of_remote:8888就可以看到jupyter的登陆界面，输入第三步中设置的密码。 7 建立SSH通道如果登陆失败，则有可能是服务器防火墙设置的问题，此时最简单的方法是在本地建立一个ssh通道：在本地终端中输入：12ssh fanfan@222.92.146.251 -L127.0.0.1:1234:127.0.0.1:6666ssh fanfan@47.106.208.254 -L127.0.0.1:1234:127.0.0.1:8888 便可以在localhost:1234直接访问远程的jupyter了。]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>jupyter notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器端安装Anaconda]]></title>
    <url>%2F2018%2F11%2F29%2FinstallAnaconda%2F</url>
    <content type="text"><![CDATA[步骤打开网址：Anaconda清华镜像，复制要下载的文件地址，执行以下命令：12345678910wget 复制的网址（会下载一个sh文件）sh sh文件名 #执行后，会显示使用条款，按enter继续阅读，会让回答几个问题，全部yesrm -rf sh文件名source ~/.bashrc （使conda生效）#设置清华conda镜像conda config --prepend channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ 注意事项 Anaconda3-5.2.0.Linux-x86_64.sh ==&gt; python3.6 Anaconda3-5.3.1.Linux-x86_64.sh ==&gt; python3.7 若wget显示网络不可达，执行以下操作： 123456#centossudo yum -y install wget#ubuntusudo apt-get updatesudo apt-get install wget 若不能运行jupyter notebook，进行如下配置：jupyter notebook配置]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>Anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux终端bash美化教程]]></title>
    <url>%2F2018%2F11%2F29%2FbeautifyBash%2F</url>
    <content type="text"><![CDATA[美化步骤12345vim .bashrc添加下行export PS1="Time:\[\033[1;35m\]\T \[\033[0m\]User:\[\033[1;33m\]\u \[\033[0m\]Dir:\[\033[1;32m\]\w\[\033[0m\]\n\$"退出vimsource .bashrc 美化效果 PS1中参数的具体含义参考链接]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo写博客步骤]]></title>
    <url>%2F2018%2F11%2F29%2FwriteArtical%2F</url>
    <content type="text"><![CDATA[博客编写步骤1 进入D:\Blog文件夹下，打开终端 2 输入：hexo new &quot;文件名&quot;，在D:\Blog\source\\_posts目录下创建了文件名.md文件 3 打开文件名.md，编写博客 4 终端输入：hexo d -g提交博客 md文件编写注意事项1234567---title: 博客名categories: 分类名tags: - 标签1 - 标签2--- 更新博客分类与标签页面12hexo cleanhexo d -g]]></content>
      <tags>
        <tag>Hexo发送文章</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简易安卓聊天软件之思路架构及源码]]></title>
    <url>%2F2018%2F11%2F04%2FweChat%2F</url>
    <content type="text"><![CDATA[安卓聊天软件完成的功能罗列1 登陆2 动态显示好友列表3 服务端程序4 客户端程序5 安全退出6 与多好友聊天，屏幕切换，可以保存原信息，每次新登陆，可以读取历史记录7 缓存消息，及离线完成 软件架构图 本地Sqlite数据库设计只有一张表，存储聊天消息，表中有三个属性，分别为：发送者 接收者 消息内容 客户端与服务端传输消息协议约定： 客户端新上线的时候，向服务端发送用户名，服务端向客户端发送好友列表与离线消息 客户端 ==&gt; 服务端：发送者：接收者：消息 服务端 ==&gt; 客户端：发送者：接收者：消息 服务端向客户端发送的是消息还是好友列表，以开头是否是”&amp;”符号区分 客户端目录结构（Android Studio） 客户端的基本思路Service负责与服务器进行网络连接与IO读写，无论是发送消息还是接受消息，Service都先把消息存到本地数据库，FriendListActivity与ChatActivity中ListView的显示，都是直接从数据库读取数据。Service与Activity的通信主要使用Intent和广播来进行。 服务端程序服务端基本思路（具体代码见文末源码地址）： 使用一个List存储所有好友 使用一个Map存储在线好友及对应Socket 使用一个Map存储离线消息 软件开发经验总结这次软件开发是以小组形式进行的，最后算是完成了聊天软件的基本功能，这次开发做的好的地方在于一开始小组就先把真个架构图设计好了，包括数据库，后面写代码基本很顺畅，得到的经验就是开发一个软件，做一个项目，写代码真的是很靠后的事情了，前期一定是先通过写用例，梳理好逻辑，画好架构图，后期按照梳理好的逻辑来写代码。后期还需要努力的地方在于UML类图，希望下次开发前期能把UML类图画出来，这样前期工作会更完善，加油，希望可以成为一个专业的程序员。 源码地址源码：github地址 注意：源码中的ImServeFinal.java文件时服务端程序，应该拿出来用java的IDE运行，记得更改ip与端口]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>java</tag>
      </tags>
  </entry>
</search>
