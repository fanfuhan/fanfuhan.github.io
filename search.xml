<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Windows下配置VS Code通过remote ssh进行远程开发]]></title>
    <url>%2F2019%2F07%2F14%2Fvscode_remote_ssh%2F</url>
    <content type="text"><![CDATA[VS Code 的远程开发功能需要通过密钥方式与服务器相连，所以需要先在本地生成密钥对，将公钥上传到服务器，再安装remote ssh 插件，进行远程开发。 1 在本机生成SSH密钥对1ssh-keygen -t rsa 一路回车，密钥对将生成到默认位置 C:\Users\&lt;用户名&gt;\.ssh\，如下图： 2 上传公钥到服务器服务器输入以下命令： 12$ mkdir ~/.ssh &amp;&amp; touch ~/.ssh/authorized_keys$ chmod 700 ~/.ssh &amp;&amp; chmod 600 ~/.ssh/authorized_keys 将id_rsa.pub 上传到服务器的 ~/.ssh 文件夹下，服务器输入以下命令： 1$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 3 本地配置 config 文件简化登陆本地打开C:\Users\&lt;用户名&gt;\.ssh\config 文件，输入以下内容： 1234Host &lt;name-you-want&gt; # ssh连接别名 HostName &lt;server-ip-address&gt; # 服务器ip User &lt;username&gt; # 服务器登陆用户名 PubkeyAuthentication yes 此时，本地可以通过设置的别名连接，如下图，lab 是我设置的别名： 4 VS Code远程配置4.1 安装Remote Development插件 4.2 SSH连接 此时会跳出新的窗口，等待一会，连接成功，如下图： 4.3 打开服务器文件，进行开发 4.4 开发插件配置 不得不提的是，VS Code配置远程开发后，服务器要单独安装需要的插件： 5 总结VS Code 的远程开发功能很强大，但连接以后非常卡顿，基本不能使用。]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>VS Code进行远程开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow、CUDA以及Cudnn版本对应关系]]></title>
    <url>%2F2019%2F07%2F14%2Ftensorflow_cuda_cudnn_version%2F</url>
    <content type="text"><![CDATA[参考博客tensorflow、CUDA以及Cudnn版本对应关系]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>版本对应关系</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch利用torchsummary实现类似keras的model.summary()输出模型信息]]></title>
    <url>%2F2019%2F06%2F22%2Fpytorch_torchsummary%2F</url>
    <content type="text"><![CDATA[详见链接torchsummary 代码123456789101112131415161718192021import torchimport torch.nn as nnfrom torchsummary import summaryclass SimpleConv(nn.Module): def __init__(self): super(SimpleConv, self).__init__() self.features = nn.Sequential( nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=1), nn.ReLU(), ) def forward(self, x, y): x1 = self.features(x) x2 = self.features(y) return x1, x2 device = torch.device("cuda" if torch.cuda.is_available() else "cpu")model = SimpleConv().to(device)summary(model, [(1, 16, 16), (1, 28, 28)]) 结果1234567891011121314151617---------------------------------------------------------------- Layer (type) Output Shape Param #================================================================ Conv2d-1 [-1, 1, 16, 16] 10 ReLU-2 [-1, 1, 16, 16] 0 Conv2d-3 [-1, 1, 28, 28] 10 ReLU-4 [-1, 1, 28, 28] 0================================================================Total params: 20Trainable params: 20Non-trainable params: 0----------------------------------------------------------------Input size (MB): 0.77Forward/backward pass size (MB): 0.02Params size (MB): 0.00Estimated Total Size (MB): 0.78----------------------------------------------------------------]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch中的一些技巧-统计参数数量，划分数据集，初始化，正则化]]></title>
    <url>%2F2019%2F06%2F17%2Fpytorch_tricks_init_L1L2%2F</url>
    <content type="text"><![CDATA[统计参数总数量1num_params = sum(param.numel() for param in model.parameters()) 划分训练集和验证集123train_size = int(0.8 * len(full_dataset))test_size = len(full_dataset) - train_sizetrain_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size]) 参数初始化Xavier 初始化 Xavier初始化的基本思想是保持输入和输出的方差一致，这样就避免了所有输出值都趋向于0。这是通用的方法，适用于任何激活函数。 123for m in model.modules(): if isinstance(m, (nn.Conv2d, nn.Linear)): nn.init.xavier_uniform(m.weight) He 初始化 He 初始化的思想是：在ReLU网络中，假定每一层有一半的神经元被激活，另一半为0。推荐在ReLU网络中使用。 123for m in model.modules(): if isinstance(m, (nn.Conv2d, nn.Linear)): nn.init.kaiming_normal(m.weight, mode=\\'fan_in\\') 正则化一般正则化，只是对模型的权重W参数进行惩罚，而偏置b是不进行惩罚的，如果对权值w和偏置b同时进行惩罚，将会导致严重的欠拟合。 L2 正则化 torch.optim 的优化器固定实现L2正则化，也只实现了L2正则化，若想实现L1正则化，见下。而torch.optim 的优化器weight_decay参数指定的权值衰减是对网络中的所有参数，包括 权值w 和 偏置b 同时进行惩罚，需要特意设置一下，只对 权值w进行惩罚。 123456789101112weight_p, bias_p = [],[]for name, p in model.named_parameters(): if 'bias' in name: bias_p += [p] else: weight_p += [p]# 这里的model中每个参数的名字都是系统自动命名的，只要是权值都是带有weight，偏置都带有biasoptim.SGD([ &#123;'params': weight_p, 'weight_decay':1e-5&#125;, &#123;'params': bias_p, 'weight_decay':0&#125; ], lr=1e-2, momentum=0.9) 另一种实现： 12345678reg = 1e-6l2_loss = Variable(torch.FloatTensor(1), requires_grad=True)for name, param in model.named_parameters(): if 'bias' not in name: l2_loss = l2_loss + 0.5 * reg * torch.sum(torch.pow(param, 2)))classify_loss = criteon(outputs, labels)loss = classify_loss + l2_loss L1 正则化 12345678reg = 1e-6l1_loss = Variable(torch.FloatTensor(1), requires_grad=True)for name, param in model.named_parameters(): if 'bias' not in name: l1_loss = l1_loss + reg * torch.sum(torch.abs(param)))classify_loss = criteon(outputs, labels)loss = classify_loss + l1_loss]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用tensorwatch对pytorch网络可视化及统计参数]]></title>
    <url>%2F2019%2F06%2F16%2Fpytorch_tensorwatch%2F</url>
    <content type="text"><![CDATA[代码12345678import sysimport torchimport tensorwatch as twimport torchvision.modelsalexnet_model = torchvision.models.alexnet()tw.draw_model(alexnet_model, [1, 3, 224, 224])tw.model_stats(alexnet_model, [1, 3, 224, 224]) 结果]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>tensorwatch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch实现迁移学习]]></title>
    <url>%2F2019%2F06%2F16%2Fpytorch_trans_learning%2F</url>
    <content type="text"><![CDATA[两个主要的迁移学习场景： Finetuning the convnet: 我们使用预训练网络初始化网络，而不是随机初始化，就像在imagenet 1000数据集上训练的网络一样。其余训练看起来像往常一样。 ConvNet as fixed feature extractor: 在这里，我们将冻结除最终完全连接层之外的所有网络的权重。最后一个全连接层被替换为具有随机权重的新层，并且仅训练该层。 参考链接：1 https://pytorch.apachecn.org/docs/1.0/transfer_learning_tutorial.html 2 https://pytorch.apachecn.org/docs/1.0/finetuning_torchvision_models_tutorial.html 导包123456789101112131415161718from __future__ import print_function, divisionimport torchimport torch.nn as nnimport torch.optim as optimfrom torch.optim import lr_schedulerimport numpy as npimport torchvisionfrom torchvision import datasets, models, transformsimport matplotlib.pyplot as pltimport timeimport osimport copyplt.ion() # interactive modeos.environ["CUDA_VISIBLE_DEVICES"] = "9"device = torch.device("cuda" if torch.cuda.is_available() else "cpu") 加载数据我们将使用 torchvision 和 torch.utils.data 包来加载数据。 我们今天要解决的问题是训练一个模型来对 蚂蚁 和 蜜蜂 进行分类。我们有大约120个训练图像，每个图像用于 蚂蚁 和 蜜蜂。每个类有75个验证图像。通常，如果从头开始训练，这是一个非常小的数据集。由于我们正在使用迁移学习，我们应该能够合理地推广。 该数据集是 imagenet 的一个非常小的子集。 注意 从 此处 下载数据并将其解压缩到当前目录。 12345678910111213141516171819202122232425262728# Data augmentation and normalization for training# Just normalization for validationdata_transform = &#123; 'train': transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), 'val': transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])&#125;data_dir = 'hymenoptera_data/'image_datasets = &#123;x: datasets.ImageFolder(os.path.join(data_dir, x), data_transform[x]) for x in ['train', 'val']&#125;dataloaders = &#123;x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val']&#125;dataset_sizes = &#123;x: len(image_datasets[x]) for x in ['train', 'val']&#125;class_names = image_datasets['train'].classesprint(dataset_sizes)print(class_names) {&apos;train&apos;: 244, &apos;val&apos;: 153} [&apos;ants&apos;, &apos;bees&apos;] 可视化图像12345678910111213141516171819def imshow(inp, title=None): """Imshow for Tensor""" inp = inp.numpy().transpose((1,2,0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = inp * std + mean inp = np.clip(inp, 0, 1) plt.imshow(inp) if title is not None: plt.title(title) # plt.pause(0.001) # get a batch of training datainputs, classes = next(iter(dataloaders['train']))# make a grid from batchout = torchvision.utils.make_grid(inputs)imshow(out, title=[class_names[x] for x in classes]) 训练模型的通用函数函数有以下功能： 训练模型 调整学习率 保存最佳的学习模型 函数中, scheduler 参数是 torch.optim.lr_scheduler 中的 LR scheduler 对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869def train_model(model, criterion, optimizer, scheduler, num_epochs=25): since = time.time() val_acc_history = [] best_model_wts = copy.deepcopy(model.state_dict()) best_acc = 0.0 for epoch in range(num_epochs): print('Epoch &#123;&#125;/&#123;&#125;'.format(epoch + 1, num_epochs)) print('-'*10) # each epoch has a train and valid phase for phase in ['train', 'val']: if phase == 'train': scheduler.step() model.train() # Set model to training mode else: model.eval() # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) # zero the parameter gradients optimizer.zero_grad() # forward # track history if only in train with torch.set_grad_enabled(phase == 'train'): outputs = model(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) # backward + optimize only if in training phase if phase == 'train': loss.backward() optimizer.step() # statistics running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects.double() / dataset_sizes[phase] print('&#123;&#125; Loss: &#123;:.4f&#125; Acc: &#123;:.4f&#125;'.format(phase, epoch_loss, epoch_acc)) # deep copy the model if phase == 'val' and epoch_acc &gt; best_acc: best_acc = epoch_acc best_model_wts = copy.deepcopy(model.state_dict()) if phase == 'val': val_acc_history.append(epoch_acc) print() time_elapsed = time.time() - since print('Training complete in &#123;:.0f&#125;m &#123;:.0f&#125;s'.format(time_elapsed // 60, time_elapsed % 60)) print('Best val Acc: &#123;:.4f&#125;'.format(best_acc)) # load best model weights model.load_state_dict(best_model_wts) return model, val_acc_history 可视化模型函数用于显示少量图像预测的通用功能 123456789101112131415161718192021222324252627282930def visualize_model(model, num_images=6): was_training = model.training model.eval() images_so_far = 0 fig = plt.figure() with torch.no_grad(): for i, (inputs, labels) in enumerate(dataloaders['val']): inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) _, preds = torch.max(outputs, 1) for j in range(inputs.size()[0]): images_so_far += 1 ax = plt.subplot(num_images//2, 2, images_so_far) plt.tight_layout() ax.axis('off') ax.set_title('true: &#123;&#125; ==&gt; predicted: &#123;&#125;' .format(class_names[labels[j]], class_names[preds[j]])) imshow(inputs.cpu().data[j]) if images_so_far == num_images: model.train() return model.train(model=was_training) # 似乎有点问题 微调卷积网络加载预训练模型并重置最终的全连接层 12345678910111213model_ft = models.resnet18(pretrained=True)num_ftrs = model_ft.fc.in_featuresmodel_ft.fc = nn.Linear(num_ftrs, 2)model_ft = model_ft.to(device)criterion = nn.CrossEntropyLoss()# Observe that all parameters are being optimizedoptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)# Decay LR by a factor of 0.1 every 7 epochsexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) 训练和评估1model_ft, val_acc_history = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler) Epoch 1/25 ---------- train Loss: 0.6349 Acc: 0.7049 val Loss: 0.5844 Acc: 0.7451 ... Epoch 25/25 ---------- train Loss: 0.3083 Acc: 0.8484 val Loss: 0.2406 Acc: 0.9085 Training complete in 29m 38s Best val Acc: 0.9346 1visualize_model(model_ft) ConvNet 作为固定特征提取器在这里，我们需要冻结除最后一层之外的所有网络。我们需要设置 requires_grad == False 冻结参数，以便在 backward() 中不计算梯度。 123456789101112131415161718model_conv = models.resnet18(pretrained=True)for param in model_conv.parameters(): param.requires_grad = False# Parameters of newly constructed modules have requires_grad=True by defaultnum_ftrs = model_conv.fc.in_featuresmodel_conv.fc = nn.Linear(num_ftrs, 2)model_conv = model_conv.to(device)criterion = nn.CrossEntropyLoss()# Observe that only parameters of final layer are being optimized as# opposed to before.optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)# Decay LR by a factor of 0.1 every 7 epochsexp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1) 训练和评估在CPU上，与前一个场景相比，这将花费大约一半的时间。这是预期的，因为不需要为大多数网络计算梯度。但是，前向传递需要计算梯度。 12model_conv, val_acc_hietory = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=10) Epoch 1/10 ---------- train Loss: 0.6322 Acc: 0.6148 val Loss: 0.2261 Acc: 0.9346 ... Epoch 10/10 ---------- train Loss: 0.4004 Acc: 0.8197 val Loss: 0.1821 Acc: 0.9477 Training complete in 6m 5s Best val Acc: 0.9477 1234567891011hist = [h.cpu().numpy() for h in val_acc_hietory]plt.figure()plt.title("Validation Accuracy")plt.xlabel("Training Epochs")plt.ylabel("Validation Accuracy")plt.ylim((0,1.))plt.xticks(np.arange(1, 11, 1.0))plt.plot(np.arange(1,11),hist,label="Pretrained")plt.legend() 1234visualize_model(model_conv)plt.ioff()plt.show()]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>迁移学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib-动态更新显示内容]]></title>
    <url>%2F2019%2F06%2F11%2Fmatplotlib-dynamic_update%2F</url>
    <content type="text"><![CDATA[知识点在matplotlib中画图有两种显示模式： （1）阻塞模式，即必须利用plt.show()显示图片，且图片关闭之前代码将阻塞在该行。 （2）交互模式，即plt.plot()后立马显示图片，且不阻塞代码的继续运行。 Matplotlib中默认是使用阻塞模式。用到的matplotlib中的几个函数： 12345plt.ion()：打开交互模式plt.ioff()：关闭交互模式plt.clf()：清除当前的Figure对象plt.cla()：清除当前的Axes对象plt.pause()：暂停功能 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import numpy as npimport matplotlibimport matplotlib.pyplot as pltimport matplotlib.font_manager as fmfrom mpl_toolkits.mplot3d import Axes3D# 解决中文乱码问题myfont = fm.FontProperties(fname="simsun.ttc", size=14)matplotlib.rcParams["axes.unicode_minus"] = Falsedef simple_plot(): """ simple plot """ # 生成画布 plt.figure(figsize=(8, 6), dpi=80) # 打开交互模式 plt.ion() # 循环 for index in range(10): # 清除原有图像 plt.cla() # 设定标题等 plt.title("动态曲线图", fontproperties=myfont) # 网格线 plt.grid(True) # 生成测试数据 x = np.linspace(-np.pi + 0.1*index, np.pi+0.1*index, 256, endpoint=True) y_cos, y_sin = np.cos(x), np.sin(x) # 设置X轴 plt.xlabel("X轴", fontproperties=myfont) plt.xlim(-4 + 0.1*index, 4 + 0.1*index) plt.xticks(np.linspace(-4 + 0.1*index, 4+0.1*index, 9, endpoint=True)) # 设置Y轴 plt.ylabel("Y轴", fontproperties=myfont) plt.ylim(-1.0, 1.0) plt.yticks(np.linspace(-1, 1, 9, endpoint=True)) # 画两条曲线 plt.plot(x, y_cos, "b--", linewidth=2.0, label="cos示例") plt.plot(x, y_sin, "g-", linewidth=2.0, label="sin示例") # 设置图例位置,loc可以为[upper, lower, left, right, center] plt.legend(loc="upper left", prop=myfont, shadow=True) # 暂停 plt.pause(0.1) # 关闭交互模式 plt.ioff() # 图形显示 plt.show() returnsimple_plot()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib-animation画动态图]]></title>
    <url>%2F2019%2F06%2F11%2Fmatplotlib-animation%2F</url>
    <content type="text"><![CDATA[详情请看莫烦老师教程莫烦python 代码12345678910111213141516171819202122232425262728293031323334import numpy as npfrom matplotlib import pyplot as pltfrom matplotlib import animationfig, ax = plt.subplots()x = np.arange(0, 2*np.pi, 0.01)line, = ax.plot(x, np.sin(x))def animate(i): line.set_ydata(np.sin(x + i / 10.0)) # update the data return line,# Init only required for blitting to give a clean slate.def init(): line.set_ydata(np.sin(x)) return line,# call the animator. blit=True means only re-draw the parts that have changed.# blit=True dose not work on Mac, set blit=False# interval= update frequencyani = animation.FuncAnimation(fig=fig, func=animate, frames=100, init_func=init, interval=20, blit=True)# save the animation as an mp4. This requires ffmpeg or mencoder to be# installed. The extra_args ensure that the x264 codec is used, so that# the video can be embedded in html5. You may need to adjust this for# your system: for more information, see# http://matplotlib.sourceforge.net/api/animation_api.html# anim.save('basic_animation.mp4', fps=30, extra_args=['-vcodec', 'libx264'])plt.show() 结果]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib-次坐标轴]]></title>
    <url>%2F2019%2F06%2F11%2Fmatplotlib-secondary_axis%2F</url>
    <content type="text"><![CDATA[详情请看莫烦老师教程莫烦python 代码123456789101112131415161718import matplotlib.pyplot as pltimport numpy as npx = np.arange(0, 10, 0.1)y1 = 0.05 * x**2y2 = -2 *y1fig, ax1 = plt.subplots()ax2 = ax1.twinx() # mirror the ax1ax1.plot(x, y1, 'g-')ax2.plot(x, y2, 'b-')ax1.set_xlabel('X data')ax1.set_ylabel('Y1 data', color='g')ax2.set_ylabel('Y2 data', color='b')plt.show() 结果]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib-图中图]]></title>
    <url>%2F2019%2F06%2F11%2Fmatplotlib-plot_in_plot%2F</url>
    <content type="text"><![CDATA[详情请看莫烦老师教程莫烦python 代码123456789101112131415161718192021222324252627282930import matplotlib.pyplot as pltfig = plt.figure()x = [1, 2, 3, 4, 5, 6, 7]y = [1, 3, 4, 2, 5, 8, 6]# below are all percentageleft, bottom, width, height = 0.1, 0.1, 0.8, 0.8ax1 = fig.add_axes([left, bottom, width, height]) # main axesax1.plot(x, y, 'r')ax1.set_xlabel('x')ax1.set_ylabel('y')ax1.set_title('title')ax2 = fig.add_axes([0.2, 0.6, 0.25, 0.25]) # inside axesax2.plot(y, x, 'b')ax2.set_xlabel('x')ax2.set_ylabel('y')ax2.set_title('title inside 1')# different method to add axes####################################plt.axes([0.6, 0.2, 0.25, 0.25])plt.plot(y[::-1], x, 'g')plt.xlabel('x')plt.ylabel('y')plt.title('title inside 2')plt.show() 结果]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib-subplot分格显示]]></title>
    <url>%2F2019%2F06%2F11%2Fmatplotlib-grid_subplot%2F</url>
    <content type="text"><![CDATA[详情请看莫烦老师教程莫烦python 代码1234567891011121314151617181920212223242526272829303132333435363738import matplotlib.pyplot as pltimport matplotlib.gridspec as gridspec# method 1: subplot2grid##########################plt.figure()plt.subplots_adjust(wspace = 0.4, hspace = 0.4) #调整子图间距ax1 = plt.subplot2grid((3, 3), (0, 0), colspan=3) # stands for axesax1.plot([1, 2], [1, 2])ax1.set_title('ax1_title')ax2 = plt.subplot2grid((3, 3), (1, 0), colspan=2)ax3 = plt.subplot2grid((3, 3), (1, 2), rowspan=2)ax4 = plt.subplot2grid((3, 3), (2, 0))ax4.scatter([1, 2], [2, 2])ax4.set_xlabel('ax4_x')ax4.set_ylabel('ax4_y')ax5 = plt.subplot2grid((3, 3), (2, 1))# method 2: gridspec#########################plt.figure()plt.subplots_adjust(wspace = 0.4, hspace = 0.4) #调整子图间距gs = gridspec.GridSpec(3, 3)# use index from 0ax6 = plt.subplot(gs[0, :])ax7 = plt.subplot(gs[1, :2])ax8 = plt.subplot(gs[1:, 2])ax9 = plt.subplot(gs[-1, 0])ax10 = plt.subplot(gs[-1, -2])# method 3: easy to define structure####################################f, ((ax11, ax12), (ax13, ax14)) = plt.subplots(2, 2, sharex=True, sharey=True)ax11.scatter([1,2], [1,2])plt.subplots_adjust(wspace = 1, hspace = 0.4) #调整子图间距plt.tight_layout()plt.show() 结果]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib-subplot多合一显示]]></title>
    <url>%2F2019%2F06%2F11%2Fmatplotlib-subplot%2F</url>
    <content type="text"><![CDATA[详情请看莫烦老师教程莫烦python 代码12345678910111213141516171819202122232425262728293031323334353637383940414243import matplotlib.pyplot as plt# example 1:###############################plt.figure(figsize=(6, 4))# plt.subplot(n_rows, n_cols, plot_num)plt.subplot(2, 2, 1)plt.plot([0, 1], [0, 1])plt.subplot(222)plt.plot([0, 1], [0, 2])plt.subplot(223)plt.plot([0, 1], [0, 3])plt.subplot(224)plt.plot([0, 1], [0, 4])plt.tight_layout()# example 2:###############################plt.figure(figsize=(6, 4))# plt.subplot(n_rows, n_cols, plot_num)plt.subplot(2, 1, 1)# figure splits into 2 rows, 1 col, plot to the 1st sub-figplt.plot([0, 1], [0, 1])plt.subplot(234)# figure splits into 2 rows, 3 col, plot to the 4th sub-figplt.plot([0, 1], [0, 2])plt.subplot(235)# figure splits into 2 rows, 3 col, plot to the 5th sub-figplt.plot([0, 1], [0, 3])plt.subplot(236)# figure splits into 2 rows, 3 col, plot to the 6th sub-figplt.plot([0, 1], [0, 4])plt.tight_layout()plt.show() 结果]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib-3D显示]]></title>
    <url>%2F2019%2F06%2F11%2Fmatplotlib-3d%2F</url>
    <content type="text"><![CDATA[详情请看莫烦老师教程莫烦python 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import numpy as npimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dfig = plt.figure()ax = Axes3D(fig)# X, Y valueX = np.arange(-4, 4, 0.25)Y = np.arange(-4, 4, 0.25)X, Y = np.meshgrid(X, Y)R = np.sqrt(X ** 2 + Y ** 2)# height valueZ = np.sin(R)ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=plt.get_cmap('rainbow'))"""============= ================================================ Argument Description ============= ================================================ *X*, *Y*, *Z* Data values as 2D arrays *rstride* Array row stride (step size), defaults to 10 *cstride* Array column stride (step size), defaults to 10 *color* Color of the surface patches *cmap* A colormap for the surface patches. *facecolors* Face colors for the individual patches *norm* An instance of Normalize to map values to colors *vmin* Minimum value to map *vmax* Maximum value to map *shade* Whether to shade the facecolors ============= ================================================"""# I think this is different from plt12_contoursax.contourf(X, Y, Z, zdir='z', offset=-2, cmap=plt.get_cmap('rainbow'))"""========== ================================================ Argument Description ========== ================================================ *X*, *Y*, Data values as numpy.arrays *Z* *zdir* The direction to use: x, y or z (default) *offset* If specified plot a projection of the filled contour on this position in plane normal to zdir ========== ================================================"""ax.set_zlim(-2, 2)plt.show() 结果]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib-图像]]></title>
    <url>%2F2019%2F06%2F10%2Fmatplotlib-image%2F</url>
    <content type="text"><![CDATA[详情请看莫烦老师教程莫烦python 代码1234567891011121314151617181920import matplotlib.pyplot as pltimport numpy as np# image dataa = np.array([0.313660827978, 0.365348418405, 0.423733120134, 0.365348418405, 0.439599930621, 0.525083754405, 0.423733120134, 0.525083754405, 0.651536351379]).reshape(3,3)"""for the value of "interpolation", check this:http://matplotlib.org/examples/images_contours_and_fields/interpolation_methods.htmlfor the value of "origin"= ['upper', 'lower'], check this:http://matplotlib.org/examples/pylab_examples/image_origin.html"""plt.imshow(a, interpolation='nearest', cmap='bone', origin='upper')plt.colorbar(shrink=.9)plt.xticks(())plt.yticks(())plt.show() 结果]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib-柱状图]]></title>
    <url>%2F2019%2F06%2F10%2Fmatplotlib-bar%2F</url>
    <content type="text"><![CDATA[详情请看莫烦老师教程莫烦python 代码123456789101112131415161718192021222324252627import matplotlib.pyplot as pltimport numpy as npn = 12X = np.arange(n)Y1 = (1 - X / float(n)) * np.random.uniform(0.5, 1.0, n)Y2 = (1 - X / float(n)) * np.random.uniform(0.5, 1.0, n)plt.bar(X, +Y1, facecolor='#9999ff', edgecolor='white')plt.bar(X, -Y2, facecolor='#ff9999', edgecolor='white')for x, y in zip(X, Y1): # ha: horizontal alignment # va: vertical alignment plt.text(x, y + 0.05, '%.2f' % y, ha='center', va='bottom')for x, y in zip(X, Y2): # ha: horizontal alignment # va: vertical alignment plt.text(x, -y - 0.05, '-%.2f' % y, ha='center', va='top')plt.xlim(-.5, n)plt.xticks(())plt.ylim(-1.25, 1.25)plt.yticks(())plt.show() 结果]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib-散点图]]></title>
    <url>%2F2019%2F06%2F10%2Fmatplotlib-scatter%2F</url>
    <content type="text"><![CDATA[详情请看莫烦老师教程莫烦python 代码12345678910111213141516import matplotlib.pyplot as pltimport numpy as npn = 1024 # data sizeX = np.random.normal(0, 1, n)Y = np.random.normal(0, 1, n)T = np.arctan2(Y, X) # for color later onplt.scatter(X, Y, s=75, c=T, alpha=.5)plt.xlim(-1.5, 1.5)plt.xticks(()) # ignore xticksplt.ylim(-1.5, 1.5)plt.yticks(()) # ignore yticksplt.show() 结果]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib-tick能见度]]></title>
    <url>%2F2019%2F06%2F10%2Fmatplotlib-tick_visibility%2F</url>
    <content type="text"><![CDATA[详情请看莫烦老师教程莫烦python 代码123456789101112131415161718192021222324import matplotlib.pyplot as pltimport numpy as npx = np.linspace(-3, 3, 50)y = 0.1*xplt.figure()plt.plot(x, y, linewidth=10, zorder=1) # set zorder for ordering the plot in plt 2.0.2 or higherplt.ylim(-2, 2)ax = plt.gca()ax.spines['right'].set_color('none')ax.spines['top'].set_color('none')ax.spines['top'].set_color('none')ax.xaxis.set_ticks_position('bottom')ax.spines['bottom'].set_position(('data', 0))ax.yaxis.set_ticks_position('left')ax.spines['left'].set_position(('data', 0))for label in ax.get_xticklabels() + ax.get_yticklabels(): label.set_fontsize(12) # set zorder for ordering the plot in plt 2.0.2 or higher label.set_bbox(dict(facecolor='white', edgecolor='none', alpha=0.8, zorder=2))plt.show() 结果]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib-对图像进行标注]]></title>
    <url>%2F2019%2F06%2F10%2Fmatplotlib-annotation%2F</url>
    <content type="text"><![CDATA[详情请看莫烦老师教程莫烦python 代码1234567891011121314151617181920212223242526272829303132333435import matplotlib.pyplot as pltimport numpy as npx = np.linspace(-3, 3, 50)y = 2*x + 1plt.figure(num=1, figsize=(8, 5),)plt.plot(x, y,)ax = plt.gca()ax.spines['right'].set_color('none')ax.spines['top'].set_color('none')ax.spines['top'].set_color('none')ax.xaxis.set_ticks_position('bottom')ax.spines['bottom'].set_position(('data', 0))ax.yaxis.set_ticks_position('left')ax.spines['left'].set_position(('data', 0))x0 = 1y0 = 2*x0 + 1plt.plot([x0, x0,], [0, y0,], 'k--', linewidth=2.5)plt.scatter([x0, ], [y0, ], s=50, color='b')# method 1:#####################plt.annotate(r'$2x+1=%s$' % y0, xy=(x0, y0), xycoords='data', xytext=(+30, -30), textcoords='offset points', fontsize=16, arrowprops=dict(arrowstyle='-&gt;', connectionstyle="arc3,rad=.2"))# method 2:########################plt.text(-3.7, 3, r'$This\ is\ the\ some\ text. \mu\ \sigma_i\ \alpha_t$', fontdict=&#123;'size': 16, 'color': 'r'&#125;)plt.show() 结果]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib-图例]]></title>
    <url>%2F2019%2F06%2F10%2Fmatplotlib-legend%2F</url>
    <content type="text"><![CDATA[详情请看莫烦老师教程莫烦python 代码12345678910111213141516171819202122232425262728293031323334import matplotlib.pyplot as pltimport numpy as npx = np.linspace(-3, 3, 50)y1 = 2*x + 1y2 = x**2plt.figure()l1, = plt.plot(x, y1, label='linear line')l2, = plt.plot(x, y2, color='red', linewidth=1.0, linestyle='--', label='square line')plt.legend(loc='best')# plt.legend(handles=[l1, l2], labels=['up', 'down'], loc='best')# the "," is very important in here l1, = plt... and l2, = plt... for this step"""legend( handles=(line1, line2, line3), labels=('label1', 'label2', 'label3'), 'upper right') The *loc* location codes are:: 'best' : 0, (currently not supported for figure legends) 'upper right' : 1, 'upper left' : 2, 'lower left' : 3, 'lower right' : 4, 'right' : 5, 'center left' : 6, 'center right' : 7, 'lower center' : 8, 'upper center' : 9, 'center' : 10,"""plt.show() 结果]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib-设置坐标轴2]]></title>
    <url>%2F2019%2F06%2F10%2Fmatplotlib-ax_setting_2%2F</url>
    <content type="text"><![CDATA[详情请看莫烦老师教程莫烦python 代码12345678910111213141516171819202122232425262728import matplotlib.pyplot as pltimport numpy as npx = np.linspace(-3, 3, 50)y1 = 2*x + 1y2 = x**2plt.figure()plt.plot(x, y2)# plot the second curve in this figure with certain parametersplt.plot(x, y1, color='red', linewidth=1.0, linestyle='--')# set x limitsplt.xlim((-1, 2))plt.ylim((-2, 3))# gca = 'get current axis'ax = plt.gca()# 取消右边框和上边框ax.spines['right'].set_color('none')ax.spines['top'].set_color('none')# 设置x,y轴ax.xaxis.set_ticks_position('bottom')ax.yaxis.set_ticks_position('left')# 移动x,y轴ax.spines['bottom'].set_position(('data', 0))ax.spines['left'].set_position(('data',0))plt.show() 结果]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib-设置坐标轴1]]></title>
    <url>%2F2019%2F06%2F10%2Fmatplotlib-ax_setting_1%2F</url>
    <content type="text"><![CDATA[详情请看莫烦老师教程莫烦python 代码12345678910111213141516171819202122232425262728import matplotlib.pyplot as pltimport numpy as npx = np.linspace(-3, 3, 50)y1 = 2*x + 1y2 = x**2plt.figure()plt.plot(x, y2)# plot the second curve in this figure with certain parametersplt.plot(x, y1, color='red', linewidth=1.0, linestyle='--')# 设置坐标轴范围plt.xlim((-1, 2))plt.ylim((-2, 3))# 设置坐标轴名称plt.xlabel('I am x')plt.ylabel('I am y')# 设置坐标轴间隔new_ticks = np.linspace(-1, 2, 5)plt.xticks(new_ticks)# use '$ $' for math text and nice looking, e.g. '$\pi$'plt.yticks([-2, -1.8, -1, 1.22, 3], [r'$really\ bad$', r'$bad$', r'$normal$', r'$good$', r'$really\ good$'])plt.show() 结果]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib-figure的使用]]></title>
    <url>%2F2019%2F06%2F10%2Fmatplotlib-figure%2F</url>
    <content type="text"><![CDATA[详情请看莫烦老师教程莫烦python 代码12345678910111213141516import matplotlib.pyplot as pltimport numpy as npx = np.linspace(-3, 3, 50)y1 = 2 * x + 1y2 = x ** 2plt.figure()plt.plot(x, y1)plt.figure(num=3, figsize=(8, 5))plt.plot(x, y2)# plot the second curve in this figure with certain parametersplt.plot(x, y1, color='red', linewidth=1.0, linestyle='--')plt.show() 结果]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch搭建ResNet识别CIFAR10数据集]]></title>
    <url>%2F2019%2F06%2F09%2Fpytorch_resnet_cifar10%2F</url>
    <content type="text"><![CDATA[导包123456789import osimport torch import torchvisionimport torch.nn as nnimport numpy as npimport torchvision.transforms as transformsimport matplotlib.pyplot as pltfrom tensorboardX import SummaryWriterfrom torchviz import make_dot 定义参数123456789# Device configurationos.environ["CUDA_VISIBLE_DEVICES"] = "9"device = torch.device('cuda:9' if torch.cuda.is_available() else 'cpu')# Hyper parametersnum_epochs = 80num_classes = 10batch_size = 100learning_rate = 0.001 图片预处理12345transform = transforms.Compose([ transforms.Pad(4), transforms.RandomHorizontalFlip(), transforms.RandomCrop(32), transforms.ToTensor()]) 导入CIFAR10数据集，定义数据加载器123456789101112131415161718# CIFAR-10 datasettrain_dataset = torchvision.datasets.CIFAR10(root='data/cifar10/', train=True, transform=transform, download=False)test_dataset = torchvision.datasets.CIFAR10(root='data/cifar10/', train=False, transform=transforms.ToTensor())# Data loadertrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) 定义网络12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# 3x3 convolutiondef conv3x3(in_channels, out_channels, stride=1): return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)# Residual blockclass ResidualBlock(nn.Module): def __init__(self, in_channels, out_channels, stride=1, downsample=None): super(ResidualBlock, self).__init__() self.conv1 = conv3x3(in_channels, out_channels, stride) self.bn1 = nn.BatchNorm2d(out_channels) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(out_channels, out_channels) self.bn2 = nn.BatchNorm2d(out_channels) self.downsample = downsample def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample: residual = self.downsample(x) out += residual out = self.relu(out) return out# ResNetclass ResNet(nn.Module): def __init__(self, block, layers, num_classes=10): super(ResNet, self).__init__() self.in_channels = 16 self.conv = conv3x3(3, 16) self.bn = nn.BatchNorm2d(16) self.relu = nn.ReLU(inplace=True) self.layer1 = self.make_layer(block, 16, layers[0]) self.layer2 = self.make_layer(block, 32, layers[1], 2) self.layer3 = self.make_layer(block, 64, layers[2], 2) self.avg_pool = nn.AvgPool2d(8) self.fc = nn.Linear(64, num_classes) def make_layer(self, block, out_channels, blocks, stride=1): downsample = None if (stride != 1) or (self.in_channels != out_channels): downsample = nn.Sequential( conv3x3(self.in_channels, out_channels, stride=stride), nn.BatchNorm2d(out_channels)) layers = [] layers.append(block(self.in_channels, out_channels, stride, downsample)) self.in_channels = out_channels for i in range(1, blocks): layers.append(block(out_channels, out_channels)) return nn.Sequential(*layers) def forward(self, x): out = self.conv(x) out = self.bn(out) out = self.relu(out) out = self.layer1(out) out = self.layer2(out) out = self.layer3(out) out = self.avg_pool(out) out = out.view(out.size(0), -1) out = self.fc(out) return out model = ResNet(ResidualBlock, [2, 2, 2]).to(device)# 使用tensorboard可视化网络# writer = SummaryWriter(logdir="logs/",comment="myresnet")# with writer:# writer.add_graph(model,input_to_model=torch.rand(20,3,32,32))# 使用pytorchviz可视化网络# x = torch.rand(20,3,32,32)# y = model(x)# make_dot(y,params=dict(model.named_parameters())) 定义损失函数和优化器12criterion = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) 更新学习率123def update_lr(optimizer, lr): for param_group in optimizer.param_groups: param_group['lr'] = lr 训练模型123456789101112131415161718192021222324total_step = len(train_loader)curr_lr = learning_ratefor epoch in range(num_epochs): for i, (images, labels) in enumerate(train_loader): images = images.to(device) labels = labels.to(device) # Forward pass outputs = model(images) loss = criterion(outputs, labels) # Backward and optimize optimizer.zero_grad() loss.backward() optimizer.step() if (i+1) % 100 == 0: print ("Epoch [&#123;&#125;/&#123;&#125;], Step [&#123;&#125;/&#123;&#125;] Loss: &#123;:.4f&#125;" .format(epoch+1, num_epochs, i+1, total_step, loss.item())) # Decay learning rate if (epoch+1) % 20 == 0: curr_lr /= 3 update_lr(optimizer, curr_lr) 测试模型1234567891011121314151617model.eval()with torch.no_grad(): correct = 0 total = 0 for images, labels in test_loader: images = images.to(device) labels = labels.to(device) outputs = model(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() print('Accuracy of the model on the test images: &#123;&#125; %'. format(100 * correct / total))# Save the model checkpoint# torch.save(model.state_dict(), 'resnet.ckpt')]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>cifar10</tag>
        <tag>pytorch</tag>
        <tag>resnet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch搭建CNN网络识别MNIST数据集]]></title>
    <url>%2F2019%2F06%2F09%2Fpytorch_mnist_cnn%2F</url>
    <content type="text"><![CDATA[导包123456import torch import torchvisionimport torch.nn as nnimport numpy as npimport torchvision.transforms as transformsimport matplotlib.pyplot as plt 定义参数12345678# Device configurationdevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')# Hyper parametersnum_epochs = 5num_classes = 10batch_size = 100learning_rate = 0.001 导入MNIST数据集，定义数据加载器123456789101112131415161718# MNIST dataset train_dataset = torchvision.datasets.MNIST(root='data/mnist/', train=True, transform=transforms.ToTensor(), download=False)test_dataset = torchvision.datasets.MNIST(root='data/mnist/', train=False, transform=transforms.ToTensor())# Data loadertrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) 定义网络123456789101112131415161718192021222324# Convolutional neural network (two convolutional layers)class ConvNet(nn.Module): def __init__(self, num_classes=10): super(ConvNet, self).__init__() self.layer1 = nn.Sequential( nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2), nn.BatchNorm2d(16), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2)) self.layer2 = nn.Sequential( nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2)) self.fc = nn.Linear(7*7*32, num_classes) def forward(self, x): out = self.layer1(x) # N*14*14*16 out = self.layer2(out) # N*7*7*32 out = out.reshape(out.size(0), -1) # (N, 7*7*32) out = self.fc(out) return out model = ConvNet(num_classes).to(device) 定义损失函数和优化器12criterion = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) 训练模型12345678910111213141516171819total_step = len(train_loader)for epoch in range(num_epochs): for i, (images, labels) in enumerate(train_loader): # Move tensors to the configured device images = images.to(device) labels = labels.to(device) # Forward pass outputs = model(images) loss = criterion(outputs, labels) # Backward and optimize optimizer.zero_grad() loss.backward() optimizer.step() if (i+1) % 100 == 0: print ('Epoch [&#123;&#125;/&#123;&#125;], Step [&#123;&#125;/&#123;&#125;], Loss: &#123;:.4f&#125;' .format(epoch+1, num_epochs, i+1, total_step, loss.item())) Epoch [1/5], Step [100/600], Loss: 0.1660 Epoch [1/5], Step [200/600], Loss: 0.1084 Epoch [1/5], Step [300/600], Loss: 0.1126 Epoch [1/5], Step [400/600], Loss: 0.1015 Epoch [1/5], Step [500/600], Loss: 0.0653 Epoch [1/5], Step [600/600], Loss: 0.0254 Epoch [2/5], Step [100/600], Loss: 0.0603 Epoch [2/5], Step [200/600], Loss: 0.0961 Epoch [2/5], Step [300/600], Loss: 0.0400 Epoch [2/5], Step [400/600], Loss: 0.0505 Epoch [2/5], Step [500/600], Loss: 0.0174 Epoch [2/5], Step [600/600], Loss: 0.0152 Epoch [3/5], Step [100/600], Loss: 0.0507 Epoch [3/5], Step [200/600], Loss: 0.0348 Epoch [3/5], Step [300/600], Loss: 0.0123 Epoch [3/5], Step [400/600], Loss: 0.0862 Epoch [3/5], Step [500/600], Loss: 0.0125 Epoch [3/5], Step [600/600], Loss: 0.0577 Epoch [4/5], Step [100/600], Loss: 0.0247 Epoch [4/5], Step [200/600], Loss: 0.0079 Epoch [4/5], Step [300/600], Loss: 0.0147 Epoch [4/5], Step [400/600], Loss: 0.0494 Epoch [4/5], Step [500/600], Loss: 0.0648 Epoch [4/5], Step [600/600], Loss: 0.0337 Epoch [5/5], Step [100/600], Loss: 0.0128 Epoch [5/5], Step [200/600], Loss: 0.0083 Epoch [5/5], Step [300/600], Loss: 0.0158 Epoch [5/5], Step [400/600], Loss: 0.0212 Epoch [5/5], Step [500/600], Loss: 0.0166 Epoch [5/5], Step [600/600], Loss: 0.0016 测试模型123456789101112131415161718# eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)model.eval() with torch.no_grad(): correct = 0 total = 0 for images, labels in test_loader: images = images.to(device) labels = labels.to(device) outputs = model(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() print('Test Accuracy of the model on the 10000 test images: &#123;&#125; %'. format(100 * correct / total))# Save the model checkpoint# torch.save(model.state_dict(), 'model.ckpt') Test Accuracy of the model on the 10000 test images: 98.94 %]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>mnist</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch搭建全连接网络识别MNIST数据集]]></title>
    <url>%2F2019%2F06%2F09%2Fpytorch_mnist_fc%2F</url>
    <content type="text"><![CDATA[导包123456import torch import torchvisionimport torch.nn as nnimport numpy as npimport torchvision.transforms as transformsimport matplotlib.pyplot as plt 定义参数12345678910# Device configurationdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')# Hyper-parameters input_size = 784hidden_size = 500num_classes = 10num_epochs = 10batch_size = 100learning_rate = 0.001 导入MNIST数据集，定义数据加载器123456789101112131415161718# MNIST dataset train_dataset = torchvision.datasets.MNIST(root='data/mnist/', train=True, transform=transforms.ToTensor(), download=False)test_dataset = torchvision.datasets.MNIST(root='data/mnist/', train=False, transform=transforms.ToTensor())# Data loadertrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) 定义网络123456789101112131415# Fully connected neural network with one hidden layerclass NeuralNet(nn.Module): def __init__(self, input_size, hidden_size, num_classes): super(NeuralNet, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, num_classes) def forward(self, x): out = self.fc1(x) out = self.relu(out) out = self.fc2(out) return outmodel = NeuralNet(input_size, hidden_size, num_classes).to(device) 定义损失函数和优化器12criterion = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) 训练模型12345678910111213141516171819total_step = len(train_loader)for epoch in range(num_epochs): for i, (images, labels) in enumerate(train_loader): # Move tensors to the configured device images = images.reshape(-1, 28*28).to(device) labels = labels.to(device) # Forward pass outputs = model(images) loss = criterion(outputs, labels) # Backward and optimize optimizer.zero_grad() loss.backward() optimizer.step() if (i+1) % 100 == 0: print ('Epoch [&#123;&#125;/&#123;&#125;], Step [&#123;&#125;/&#123;&#125;], Loss: &#123;:.4f&#125;' .format(epoch+1, num_epochs, i+1, total_step, loss.item())) Epoch [1/10], Step [100/600], Loss: 0.3704 Epoch [1/10], Step [200/600], Loss: 0.2298 Epoch [1/10], Step [300/600], Loss: 0.2699 Epoch [1/10], Step [400/600], Loss: 0.2355 Epoch [1/10], Step [500/600], Loss: 0.2695 Epoch [1/10], Step [600/600], Loss: 0.1755 Epoch [2/10], Step [100/600], Loss: 0.1354 Epoch [2/10], Step [200/600], Loss: 0.0762 Epoch [2/10], Step [300/600], Loss: 0.0893 Epoch [2/10], Step [400/600], Loss: 0.1229 Epoch [2/10], Step [500/600], Loss: 0.0545 Epoch [2/10], Step [600/600], Loss: 0.0268 Epoch [3/10], Step [100/600], Loss: 0.0358 Epoch [3/10], Step [200/600], Loss: 0.0872 Epoch [3/10], Step [300/600], Loss: 0.0946 Epoch [3/10], Step [400/600], Loss: 0.0441 Epoch [3/10], Step [500/600], Loss: 0.1179 Epoch [3/10], Step [600/600], Loss: 0.0320 Epoch [4/10], Step [100/600], Loss: 0.0273 Epoch [4/10], Step [200/600], Loss: 0.0865 Epoch [4/10], Step [300/600], Loss: 0.0621 Epoch [4/10], Step [400/600], Loss: 0.0578 Epoch [4/10], Step [500/600], Loss: 0.0433 Epoch [4/10], Step [600/600], Loss: 0.0991 Epoch [5/10], Step [100/600], Loss: 0.0414 Epoch [5/10], Step [200/600], Loss: 0.0539 Epoch [5/10], Step [300/600], Loss: 0.0586 Epoch [5/10], Step [400/600], Loss: 0.0080 Epoch [5/10], Step [500/600], Loss: 0.0269 Epoch [5/10], Step [600/600], Loss: 0.0598 Epoch [6/10], Step [100/600], Loss: 0.0172 Epoch [6/10], Step [200/600], Loss: 0.0168 Epoch [6/10], Step [300/600], Loss: 0.0583 Epoch [6/10], Step [400/600], Loss: 0.0109 Epoch [6/10], Step [500/600], Loss: 0.0197 Epoch [6/10], Step [600/600], Loss: 0.0407 Epoch [7/10], Step [100/600], Loss: 0.0273 Epoch [7/10], Step [200/600], Loss: 0.0346 Epoch [7/10], Step [300/600], Loss: 0.0148 Epoch [7/10], Step [400/600], Loss: 0.0235 Epoch [7/10], Step [500/600], Loss: 0.0157 Epoch [7/10], Step [600/600], Loss: 0.0589 Epoch [8/10], Step [100/600], Loss: 0.0116 Epoch [8/10], Step [200/600], Loss: 0.0071 Epoch [8/10], Step [300/600], Loss: 0.0067 Epoch [8/10], Step [400/600], Loss: 0.0111 Epoch [8/10], Step [500/600], Loss: 0.0081 Epoch [8/10], Step [600/600], Loss: 0.0023 Epoch [9/10], Step [100/600], Loss: 0.0062 Epoch [9/10], Step [200/600], Loss: 0.0041 Epoch [9/10], Step [300/600], Loss: 0.0011 Epoch [9/10], Step [400/600], Loss: 0.0011 Epoch [9/10], Step [500/600], Loss: 0.0050 Epoch [9/10], Step [600/600], Loss: 0.0390 Epoch [10/10], Step [100/600], Loss: 0.0018 Epoch [10/10], Step [200/600], Loss: 0.0152 Epoch [10/10], Step [300/600], Loss: 0.0134 Epoch [10/10], Step [400/600], Loss: 0.0181 Epoch [10/10], Step [500/600], Loss: 0.0428 Epoch [10/10], Step [600/600], Loss: 0.0164 测试模型1234567891011121314151617# In test phase, we don't need to compute gradients (for memory efficiency)with torch.no_grad(): correct = 0 total = 0 for images, labels in test_loader: images = images.reshape(-1, 28*28).to(device) labels = labels.to(device) outputs = model(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() print('Accuracy of the network on the 10000 test images: &#123;&#125; %'. format(100 * correct / total))# Save the model checkpoint# torch.save(model.state_dict(), 'model.ckpt') Accuracy of the network on the 10000 test images: 97.72 %]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>mnist</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python-面向对象，运算符重载，类函数与静态函数，多重继承，抽象类]]></title>
    <url>%2F2019%2F06%2F03%2Fpython-oo%2F</url>
    <content type="text"><![CDATA[面向对象基本概念1234567891011121314151617181920212223242526272829303132333435363738394041class Document(): def __init__(self, title, author, context): print('init function called') self.title = title self.author = author self.__context = context # __ 开头的属性是私有属性 def get_context_length(self): return len(self.__context) def intercept_context(self, length): self.__context = self.__context[:length]harry_potter_book = Document('Harry Potter', 'J. K. Rowling', '... Forever Do not believe any thing is capable of thinking independently ...')print(harry_potter_book.title)print(harry_potter_book.author)print(harry_potter_book.get_context_length())harry_potter_book.intercept_context(10)print(harry_potter_book.get_context_length())print(harry_potter_book.__context)########## 输出 ##########init function calledHarry PotterJ. K. Rowling7710---------------------------------------------------------------------------AttributeError Traceback (most recent call last)&lt;ipython-input-5-b4d048d75003&gt; in &lt;module&gt;() 22 print(harry_potter_book.get_context_length()) 23 ---&gt; 24 print(harry_potter_book.__context)AttributeError: 'Document' object has no attribute '__context' 类：一群有着相似性的事物的集合，这里对应 Python 的 class。 对象：集合中的一个事物，这里对应由 class 生成的某一个 object，比如代码中的 -harry_potter_book。 属性：对象的某个静态特征，比如上述代码中的 title、author 和 __context。 函数：对象的某个动态能力，比如上述代码中的 intercept_context () 函数。 注意：如果一个属性以 （注意，此处有两个 _） 开头，我们就默认这个属性是私有属性。私有属性，是指不希望在类的函数之外的地方被访问和修改的属性。所以，你可以看到，title 和 author 能够很自由地被打印出来，但是 print(harry_potter_book.context)会报错。 类的专有方法： 1234567891011121314__init__ : 构造函数，在生成对象时调用__del__ : 析构函数，释放对象时使用__repr__ : 打印，转换__setitem__ : 按照索引赋值__getitem__: 按照索引获取值__len__: 获得长度__cmp__: 比较运算__call__: 函数调用__add__: 加运算__sub__: 减运算__mul__: 乘运算__truediv__: 除运算__mod__: 求余运算__pow__: 乘方 举例，实现运算符重载： 1234567891011121314151617class Vector: def __init__(self, a, b): self.a = a self.b = b def __str__(self): return 'Vector (%d, %d)' % (self.a, self.b) def __add__(self,other): return Vector(self.a + other.a, self.b + other.b) v1 = Vector(2,10)v2 = Vector(5,-2)print (v1 + v2)## output# Vector(7,8) 类函数与静态函数12345678910111213141516171819202122232425262728293031323334353637class Document(): # 类中定义常量 WELCOME_STR = 'Welcome! The context for this book is &#123;&#125;.' def __init__(self, title, author, context): print('init function called') self.title = title self.author = author self.__context = context # 类函数 @classmethod def create_empty_book(cls, title, author): return cls(title=title, author=author, context='nothing') # 成员函数 def get_context_length(self): return len(self.__context) # 静态函数 @staticmethod def get_welcome(context): return Document.WELCOME_STR.format(context)empty_book = Document.create_empty_book('What Every Man Thinks About Apart from Sex', 'Professor Sheridan Simove')print(empty_book.get_context_length())print(empty_book.get_welcome('indeed nothing'))########## 输出 ##########init function called7Welcome! The context for this book is indeed nothing. WELCOME_STR 为类常量，类中使用 self.WELCOME_STR ，或者在类外使用Document.WELCOME_STR来表达这个字符串。 静态函数与类没有什么关联，最明显的特征便是，静态函数的第一参数没有任何特殊性，静态函数中只能访问类中的常量或其他静态函数。一般而言，静态函数可以用来做一些简单独立的任务，既方便测试，也能优化代码结构。 类函数的第一个参数一般为 cls，表示必须传一个类进来，cls是一个持有类本身的对象。类函数最常用的功能是实现不同的init 构造函数，比如上文代码中，我们使用 create_empty_book 类函数，来创造新的书籍对象，其 context 一定为 ‘nothing’。通过类函数，可以实现多个构造函数。 进一步说明类函数与静态函数 123456789101112131415161718192021class Date(object): def __init__(self, day=0, month=0, year=0): self.day = day self.month = month self.year = year @classmethod def from_string(cls, date_as_string): day, month, year = map(int, date_as_string.split('-')) date1 = cls(day, month, year) return date1 @staticmethod def is_date_valid(date_as_string): day, month, year = map(int, date_as_string.split('-')) return day &lt;= 31 and month &lt;= 12 and year &lt;= 3999date1 = Date(11,09,2012)date2 = Date.from_string('11-09-2012')is_date = Date.is_date_valid('11-09-2012') 类函数 Date.from_string 相当于创建了另一个“构造函数”，接受一个字符串。 假设我们有一个日期字符串，我们想以某种方式验证它，逻辑上讲可以绑定到Date类，但不需要实例化就可使用。此时就可以使用静态函数了，静态函数无法访问类的内容，基本上只是一个函数。 继承123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# class Entity(object): 新式类class Entity(): def __init__(self, object_type): print('parent class init called') self.object_type = object_type def get_context_length(self): raise Exception('get_context_length not implemented') def print_title(self): print(self.title)class Document(Entity): def __init__(self, title, author, context): print('Document class init called') # super(Document, self).__init__('document') 新式类 Entity.__init__(self, 'document') self.title = title self.author = author self.__context = context def get_context_length(self): return len(self.__context) class Video(Entity): def __init__(self, title, author, video_length): print('Video class init called') # super(Video, self).__init__('video') 新式类 Entity.__init__(self, 'video') self.title = title self.author = author self.__video_length = video_length def get_context_length(self): return self.__video_lengthharry_potter_book = Document('Harry Potter(Book)', 'J. K. Rowling', '... Forever Do not believe any thing is capable of thinking independently ...')harry_potter_movie = Video('Harry Potter(Movie)', 'J. K. Rowling', 120)print(harry_potter_book.object_type)print(harry_potter_movie.object_type)harry_potter_book.print_title()harry_potter_movie.print_title()print(harry_potter_book.get_context_length())print(harry_potter_movie.get_context_length())########## 输出 ########### Document class init called# parent class init called# Video class init called# parent class init called# document# video# Harry Potter(Book)# Harry Potter(Movie)# 77# 120 注意：形如class A(object)为新式类，形如class A()为经典(老式类)定义 首先需要注意的是构造函数。每个类都有构造函数，继承类在生成对象的时候，是不会自动调用父类的构造函数的，因此你必须在 init() 函数中显式调用父类的构造函数。它们的执行顺序是 子类的构造函数 -&gt; 父类的构造函数。 其次需要注意父类 get_context_length() 函数。如果使用 Entity 直接生成对象，调用get_context_length() 函数，就会 raise error 中断程序的执行。这其实是一种很好的写法，叫做函数重写，可以使子类必须重新写一遍 get_context_length() 函数，来覆盖掉原有函数。 最后需要注意到 print_title() 函数，这个函数定义在父类中，但是子类的对象可以毫无阻力地使用它来打印 title，这也就体现了继承的优势：减少重复的代码，降低系统的熵值（即复杂度）。 关于多重继承 在Python里，当你新构造一个对象时，有两个步骤：首先是自底向上，从左至右调用new，然后再依照递归栈依次调用init。这个问题可以用以下代码说明： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465class A(object): def __new__(cls, *argv, **kwargs): print('nA') return super().__new__(cls) def __init__(self, a): print('A') self.a = a def comeon(self): print('A.comeon')class B(A): def __new__(cls, *argv, **kwargs): print('nB') return super().__new__(cls) def __init__(self, b): super(B, self).__init__(b) print('B') self.b = b def comeon(self): print('B.comeon')class C(A): def __new__(cls, *argv, **kwargs): print('nC') return super().__new__(cls) def __init__(self, c): super(C, self).__init__(c) print('C') self.c = c def comeon(self): print('C.comeon')class D(B, C): def __new__(cls, *argv, **kwargs): print('nD') return super().__new__(cls) def __init__(self, d): super(D, self).__init__(d) print('D') self.d = dd = D('d')d.comeon()############输出###########nDnBnCnAACBDB.comeon 首先看到：d.comeon是从左自右得来的左边的那个B的comeon。那么如何实现这样的效果呢？很简单，让B的init最后一个执行，就能覆盖掉C和D写入的comeon。 所以实际调用new的顺序就是D–B–C–A，之后递归栈回过头来初始化，调用init的顺序就是A–C–B–D，只有这样才能保证B里的comeon能够覆盖掉A的init带入的comeon和C带入的comeon，同样保证如果你的D里有个comeon，它是最后一个init的，将最后写入而覆盖掉其它的。 类函数与静态函数的继承 1234567891011121314151617181920212223242526272829303132class Foo(object): X = 1 Y = 2 @staticmethod def averag(*mixes): return sum(mixes) / len(mixes) @staticmethod def static_method(): return Foo.averag(Foo.X, Foo.Y) @classmethod def class_method(cls): return cls.averag(cls.X, cls.Y)class Son(Foo): X = 3 Y = 5 @staticmethod def averag(*mixes): return sum(mixes) / 3p = Son()print(p.static_method())print(p.class_method())############输出############ 1.5# 2.6666666666666665 子类的实例继承了父类的static_method静态方法，调用该方法，还是调用的父类的方法和类属性。子类的实例继承了父类的class_method类方法，调用该方法，调用的是子类的方法和子类的类属性。 抽象类和抽象函数12345678910111213141516171819202122232425262728293031323334353637from abc import ABCMeta, abstractmethodclass Entity(metaclass=ABCMeta): @abstractmethod def get_title(self): pass @abstractmethod def set_title(self, title): passclass Document(Entity): def get_title(self): return self.title def set_title(self, title): self.title = titledocument = Document()document.set_title('Harry Potter')print(document.get_title())entity = Entity()########## 输出 ##########Harry Potter---------------------------------------------------------------------------TypeError Traceback (most recent call last)&lt;ipython-input-7-266b2aa47bad&gt; in &lt;module&gt;() 21 print(document.get_title()) 22 ---&gt; 23 entity = Entity() 24 entity.set_title('Test')TypeError: Can't instantiate abstract class Entity with abstract methods get_title, set_title Entity是一个抽象类，抽象类是一种特殊的类，它生下来就是作为父类存在的，一旦对象化就会报错。同样，抽象函数定义在抽象类之中，子类必须重写该函数才能使用。相应的抽象函数，则是使用装饰器 @abstractmethod 来表示。 抽象类就是这么一种存在，它是一种自上而下的设计风范，你只需要用少量的代码描述清楚要做的事情，定义好接口，然后就可以交给不同开发人员去开发和对接。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python-面向对象，运算符重载，类函数与静态函数，多重继承，抽象类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用tensorflow利用KNN算法对mnist数据集进行分类]]></title>
    <url>%2F2019%2F06%2F02%2Fknn-mnist%2F</url>
    <content type="text"><![CDATA[KNN算法思想总结在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前K个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个分类，其算法的描述为： 计算测试数据与各个训练数据之间的距离； 按照距离的递增关系进行排序； 选取距离最小的K个点； 确定前K个点所在类别的出现频率； 返回前K个点中出现频率最高的类别作为测试数据的预测分类。 加载mnist数据123456import tensorflow as tfimport numpy as npimport randomfrom tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets('data/', one_hot=True) Extracting data/train-images-idx3-ubyte.gz Extracting data/train-labels-idx1-ubyte.gz Extracting data/t10k-images-idx3-ubyte.gz Extracting data/t10k-labels-idx1-ubyte.gz 12print(mnist.train.images.shape)print(mnist.test.images.shape) (55000, 784) (10000, 784) 设置属性12345trainNum = 55000 # 训练图片总数testNum = 10000 # 测试图片总数trainSize = 5000 # 训练的时候使用的图片数量testSize = 5 # 测试的时候使用的图片数量k = 4 # 距离最小的K个图片 数据分解1234567891011121314151617# 生成不重复的随机数 trainIndex = np.random.choice(trainNum,trainSize,replace=False)testIndex = np.random.choice(testNum,testSize,replace=False)# 生成训练数据trainData = mnist.train.images[trainIndex]trainLabel = mnist.train.labels[trainIndex]# 生成测试数据testData = mnist.test.images[testIndex]testLabel = mnist.test.labels[testIndex]print('trainData.shape=', trainData.shape)print('trainLabel.shape=', trainLabel.shape)print('testData.shape=', testData.shape)print('testLabel.shape=', testLabel.shape)print('testLabel=', testLabel) trainData.shape= (5000, 784) trainLabel.shape= (5000, 10) testData.shape= (5, 784) testLabel.shape= (5, 10) testLabel= [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] 数据训练1 设置变量1234trainDataInput = tf.placeholder(shape=[None,784],dtype=tf.float32)trainLabelInput = tf.placeholder(shape=[None,10],dtype=tf.float32)testDataInput = tf.placeholder(shape=[None,784],dtype=tf.float32)testLabelInput = tf.placeholder(shape=[None,10],dtype=tf.float32) 2 计算KNN距离，使用曼哈顿距离123456789101112131415161718192021# expand_dim()来增加维度f1 = tf.expand_dims(testDataInput,1) # subtract()相减，得到一个三维数据f2 = tf.subtract(trainDataInput,f1)# tf.abs()求数据绝对值# tf.reduce_sum()完成数据累加，把数据放到f3中# f3 保存的是每张测试图片到所有训练的距离f3 = tf.reduce_sum(tf.abs(f2),reduction_indices=2)with tf.Session() as sess: p1 = sess.run(f1,feed_dict=&#123;testDataInput:testData[0:5]&#125;) print('p1=',p1.shape) p2 = sess.run(f2,feed_dict=&#123;trainDataInput:trainData,testDataInput:testData[0:5]&#125;) print('p2=',p2.shape) p3 = sess.run(f3,feed_dict=&#123;trainDataInput:trainData,testDataInput:testData[0:5]&#125;) print('p3=',p3.shape) print('p3[0,0]=',p3[0,0]) p1= (5, 1, 784) p2= (5, 5000, 784) p3= (5, 5000) p3[0,0]= 107.035324 3 选取距离最小的K个图片12345678910111213141516171819# tf.negative(x,name=None)，取负运算（f4 ＝－f3）f4 = tf.negative(f3)# f5，选取f4最大的四个值，即f3最小的四个值# f6，这四个值对应的索引f5,f6 = tf.nn.top_k(f4,k=4) with tf.Session() as sess: p4 = sess.run(f4,feed_dict=&#123;trainDataInput:trainData,testDataInput:testData[0:5]&#125;) print('p4=',p4.shape) print('p4[0,0]=',p4[0,0]) # p5= (5, 4)，每一张测试图片（共5张），分别对应4张最近训练图片，共20张 p5,p6 = sess.run((f5,f6),feed_dict=&#123;trainDataInput:trainData,testDataInput:testData[0:5]&#125;) print('p5=',p5.shape) print('p6=',p6.shape) print('p5',p5) print('p6',p6) p4= (5, 5000) p4[0,0]= -107.035324 p5= (5, 4) p6= (5, 4) p5 [[-58.270588 -63.31764 -66.56078 -66.59606 ] [-50.70195 -59.564705 -60.10588 -60.713737 ] [-10.211766 -13.3529415 -13.843139 -14.133332 ] [-24.886272 -35.011753 -36.38429 -36.733334 ] [ -8.498037 -9.266665 -11.807843 -12.474509 ]] p6 [[3015 3148 3455 3798] [4024 937 4708 4898] [2627 4520 4514 3382] [1312 4535 1769 3221] [2512 4388 2169 2942]] 4 确定K个图片在类型出现的概率123456789101112131415161718192021# 根据索引找到对应的标签值f7 = tf.gather(trainLabelInput,f6)# 累加维度1的数值f8 = tf.reduce_sum(f7,reduction_indices=1)# 返回的是f8中的最大值的索引号f9 = tf.argmax(f8,dimension=1)with tf.Session() as sess: p7 = sess.run(f7,feed_dict=&#123;trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel&#125;) print('p7=',p7.shape) print('p7[]',p7) p8 = sess.run(f8,feed_dict=&#123;trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel&#125;) print('p8=',p8.shape) print('p8[]=',p8) p9 = sess.run(f9,feed_dict=&#123;trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel&#125;) print('p9=',p9.shape) print('p9[]=',p9) p7= (5, 4, 10) p7[] [[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]] [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]] p8= (5, 10) p8[]= [[0. 0. 0. 0. 4. 0. 0. 0. 0. 0.] [0. 0. 4. 0. 0. 0. 0. 0. 0. 0.] [0. 4. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 4. 0. 0. 0.] [0. 4. 0. 0. 0. 0. 0. 0. 0. 0.]] p9= (5,) p9[]= [4 2 1 6 1] 5 检验结果123456789101112with tf.Session() as sess: # p9=p10，代表正确 p10 = np.argmax(testLabel[0:5], axis=1) print('p10[]=', p10)count = 0for i in range(0, 5): if p10[i] == p9[i]: count = count+1# 正确率print('ac=', j*100/5) p10[]= [4 2 1 6 1] ac= 100.0]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>tensorflow_knn_mnist</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[训练神经网络的一些注意事项（一）]]></title>
    <url>%2F2019%2F06%2F02%2Fnet_train_attention%2F</url>
    <content type="text"><![CDATA[硬搬网络的问题经典卷积网络的输入一般都是224*224，我们在使用迁移学习的时候，要考虑自己的图片大小，如果我们的图片太小，不能硬搬网络，需要自己利用经典的结构设计适合自己的网络。 总之要遵循这样一个原则： 最后一层特征图的大小一般是7*7左右，最大不会超过9*9，最小不能小于3*3。对于一个网络，最后一个卷积层的输出大小是最重要的。 为什么要进行归一化 特别需要注意的是： 在对图片数据归一化时，直接除以255.0就可以，在对其他数据归一化时，要先对训练集进行归一化，记录下平均值标准差等，然后利用训练集的这些特征对验证集与测试集归一化，切不可对训练集、验证集、测试集分别单独进行归一化，也不可以对所有数据归一化后再划分训练集和验证集，要先划分，再归一化。 sklearn中有相关API，如下： 123456from sklearn.preprocessing import MinMaxScalerscalar = MinMaxScaler()X_train = scalar.fit_transform(X_train)X_valid = scalar.transform(X_valid)X_test = scalar.transform(X_test) 样本不均衡问题如果原始数据各类别样本不均衡，解决方法一般有两个： 使用数据增强，是各类别样本均衡 设置损失权重，让网络更多的关注样本少的类 关于设置损失权重，如果使用keras框架，代码如下： 123456# 利用sklearn计算权重from sklearn.utils import class_weightclass_weight = class_weight.compute_class_weight( "balanced", np.unique(y_train), y_train)model.fit(X, y_train, batch_size, epochs, class_weight=class_weight)]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python-匿名函数与函数式编程]]></title>
    <url>%2F2019%2F06%2F01%2Fpython_lambda_map_reduce_filter%2F</url>
    <content type="text"><![CDATA[匿名函数匿名函数的关键字是 lambda，之后是一系列的参数，然后用冒号隔开，最后则是由这些参数组成的表达式。比如： 12345678910square = lambda x: x**2square(3) # 9======================== 等价于========================def square(x): return x**2square(3) # 9 注意一：lambda 是一个表达式(expression)，并不是一个语句(statement) 所谓的表达式，就是用一系列“公式”去表达一个东西，比如x + 2、 x**2等等； 而所谓的语句，则一定是完成了某些功能，比如赋值语句x = 1完成了赋值，print 语句print(x)完成了打印，条件语句 if x &lt; 0:完成了选择功能等等。 因此，lambda 可以用在一些常规函数 def 不能用的地方，比如，lambda 可以用在列表内部，而常规函数却不能： 1234[(lambda x: x*x)(x) for x in range(10)]# 输出[0, 1, 4, 9, 16, 25, 36, 49, 64, 81] 再比如，lambda 可以被用作某些函数的参数，而常规函数def 则不能： 123456l = [(1, 20), (3, 0), (9, 10), (2, -1)]l.sort(key=lambda x: x[1]) # 按列表中元祖的第二个元素排序print(l)# 输出[(2, -1), (3, 0), (9, 10), (1, 20)] 注意二，lambda 的主体是只有一行的简单表达式，并不能扩展成一个多行的代码块 什么时候使用匿名函数？ 当我们需要一个函数，但它非常简短，只需要一行就能完成；同时它在程序中只被调用一次。 python 函数式编程Python 主要提供了这么几个函数：map()、filter()、reduce()，通常结合匿名函数lambda一起使用。 map(function,iterable) 表示，对 iterable 中的每个元素，都运用 function这个函数，最后返回一个新的可遍历的集合。 12l = [1, 2, 3, 4, 5]new_list = map(lambda x: x * 2, l) # [2， 4， 6， 8， 10] map()函数直接由C语言写的，运行时不需要通过python解释器间接调用，并且内部做了诸多优化，所以运行速度很快。 filter(function,iterable) filter() 函数表示对 iterable 中的每个元素，都使用 function 判断，并返回 True 或者 False，最后将返回 True 的元素组成一个新的可遍历集合。 举个例子，比如要返回一个列表中的所有偶数，可以写成下面这样： 12l = [1, 2, 3, 4, 5]new_list = filter(lambda x: x % 2 == 0, l) # [2, 4] reduce(function,iterable) function 同样是一个函数对象，规定它有两个参数，表示对 iterable 中的每个元素以及上一次调用后的结果，运用 function 进行计算，所以最后返回的是一个单独的数。 举个例子，想要计算某个列表元素的乘积，就可以用 reduce()函数来表示： 12l = [1, 2, 3, 4, 5]product = reduce(lambda x, y: x * y, l) # 1*2*3*4*5 = 120 总结 通常来说，在我们想对集合中的元素进行一些操作时，如果操作非常简单，比如相加、累积这种，那么我们优先考虑 map()、filter()、reduce() 这类或者 list comprehension 的形式。至于这两种方式的选择： 在数据量非常多的情况下，比如机器学习的应用，那我们一般更倾向于函数式编程的表示，因为效率更高； 在数据量不多的情况下，并且你想要程序更加 Pythonic 的话，那么 list comprehension 也不失为一个好选择。 不过，如果你要对集合中的元素，做一些比较复杂的操作，那么，考虑到代码的可读性，我们通常会使用 for 循环，这样更加清晰明了。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python-匿名函数与函数式编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python-函数的嵌套、变量作用域、闭包]]></title>
    <url>%2F2019%2F06%2F01%2Fpython_function%2F</url>
    <content type="text"><![CDATA[函数的嵌套函数的嵌套有两个方面的作用： 第一，函数的嵌套能够保证内部函数的隐私。内部函数只能被外部函数所调用和访问，不会暴露在全局作用域，因此，如果你的函数内部有一些隐私数据（比如数据库的用户、密码等），不想暴露在外，那你就可以使用函数的的嵌套，将其封装在内部函数中，只通过外部函数来访问。比如： 123456def connect_DB(): def get_DB_configuration(): ... return host, username, password conn = connector.connect(get_DB_configuration()) return conn 第二，合理使用函数嵌套，能够提高程序的运行效率，比如： 12345678910111213141516def factorial(input): # validation check if not isinstance(input, int): raise Exception('input must be an integer.') if input &lt; 0: raise Exception('input must be greater or equal to 0' ) ... def inner_factorial(input): if input &lt;= 1: return 1 return input * inner_factorial(input-1) return inner_factorial(input)print(factorial(5)) 这里，我们使用递归的方式计算一个数的阶乘。因为在计算之前，需要检查输入是否合法，所以我写成了函数嵌套的形式，这样一来，输入是否合法就只用检查一次。而如果我们不使用函数嵌套，那么每调用一次递归便会检查一次，这是没有必要的，也会降低程序的运行效率。 函数变量作用域全局变量是定义在整个文件层次上的，比如： 12345MIN_VALUE = 1MAX_VALUE = 10def validation_check(value): if value &lt; MIN_VALUE or value &gt; MAX_VALUE: raise Exception('validation check fails') 这里的 MIN_VALUE 和 MAX_VALUE 就是全局变量，可以在文件内的任何地方被访问，当然在函数内部也是可以的。不过，我们不能在函数内部随意改变全局变量的值。比如，下面的写法就是错误的： 1234567MIN_VALUE = 1MAX_VALUE = 10def validation_check(value): ... MIN_VALUE += 1 ...validation_check(5) 如果运行这段代码，程序会报错： 1UnboundLocalError: local variable 'MIN_VALUE' referenced before assignment 这是因为，Python 的解释器会默认函数内部的变量为局部变量，但是又发现局部变量MIN_VALUE并没有声明，因此就无法执行相关操作，所以，如果我们一定要在函数内部改变全局变量的值，就必须加上global这个声明： 12345678MIN_VALUE = 1MAX_VALUE = 10def validation_check(value): global MIN_VALUE ... MIN_VALUE += 1 ...validation_check(5) 这里的 global 关键字，并不表示重新创建了一个全局变量 MIN_VALUE，而是告诉 Python 解释器，函数内部的变量 MIN_VALUE，就是之前定义的全局变量，并不是新的全局变量，也不是局部变量。这样，程序就可以在函数内部访问全局变量，并修改它的值了。另外，如果遇到函数内部局部变量和全局变量同名的情况，那么在函数内部，局部变量会覆盖全局变量。 另外，对于嵌套函数来说，内部函数可以访问外部函数定义的变量，但是无法修改，若要修改，必须加上nonlocal这个关键字： 123456789101112def outer(): x = "local" def inner(): nonlocal x # nonlocal 关键字表示这里的 x 就是外部函数 outer 定义的变量 x x = 'nonlocal' print("inner:", x) inner() print("outer:", x)outer()# 输出inner: nonlocalouter: nonlocal 如果不加上 nonlocal 这个关键字，而内部函数的变量又和外部函数变量同名，那么同样的，内部函数变量会覆盖外部函数的变量。 闭包闭包其实和嵌套函数类似，不同的是，这里的外部函数返回的是一个函数，而不是一个具体的值，返回的函数通常赋予一个变量，这个变量可以在后面被继续调用。 比如，我们想计算一个数的 n 次幂，用闭包可以写成下面的代码： 1234567891011121314151617181920def nth_power(exponent): def exponent_of(base): return base ** exponent return exponent_of # 返回值是 exponent_of 函数square = nth_power(2) # 计算一个数的平方cube = nth_power(3) # 计算一个数的立方 square# 输出&lt;function __main__.nth_power.&lt;locals&gt;.exponent(base)&gt;cube# 输出&lt;function __main__.nth_power.&lt;locals&gt;.exponent(base)&gt;print(square(2)) # 计算 2 的平方print(cube(2)) # 计算 2 的立方# 输出4 # 2^28 # 2^3 这里外部函数 nth_power() 返回值，是函数 exponent_of()，而不是一个具体的数值。需要注意的是，在执行完square = nth_power(2)和cube = nth_power(3)后，外部函数 nth_power() 的参数 exponent，仍然会被内部函数 exponent_of() 记住。这样，之后我们调用 square(2) 或者 cube(2) 时，程序就能顺利地输出结果，而不会报错说参数 exponent 没有定义了。 为什么要闭包呢？上面的程序，也可以写成下面的形式： 12def nth_power_rewrite(base, exponent): return base ** exponent 这样是可以的，使用闭包的一个原因是让程序变得更简洁易读，当需要计算很多个数的平方时： 123456789101112# 不使用闭包res1 = nth_power_rewrite(base1, 2)res2 = nth_power_rewrite(base2, 2)res3 = nth_power_rewrite(base3, 2)...# 使用闭包square = nth_power(2)res1 = square(base1)res2 = square(base2)res3 = square(base3)... 使用闭包，每次调用都可以少传入一个参数，表达更为简洁，而且，当函数开头需要做一些额外的操作，而我们又需要多次调用这个函数时，将那些额外的操作放在外部函数，就可以减少多次调用导致的不必要的开销，提高程序的运行效率，另外，闭包常常和装饰器一起使用。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python-函数的嵌套、变量作用域、闭包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-137-DNN 实现性别与年龄预测]]></title>
    <url>%2F2019%2F05%2F31%2Fopencv-137%2F</url>
    <content type="text"><![CDATA[知识点在OpenCV DNN中如何调用多个模型，相互配合使用Gender Net and Age Nethttps://www.dropbox.com/s/iyv483wz7ztr9gh/gender_net.caffemodel?dl=0”https://www.dropbox.com/s/xfb20y596869vbb/age_net.caffemodel?dl=0”上述两个模型一个是预测性别的，一个是预测年龄的。 性别预测返回的是一个二分类结果MaleFemale 年龄预测返回的是8个年龄的阶段！‘(0-2)’,‘(4-6)’,‘(8-12)’,‘(15-20)’,‘(25-32)’,‘(38-43)’,‘(48-53)’,‘(60-100)’ 实现步骤： 完整的实现步骤需要如下几步： 预先加载三个网络模型 打开摄像头视频流/加载图像 对每一帧进行人脸检测 对检测到的人脸进行性别与年龄预测 解析预测结果 显示结果 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/dnn.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace cv::dnn;using namespace std;const size_t width = 300;const size_t height = 300;String model_bin = "D:/projects/opencv_tutorial/data/models/face_detector/opencv_face_detector_uint8.pb";String config_text = "D:/projects/opencv_tutorial/data/models/face_detector/opencv_face_detector.pbtxt";String ageProto = "D:/projects/opencv_tutorial/data/models/cnn_age_gender_models/age_deploy.prototxt";String ageModel = "D:/projects/opencv_tutorial/data/models/cnn_age_gender_models/age_net.caffemodel";String genderProto = "D:/projects/opencv_tutorial/data/models/cnn_age_gender_models/gender_deploy.prototxt";String genderModel = "D:/projects/opencv_tutorial/data/models/cnn_age_gender_models/gender_net.caffemodel";String ageList[] = &#123; "(0-2)", "(4-6)", "(8-12)", "(15-20)", "(25-32)", "(38-43)", "(48-53)", "(60-100)" &#125;;String genderList[] = &#123; "Male", "Female" &#125;;int main(int argc, char** argv) &#123; Mat frame = imread("D:/images/dannis2.jpg"); if (frame.empty()) &#123; printf("could not load image...\n"); return -1; &#125; namedWindow("input image", WINDOW_AUTOSIZE); imshow("input image", frame); Net net = readNetFromTensorflow(model_bin, config_text); net.setPreferableBackend(DNN_BACKEND_OPENCV); net.setPreferableTarget(DNN_TARGET_CPU); Net ageNet = readNet(ageModel, ageProto); Net genderNet = readNet(genderModel, genderProto); Mat blobImage = blobFromImage(frame, 1.0, Size(300, 300), Scalar(104.0, 177.0, 123.0), false, false); net.setInput(blobImage, "data"); Mat detection = net.forward("detection_out"); vector&lt;double&gt; layersTimings; double freq = getTickFrequency() / 1000; double time = net.getPerfProfile(layersTimings) / freq; printf("execute time : %.2f ms\n", time); int padding = 20; Mat detectionMat(detection.size[2], detection.size[3], CV_32F, detection.ptr&lt;float&gt;()); float confidence_threshold = 0.5; for (int i = 0; i &lt; detectionMat.rows; i++) &#123; float confidence = detectionMat.at&lt;float&gt;(i, 2); if (confidence &gt; confidence_threshold) &#123; size_t objIndex = (size_t)(detectionMat.at&lt;float&gt;(i, 1)); float tl_x = detectionMat.at&lt;float&gt;(i, 3) * frame.cols; float tl_y = detectionMat.at&lt;float&gt;(i, 4) * frame.rows; float br_x = detectionMat.at&lt;float&gt;(i, 5) * frame.cols; float br_y = detectionMat.at&lt;float&gt;(i, 6) * frame.rows; Rect object_box((int)tl_x, (int)tl_y, (int)(br_x - tl_x), (int)(br_y - tl_y)); Rect roi; roi.x = max(0, object_box .x - padding); roi.y = max(0, object_box.y - padding); roi.width = min(object_box .width+ padding, frame.cols - 1); roi.height = min(object_box.height + padding, frame.rows - 1); Mat face = frame(roi); Mat faceblob = blobFromImage(face, 1.0, Size(227, 227), Scalar(78.4263377603, 87.7689143744, 114.895847746), false, false); ageNet.setInput(faceblob); genderNet.setInput(faceblob); Mat agePreds = ageNet.forward(); Mat genderPreds = genderNet.forward(); Mat probMat = agePreds.reshape(1, 1); Point classNumber; double classProb; minMaxLoc(probMat, NULL, &amp;classProb, NULL, &amp;classNumber); int classidx = classNumber.x; String age = ageList[classidx]; probMat = genderPreds.reshape(1, 1); minMaxLoc(probMat, NULL, &amp;classProb, NULL, &amp;classNumber); classidx = classNumber.x; String gender = genderList[classidx]; rectangle(frame, object_box, Scalar(0, 0, 255), 2, 8, 0); putText(frame, format("age:%s gender:%s", age.c_str(), gender.c_str()), object_box.tl(), FONT_HERSHEY_SIMPLEX, 0.8, Scalar(255, 0, 0), 1, 8); &#125; &#125; imshow("ssd-face-detection", frame); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687"""DNN 实现性别与年龄预测"""import cv2 as cvimport timedef getFaceBox(net, frame, conf_threshold=0.7): frameOpencvDnn = frame.copy() frameHeight = frameOpencvDnn.shape[0] frameWidth = frameOpencvDnn.shape[1] blob = cv.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False) net.setInput(blob) detections = net.forward() bboxes = [] for i in range(detections.shape[2]): confidence = detections[0, 0, i, 2] if confidence &gt; conf_threshold: x1 = int(detections[0, 0, i, 3] * frameWidth) y1 = int(detections[0, 0, i, 4] * frameHeight) x2 = int(detections[0, 0, i, 5] * frameWidth) y2 = int(detections[0, 0, i, 6] * frameHeight) bboxes.append([x1, y1, x2, y2]) cv.rectangle(frameOpencvDnn, (x1, y1), (x2, y2), (0, 255, 0), int(round(frameHeight / 150)), 8) return frameOpencvDnn, bboxesfaceProto = "opencv_face_detector.pbtxt"faceModel = "opencv_face_detector_uint8.pb"ageProto = "age_deploy.prototxt"ageModel = "age_net.caffemodel"genderProto = "gender_deploy.prototxt"genderModel = "gender_net.caffemodel"MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']genderList = ['Male', 'Female']# Load networkageNet = cv.dnn.readNet(ageModel, ageProto)genderNet = cv.dnn.readNet(genderModel, genderProto)faceNet = cv.dnn.readNet(faceModel, faceProto)cap = cv.VideoCapture(0)padding = 20while cv.waitKey(1) &lt; 0: t = time.time() hasFrame, frame = cap.read() frame = cv.flip(frame, 1) if not hasFrame: cv.waitKey() break frameFace, bboxes = getFaceBox(faceNet, frame) if not bboxes: print("No face Detected, Checking next frame") continue for bbox in bboxes: # print(bbox) face = frame[max(0, bbox[1] - padding):min(bbox[3] + padding, frame.shape[0] - 1), max(0, bbox[0] - padding):min(bbox[2] + padding, frame.shape[1] - 1)] blob = cv.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False) genderNet.setInput(blob) genderPreds = genderNet.forward() gender = genderList[genderPreds[0].argmax()] print("Gender : &#123;&#125;, conf = &#123;:.3f&#125;".format(gender, genderPreds[0].max())) ageNet.setInput(blob) agePreds = ageNet.forward() age = ageList[agePreds[0].argmax()] print("Age Output : &#123;&#125;".format(agePreds)) print("Age : &#123;&#125;, conf = &#123;:.3f&#125;".format(age, agePreds[0].max())) label = "&#123;&#125;,&#123;&#125;".format(gender, age) cv.putText(frameFace, label, (bbox[0], bbox[1] - 10), cv.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv.LINE_AA) cv.imshow("Age Gender Demo", frameFace) print("time : &#123;:.3f&#125; ms".format(time.time() - t))cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>DNN 实现性别与年龄预测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-136-DNN解析网络输出结果]]></title>
    <url>%2F2019%2F05%2F31%2Fopencv-136%2F</url>
    <content type="text"><![CDATA[知识点多数时候DNN模块中深度学习网络的输出结果，可能是二维、三维、或者四维的，具体跟网络的结构有很大的关系，一般常见的图像分类网络，是一个1XN维的向量，通过reshape之后就很容易解析，解析代码如下：Mat flat = prob.reshape(1,1)Point maxLoc;minMaxLoc(flat, 0, 0, &amp;maxLoc)int predict = maxLoc.x; 如果是对象检测网络SSD/RCNN/Faster-RCNN网络，输出的是NX7的模式所以其解析方式如下：Mat detectionMat(out.size[2]， out.size[3], CV_32F, out.ptr())就可以解析该结构！ 如果对象检测网络是基于Region的YOLO网络，则解析方式变为Mat scores = outs[i].row(j).colRange(5, outs[i].cols);前面五个为cx,cy,w, h, objectness 如果模型网络是图像分割的网络，最后一层输出是3通道的图像对象，则解析方式为：Mat green(224, 224, CV_32F, blob.ptr(0, 1)) // 表示绿色通道！]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>DNN解析网络输出结果</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-132-DNN单张与多张图像的推断]]></title>
    <url>%2F2019%2F05%2F29%2Fopencv-132%2F</url>
    <content type="text"><![CDATA[知识点OpenCV DNN中支持单张图像推断，同时还支持分批次方式的图像推断，对应的两个相关API分别为blobFromImage与blobFromImages，它们的返回对象都是一个四维的Mat对象-按照顺序分别为NCHW 其组织方式详解如下：N表示多张图像C表示接受输入图像的通道数目H表示接受输入图像的高度W表示接受输入图像的宽度 123456789101112131415161718192021222324252627Mat cv::dnn::blobFromImage( InputArray image, double scalefactor = 1.0, const Size &amp; size = Size(), const Scalar &amp; mean = Scalar(), bool swapRB = false, bool crop = false, int ddepth = CV_32F)Mat cv::dnn::blobFromImages( InputArrayOfArrays images, double scalefactor = 1.0, Size size = Size(), const Scalar &amp; mean = Scalar(), bool swapRB = false, bool crop = false, int ddepth = CV_32F )参数解释Images表示多张图像,image表示单张图像Scalefactor表示放缩Size表示图像大小Mean表示均值swapRB是否交换通道crop是否剪切ddepth 输出的类型，默认是浮点数格式 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/dnn.hpp&gt;#include &lt;iostream&gt;#include &lt;fstream&gt;/***************************************************************************************************************/using namespace cv;using namespace cv::dnn;using namespace std;String bin_model = "D:/projects/opencv_tutorial/data/models/googlenet/bvlc_googlenet.caffemodel";String protxt = "D:/projects/opencv_tutorial/data/models/googlenet/bvlc_googlenet.prototxt";String labels_txt_file = "D:/vcworkspaces/classification_classes_ILSVRC2012.txt";vector&lt;String&gt; readClassNames();int main(int argc, char** argv) &#123; Mat image1 = imread("D:/images/cat.jpg"); Mat image2 = imread("D:/images/aeroplane.jpg"); vector&lt;Mat&gt; images; images.push_back(image1); images.push_back(image2); vector&lt;String&gt; labels = readClassNames(); int w = 224; int h = 224; // 加载网络 Net net = readNetFromCaffe(protxt, bin_model); net.setPreferableBackend(DNN_BACKEND_INFERENCE_ENGINE); net.setPreferableTarget(DNN_TARGET_CPU); if (net.empty()) &#123; printf("read caffe model data failure...\n"); return -1; &#125; Mat inputBlob = blobFromImages(images, 1.0, Size(w, h), Scalar(104, 117, 123), false, false); // 执行图像分类 Mat prob; net.setInput(inputBlob); prob = net.forward(); vector&lt;double&gt; times; double time = net.getPerfProfile(times); float ms = (time * 1000) / getTickFrequency(); printf("current inference time : %.2f ms \n", ms); // 得到最可能分类输出 for (int n = 0; n &lt; prob.rows; n++) &#123; Point classNumber; double classProb; Mat probMat = prob(Rect(0, n, 1000, 1)).clone(); Mat result = probMat.reshape(1, 1); minMaxLoc(result, NULL, &amp;classProb, NULL, &amp;classNumber); int classidx = classNumber.x; printf("\n current image classification : %s, possible : %.2f\n", labels.at(classidx).c_str(), classProb); // 显示文本 putText(images[n], labels.at(classidx), Point(20, 50), FONT_HERSHEY_SIMPLEX, 1.0, Scalar(0, 0, 255), 2, 8); imshow("Image Classification", images[n]); waitKey(0); &#125; return 0;&#125;std::vector&lt;String&gt; readClassNames()&#123; std::vector&lt;String&gt; classNames; std::ifstream fp(labels_txt_file); if (!fp.is_open()) &#123; printf("could not open file...\n"); exit(-1); &#125; std::string name; while (!fp.eof()) &#123; std::getline(fp, name); if (name.length()) classNames.push_back(name); &#125; fp.close(); return classNames;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748"""DNN单张与多张图像的推断"""import cv2 as cvimport numpy as npbin_model = "bvlc_googlenet.caffemodel"protxt = "bvlc_googlenet.prototxt"# Load names of classesclasses = Nonewith open("classification_classes_ILSVRC2012.txt", 'rt') as f: classes = f.read().rstrip('\n').split('\n')# load CNN modelnet = cv.dnn.readNetFromCaffe(protxt, bin_model)# read input dataimage1 = cv.imread("images/dog.jpg")image2 = cv.imread("images/airplane.jpg")images = []images.append(image1)images.append(image2)blobs = cv.dnn.blobFromImages(np.asarray(images), 1.0, (224, 224), (104, 117,123), False, crop=False)print(blobs.shape)# Run a modelnet.setInput(blobs)out = net.forward()# Put efficiency information.t, _ = net.getPerfProfile()label = 'Inference time: %.2f ms' % (t * 1000.0 / cv.getTickFrequency())print(out.shape)# Get a class with a highest score.for i in range(len(out)): classId = np.argmax(out[i]) confidence = out[i][classId] cv.putText(images[i], label, (0, 15), cv.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0)) # Print predicted class. text_label = '%s: %.4f' % (classes[classId] if classes else 'Class #%d' % classId, confidence) cv.putText(images[i], text_label, (50, 50), cv.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2) cv.namedWindow("googlenet-demo", cv.WINDOW_NORMAL) cv.imshow("googlenet-demo", images[i]) cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>DNN单张与多张图像的推断</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-133-DNN 图像颜色化模型使用]]></title>
    <url>%2F2019%2F05%2F29%2Fopencv-133%2F</url>
    <content type="text"><![CDATA[知识点OpenCV DNN在4.0还支持灰度图像的彩色化模型，是根据2016年ECCV的论文而来，基于卷积神经网络模型，通过对Lab色彩空间进行量化分割，映射到最终的CNN输出结果，最后转换为RGB彩色图像。模型下载地址：GitHub - richzhang/colorization: Automatic coloriz…OpenCV DNN使用该模型时候，除了正常的Caffe模型与配置文件之外，还需要一个Lab的量化表。 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788#include &lt;opencv2/dnn.hpp&gt;#include &lt;opencv2/imgproc.hpp&gt;#include &lt;opencv2/highgui.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace cv::dnn;using namespace std;// the 313 ab cluster centers from pts_in_hull.npy (already transposed)static float hull_pts[] = &#123; -90., -90., -90., -90., -90., -80., -80., -80., -80., -80., -80., -80., -80., -70., -70., -70., -70., -70., -70., -70., -70., -70., -70., -60., -60., -60., -60., -60., -60., -60., -60., -60., -60., -60., -60., -50., -50., -50., -50., -50., -50., -50., -50., -50., -50., -50., -50., -50., -50., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -30., -30., -30., -30., -30., -30., -30., -30., -30., -30., -30., -30., -30., -30., -30., -30., -20., -20., -20., -20., -20., -20., -20., -20., -20., -20., -20., -20., -20., -20., -20., -20., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 30., 30., 30., 30., 30., 30., 30., 30., 30., 30., 30., 30., 30., 30., 30., 30., 30., 30., 30., 40., 40., 40., 40., 40., 40., 40., 40., 40., 40., 40., 40., 40., 40., 40., 40., 40., 40., 40., 40., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 60., 60., 60., 60., 60., 60., 60., 60., 60., 60., 60., 60., 60., 60., 60., 60., 60., 60., 60., 60., 70., 70., 70., 70., 70., 70., 70., 70., 70., 70., 70., 70., 70., 70., 70., 70., 70., 70., 70., 70., 80., 80., 80., 80., 80., 80., 80., 80., 80., 80., 80., 80., 80., 80., 80., 80., 80., 80., 80., 90., 90., 90., 90., 90., 90., 90., 90., 90., 90., 90., 90., 90., 90., 90., 90., 90., 90., 90., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 50., 60., 70., 80., 90., 20., 30., 40., 50., 60., 70., 80., 90., 0., 10., 20., 30., 40., 50., 60., 70., 80., 90., -20., -10., 0., 10., 20., 30., 40., 50., 60., 70., 80., 90., -30., -20., -10., 0., 10., 20., 30., 40., 50., 60., 70., 80., 90., 100., -40., -30., -20., -10., 0., 10., 20., 30., 40., 50., 60., 70., 80., 90., 100., -50., -40., -30., -20., -10., 0., 10., 20., 30., 40., 50., 60., 70., 80., 90., 100., -50., -40., -30., -20., -10., 0., 10., 20., 30., 40., 50., 60., 70., 80., 90., 100., -60., -50., -40., -30., -20., -10., 0., 10., 20., 30., 40., 50., 60., 70., 80., 90., 100., -70., -60., -50., -40., -30., -20., -10., 0., 10., 20., 30., 40., 50., 60., 70., 80., 90., 100., -80., -70., -60., -50., -40., -30., -20., -10., 0., 10., 20., 30., 40., 50., 60., 70., 80., 90., -80., -70., -60., -50., -40., -30., -20., -10., 0., 10., 20., 30., 40., 50., 60., 70., 80., 90., -90., -80., -70., -60., -50., -40., -30., -20., -10., 0., 10., 20., 30., 40., 50., 60., 70., 80., 90., -100., -90., -80., -70., -60., -50., -40., -30., -20., -10., 0., 10., 20., 30., 40., 50., 60., 70., 80., 90., -100., -90., -80., -70., -60., -50., -40., -30., -20., -10., 0., 10., 20., 30., 40., 50., 60., 70., 80., -110., -100., -90., -80., -70., -60., -50., -40., -30., -20., -10., 0., 10., 20., 30., 40., 50., 60., 70., 80., -110., -100., -90., -80., -70., -60., -50., -40., -30., -20., -10., 0., 10., 20., 30., 40., 50., 60., 70., 80., -110., -100., -90., -80., -70., -60., -50., -40., -30., -20., -10., 0., 10., 20., 30., 40., 50., 60., 70., -110., -100., -90., -80., -70., -60., -50., -40., -30., -20., -10., 0., 10., 20., 30., 40., 50., 60., 70., -90., -80., -70., -60., -50., -40., -30., -20., -10., 0.&#125;;int main(int argc, char **argv)&#123; string modelTxt = "D:/projects/models/color/colorization_deploy_v2.prototxt"; string modelBin = "D:/projects/models/color/colorization_release_v2.caffemodel"; Mat img = imread("D:/images/dannis2.jpg"); // fixed input size for the pretrained network const int W_in = 224; const int H_in = 224; Net net = dnn::readNetFromCaffe(modelTxt, modelBin); // setup additional layers: int sz[] = &#123; 2, 313, 1, 1 &#125;; const Mat pts_in_hull(4, sz, CV_32F, hull_pts); Ptr&lt;dnn::Layer&gt; class8_ab = net.getLayer("class8_ab"); class8_ab-&gt;blobs.push_back(pts_in_hull); Ptr&lt;dnn::Layer&gt; conv8_313_rh = net.getLayer("conv8_313_rh"); conv8_313_rh-&gt;blobs.push_back(Mat(1, 313, CV_32F, Scalar(2.606))); // extract L channel and subtract mean Mat lab, L, input; img.convertTo(img, CV_32F, 1.0 / 255); cvtColor(img, lab, COLOR_BGR2Lab); extractChannel(lab, L, 0); resize(L, input, Size(W_in, H_in)); input -= 50; // run the L channel through the network Mat inputBlob = blobFromImage(input); net.setInput(inputBlob); Mat result = net.forward(); // retrieve the calculated a,b channels from the network output Size siz(result.size[2], result.size[3]); Mat a = Mat(siz, CV_32F, result.ptr(0, 0)); Mat b = Mat(siz, CV_32F, result.ptr(0, 1)); resize(a, a, img.size()); resize(b, b, img.size()); // merge, and convert back to BGR Mat color, chn[] = &#123; L, a, b &#125;; merge(chn, 3, lab); cvtColor(lab, color, COLOR_Lab2BGR); imshow("dnn-color", color); imshow("original-gray", img); waitKey(); return 0;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354"""DNN 图像颜色化模型使用"""import cv2 as cvimport numpy as npW_in = 224H_in = 224modelTxt = "colorization_deploy_v2.prototxt"modelBin = "colorization_release_v2.caffemodel"pts_txt = "pts_in_hull.npy"# Select desired modelnet = cv.dnn.readNetFromCaffe(modelTxt, modelBin)pts_in_hull = np.load(pts_txt) # load cluster centers# populate cluster centers as 1x1 convolution kernelpts_in_hull = pts_in_hull.transpose().reshape(2, 313, 1, 1)net.getLayer(net.getLayerId('class8_ab')).blobs = [pts_in_hull.astype(np.float32)]net.getLayer(net.getLayerId('conv8_313_rh')).blobs = [np.full([1, 313], 2.606, np.float32)]frame = cv.imread("images/test1.png")h, w = frame.shape[:2]img_rgb = (frame[:,:,[2, 1, 0]] * 1.0 / 255).astype(np.float32)img_lab = cv.cvtColor(img_rgb, cv.COLOR_RGB2Lab)img_l = img_lab[:,:,0] # pull out L channel(H_orig,W_orig) = img_rgb.shape[:2] # original image size# resize image to network input sizeimg_rs = cv.resize(img_rgb, (W_in, H_in))img_lab_rs = cv.cvtColor(img_rs, cv.COLOR_RGB2Lab)img_l_rs = img_lab_rs[:,:,0]img_l_rs -= 50 # subtract 50 for mean-centering# run networknet.setInput(cv.dnn.blobFromImage(img_l_rs))ab_dec = net.forward()[0,:,:,:].transpose((1,2,0))(H_out,W_out) = ab_dec.shape[:2]ab_dec_us = cv.resize(ab_dec, (W_orig, H_orig))img_lab_out = np.concatenate((img_l[:,:,np.newaxis],ab_dec_us),axis=2)img_bgr_out = np.clip(cv.cvtColor(img_lab_out, cv.COLOR_Lab2BGR), 0, 1)frame = cv.resize(frame, (w, h))cv.imshow('origin', frame)cv.imshow('gray', cv.cvtColor(frame, cv.COLOR_RGB2GRAY))# fix 4.0 imshow issuecv.normalize(img_bgr_out, img_bgr_out, 0, 255, cv.NORM_MINMAX)cv.imshow('colorized', cv.resize(np.uint8(img_bgr_out), (w, h)))cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>DNN 图像颜色化模型使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-134-DNN ENet实现图像分割]]></title>
    <url>%2F2019%2F05%2F29%2Fopencv-134%2F</url>
    <content type="text"><![CDATA[知识点OpenCV DNN支持ENet网络模型的图像分割，这里采用的预先训练的ENet网络模型下载地址如下：GitHub - e-lab/ENet-training该模型是torch模型，加载的API为： 123456Net cv::dnn::readNetFromTorch( const String &amp; model, bool isBinary = true )model参数表示二进制的模型权重文件isBinary 默认为true 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132#include &lt;fstream&gt;#include &lt;sstream&gt;#include &lt;opencv2/dnn.hpp&gt;#include &lt;opencv2/imgproc.hpp&gt;#include &lt;opencv2/highgui.hpp&gt;using namespace cv;using namespace dnn;std::vector&lt;std::string&gt; classes;std::vector&lt;Vec3b&gt; colors;void showLegend();void colorizeSegmentation(const Mat &amp;score, Mat &amp;segm);String enet_model = "D:/projects/models/enet/model-best.net";int main(int argc, char** argv)&#123; Mat frame = imread("D:/projects/models/enet/test.png"); Net net = readNetFromTorch(enet_model); net.setPreferableBackend(DNN_BACKEND_OPENCV); net.setPreferableTarget(DNN_TARGET_CPU); // Create a window static const std::string kWinName = "ENet-Demo"; namedWindow(kWinName, WINDOW_AUTOSIZE); imshow("input", frame); // Process frames. Mat blob = blobFromImage(frame, 0.00392, Size(1024, 512), Scalar(0, 0, 0), true, false); net.setInput(blob); Mat score = net.forward(); Mat segm; colorizeSegmentation(score, segm); resize(segm, segm, frame.size(), 0, 0, INTER_NEAREST); addWeighted(frame, 0.1, segm, 0.9, 0.0, frame); // Put efficiency information. std::vector&lt;double&gt; layersTimes; double freq = getTickFrequency() / 1000; double t = net.getPerfProfile(layersTimes) / freq; std::string label = format("Inference time: %.2f ms", t); putText(frame, label, Point(0, 15), FONT_HERSHEY_SIMPLEX, 0.5, Scalar(0, 255, 0)); imshow("ENet-Demo", frame); if (!classes.empty()) showLegend(); waitKey(0); return 0;&#125;void colorizeSegmentation(const Mat &amp;score, Mat &amp;segm)&#123; const int rows = score.size[2]; const int cols = score.size[3]; const int chns = score.size[1]; if (colors.empty()) &#123; // Generate colors. colors.push_back(Vec3b()); for (int i = 1; i &lt; chns; ++i) &#123; Vec3b color; for (int j = 0; j &lt; 3; ++j) color[j] = (colors[i - 1][j] + rand() % 256) / 2; colors.push_back(color); &#125; &#125; else if (chns != (int)colors.size()) &#123; CV_Error(Error::StsError, format("Number of output classes does not match " "number of colors (%d != %zu)", chns, colors.size())); &#125; Mat maxCl = Mat::zeros(rows, cols, CV_8UC1); Mat maxVal(rows, cols, CV_32FC1, score.data); for (int ch = 1; ch &lt; chns; ch++) &#123; for (int row = 0; row &lt; rows; row++) &#123; const float *ptrScore = score.ptr&lt;float&gt;(0, ch, row); uint8_t *ptrMaxCl = maxCl.ptr&lt;uint8_t&gt;(row); float *ptrMaxVal = maxVal.ptr&lt;float&gt;(row); for (int col = 0; col &lt; cols; col++) &#123; if (ptrScore[col] &gt; ptrMaxVal[col]) &#123; ptrMaxVal[col] = ptrScore[col]; ptrMaxCl[col] = (uchar)ch; &#125; &#125; &#125; &#125; segm.create(rows, cols, CV_8UC3); for (int row = 0; row &lt; rows; row++) &#123; const uchar *ptrMaxCl = maxCl.ptr&lt;uchar&gt;(row); Vec3b *ptrSegm = segm.ptr&lt;Vec3b&gt;(row); for (int col = 0; col &lt; cols; col++) &#123; ptrSegm[col] = colors[ptrMaxCl[col]]; &#125; &#125;&#125;void showLegend()&#123; static const int kBlockHeight = 30; static Mat legend; if (legend.empty()) &#123; const int numClasses = (int)classes.size(); if ((int)colors.size() != numClasses) &#123; CV_Error(Error::StsError, format("Number of output classes does not match " "number of labels (%zu != %zu)", colors.size(), classes.size())); &#125; legend.create(kBlockHeight * numClasses, 200, CV_8UC3); for (int i = 0; i &lt; numClasses; i++) &#123; Mat block = legend.rowRange(i * kBlockHeight, (i + 1) * kBlockHeight); block.setTo(colors[i]); putText(block, classes[i], Point(0, kBlockHeight / 2), FONT_HERSHEY_SIMPLEX, 0.5, Vec3b(255, 255, 255)); &#125; namedWindow("Legend", WINDOW_NORMAL); imshow("Legend", legend); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667"""DNN ENet实现图像分割"""import cv2 as cvimport numpy as np# load CNN modelbin_model = "model-best.net"net = cv.dnn.readNetFromTorch(bin_model)# read input dataframe = cv.imread("images/cityscapes_test.jpg")blob = cv.dnn.blobFromImage(frame, 0.00392, (1024, 512), (0, 0, 0), True, False);cv.namedWindow("input", cv.WINDOW_NORMAL)cv.imshow("input", frame)# Run a modelnet.setInput(blob)score = net.forward()print(score.shape)# Put efficiency information.t, _ = net.getPerfProfile()label = 'Inference time: %.2f ms' % (t * 1000.0 / cv.getTickFrequency())# generate color tablecolor_lut = []n, con, h, w = score.shapefor i in range(con): b = np.random.randint(0, 256) g = np.random.randint(0, 256) r = np.random.randint(0, 256) color_lut.append((b, g, r))# find max score for 20 channels on pixel-wisemaxCl = np.zeros((h, w), dtype=np.int32)maxVal = np.zeros((h, w), dtype=np.float32)for i in range(con): for row in range(h): for col in range(w): t = maxVal[row, col] s = score[0, i, row, col] if s &gt; t: maxVal[row, col] = s maxCl[row, col] = i# colorful the segmentation imagesegm = np.zeros((h, w, 3), dtype=np.uint8)for row in range(h): for col in range(w): index = maxCl[row, col] segm[row, col] = color_lut[index]h, w = frame.shape[:2]segm = cv.resize(segm, (w, h), None, 0, 0, cv.INTER_NEAREST)print(segm.shape, frame.shape)cv.namedWindow("result", cv.WINDOW_NORMAL)cv.imshow("result", segm)frame_segm = cv.addWeighted(frame, 0.2, segm, 0.8, 0.0)cv.putText(frame_segm, label, (0, 15), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0))cv.namedWindow("input_result", cv.WINDOW_NORMAL)cv.imshow("input_result", frame_segm)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>DNN ENet实现图像分割</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-135-DNN 实时快速的图像风格迁移]]></title>
    <url>%2F2019%2F05%2F29%2Fopencv-135%2F</url>
    <content type="text"><![CDATA[知识点OpenCV DNN模块现在还支持图像风格迁移网络模型的加载与使用，支持的模型是基于李飞飞等人在论文《Perceptual Losses for Real-Time Style Transfer and Super-Resolution》中提到的快速图像风格迁移网络，基于感知损失来提取特征，生成图像特征与高分辨率图像。整个网络模型是基于DCGAN + 5个残差层构成，是一个典型的全卷积网络，关于DCGAN可以看这里的介绍与代码实现：使用DCGAN实现图像生成模型下载地址GitHub - jcjohnson/fast-neural-style: Feedforward …这个网络可以支持任意尺寸的图像输入，作者提供了很多种预训练的风格迁移模型： composition_vii.t7 starry_night.t7 la_muse.t7 the_wave.t7 mosaic.t7 the_scream.t7 feathers.t7 candy.t7 udnie.t7 这些模型都是torch框架支持的二进制权重文件，加载模型之后，就可以调用forward得到结果，通过对输出结果反向加上均值，rescale到0~255的RGB色彩空间，即可显示。 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/dnn.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace cv::dnn;using namespace std;const size_t width = 256;const size_t height = 256;String base_dir = "D:/projects/opencv_tutorial/data/models/fast_style/";String styles[9] = &#123; "composition_vii.t7", "starry_night.t7", "la_muse.t7", "the_wave.t7","mosaic.t7", "the_scream.t7", "feathers.t7", "candy.t7", "udnie.t7" &#125;;int main(int argc, char** argv) &#123; int index = 0; VideoCapture capture = VideoCapture(0); Net net = readNetFromTorch(format("%s%s", base_dir.c_str(), styles[index].c_str())); net.setPreferableBackend(DNN_BACKEND_INFERENCE_ENGINE); net.setPreferableTarget(DNN_TARGET_CPU); Mat frame; while (true) &#123; capture.read(frame); imshow("input", frame); Mat blobImage = blobFromImage(frame, 1.0, Size(width, height), Scalar(103.939, 116.779, 123.68), false, false); net.setInput(blobImage); Mat out = net.forward(); vector&lt;double&gt; layersTimings; double freq = getTickFrequency() / 1000; double time = net.getPerfProfile(layersTimings) / freq; printf("execute time : %.2f ms\n", time); int ch = out.size[1]; int h = out.size[2]; int w = out.size[3]; Mat result = Mat::zeros(Size(w, h), CV_32FC3); float* data = out.ptr&lt;float&gt;(); // decode 4-d Mat object for (int c = 0; c &lt; ch; c++) &#123; for (int row = 0; row &lt; h; row++) &#123; for (int col = 0; col &lt; w; col++) &#123; result.at&lt;Vec3f&gt;(row, col)[c] = *data++; &#125; &#125; &#125; // ���Ͻ����� printf("channels : %d, height: %d, width: %d \n", ch, h, w); add(result, Scalar(103.939, 116.779, 123.68), result); result /= 255.0; // ��ֵ�˲� medianBlur(result, result, 5); Mat dst; resize(result, dst, frame.size()); imshow("styled-video", dst); // ESC means exit char c = waitKey(1); if (c == 27) &#123; break; &#125; &#125; waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647"""DNN 实时快速的图像风格迁移"""import cv2 as cvimport numpy as npstyles = ["composition_vii.t7", "starry_night.t7", "la_muse.t7", "the_wave.t7", "mosaic.t7", "the_scream.t7", "feathers.t7", "candy.t7", "udnie.t7"]# 加载模型index = 2net = cv.dnn.readNetFromTorch(styles[index])# 读取图片frame = cv.imread("images/test.png")cv.imshow("input", frame)# 执行风格迁移blob = cv.dnn.blobFromImage(frame, 1.0, (256, 256), (103.939, 116.779, 123.68), swapRB=False, crop=False)net.setInput(blob)out = net.forward()print(out.shape)# 解析输出out = out.reshape(3, out.shape[2], out.shape[3])print(out.shape)out[0] += 103.939out[1] += 116.779out[2] += 123.68out /= 255.0out = out.transpose(1, 2, 0)print(out.shape)out = np.clip(out, 0.0, 1.0)# rescale与中值模糊，消除极值点噪声cv.normalize(out, out, 0, 255, cv.NORM_MINMAX)out = cv.medianBlur(out, 5)# resize and showh, w = frame.shape[:2]result = np.uint8(cv.resize(out, (w, h)))cv.imshow('Fast Style Demo', result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>DNN 实时快速的图像风格迁移</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-131-DNN 支持YOLOv3-tiny版本实时对象检测]]></title>
    <url>%2F2019%2F05%2F29%2Fopencv-131%2F</url>
    <content type="text"><![CDATA[知识点YOLOv3的模型在CPU上无法做到实时运行，而YOLO作者提供了个YOLOv3版本的精简版对象检测模型，大小只有30MB左右，但是模型可以在CPU上做到实时运行，这个模型就是YOLOv3-tiny模型，其下载地址如下：YOLO: Real-Time Object Detection相比YOLOv3，YOLOv3-tiny只有两个输出层，而且权重参数层与参数文件大小都大大的下降，可以在嵌入式设备与前端实时运行。 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/dnn.hpp&gt;#include &lt;fstream&gt;#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;cstdlib&gt;using namespace std;using namespace cv;using namespace cv::dnn;void image_detection();String yolo_tiny_model = "D:/projects/opencv_tutorial/data/models/yolov3-tiny-coco/yolov3-tiny.weights";String yolo_tiny_cfg = "D:/projects/opencv_tutorial/data/models/yolov3-tiny-coco/yolov3-tiny.cfg";int main(int argc, char** argv)&#123; image_detection();&#125;void image_detection() &#123; Net net = readNetFromDarknet(yolo_tiny_cfg, yolo_tiny_model); net.setPreferableBackend(DNN_BACKEND_INFERENCE_ENGINE); net.setPreferableTarget(DNN_TARGET_CPU); std::vector&lt;String&gt; outNames = net.getUnconnectedOutLayersNames(); for (int i = 0; i &lt; outNames.size(); i++) &#123; printf("output layer name : %s\n", outNames[i].c_str()); &#125; vector&lt;string&gt; classNamesVec; ifstream classNamesFile("D:/projects/opencv_tutorial/data/models/object_detection_classes_yolov3.txt"); if (classNamesFile.is_open()) &#123; string className = ""; while (std::getline(classNamesFile, className)) classNamesVec.push_back(className); &#125; // ����ͼ�� Mat frame = imread("D:/images/pedestrian.png"); Mat inputBlob = blobFromImage(frame, 1 / 255.F, Size(416, 416), Scalar(), true, false); net.setInput(inputBlob); // ��� std::vector&lt;Mat&gt; outs; net.forward(outs, outNames); vector&lt;double&gt; layersTimings; double freq = getTickFrequency() / 1000; double time = net.getPerfProfile(layersTimings) / freq; ostringstream ss; ss &lt;&lt; "detection time: " &lt;&lt; time &lt;&lt; " ms"; putText(frame, ss.str(), Point(20, 20), 0, 0.5, Scalar(0, 0, 255)); vector&lt;Rect&gt; boxes; vector&lt;int&gt; classIds; vector&lt;float&gt; confidences; for (size_t i = 0; i&lt;outs.size(); ++i) &#123; float* data = (float*)outs[i].data; for (int j = 0; j &lt; outs[i].rows; ++j, data += outs[i].cols) &#123; Mat scores = outs[i].row(j).colRange(5, outs[i].cols); Point classIdPoint; double confidence; minMaxLoc(scores, 0, &amp;confidence, 0, &amp;classIdPoint); if (confidence &gt; 0.5) &#123; int centerX = (int)(data[0] * frame.cols); int centerY = (int)(data[1] * frame.rows); int width = (int)(data[2] * frame.cols); int height = (int)(data[3] * frame.rows); int left = centerX - width / 2; int top = centerY - height / 2; classIds.push_back(classIdPoint.x); confidences.push_back((float)confidence); boxes.push_back(Rect(left, top, width, height)); &#125; &#125; &#125; vector&lt;int&gt; indices; NMSBoxes(boxes, confidences, 0.5, 0.2, indices); for (size_t i = 0; i &lt; indices.size(); ++i) &#123; int idx = indices[i]; Rect box = boxes[idx]; String className = classNamesVec[classIds[idx]]; putText(frame, className.c_str(), box.tl(), FONT_HERSHEY_SIMPLEX, 1.0, Scalar(255, 0, 0), 2, 8); rectangle(frame, box, Scalar(0, 0, 255), 2, 8, 0); &#125; imshow("YOLOv3-Detections", frame); waitKey(0); return;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384"""DNN 支持YOLOv3-tiny版本实时对象检测"""import cv2 as cvimport numpy as npyolo_tiny_model = "yolov3-tiny.weights"yolo_tiny_cfg = "yolov3-tiny.cfg"# Load names of classesclasses = Nonewith open("object_detection_classes_yolov3.txt", 'rt') as f: classes = f.read().rstrip('\n').split('\n')# load Darknetmodelnet = cv.dnn.readNetFromDarknet(yolo_tiny_cfg, yolo_tiny_model)# set back-end# net.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)# net.setPreferableTarget(cv.dnn.DNN_TARGET_CPU)cap = cv.VideoCapture(0)height = cap.get(cv.CAP_PROP_FRAME_HEIGHT)width = cap.get(cv.CAP_PROP_FRAME_WIDTH)while True: ret, image = cap.read() if ret is False: break image = cv.flip(image, 1) h, w = image.shape[:2] # 基于多个Region层输出getUnconnectedOutLayersNames blobImage = cv.dnn.blobFromImage(image, 1.0/255.0, (416, 416), None, True, False) outNames = net.getUnconnectedOutLayersNames() net.setInput(blobImage) outs = net.forward(outNames) # Put efficiency information. t, _ = net.getPerfProfile() fps = 1000 / (t * 1000.0 / cv.getTickFrequency()) label = 'FPS: %.2f' % fps cv.putText(image, label, (0, 15), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0)) # 绘制检测矩形 classIds = [] confidences = [] boxes = [] for out in outs: for detection in out: scores = detection[5:] classId = np.argmax(scores) confidence = scores[classId] # numbers are [center_x, center_y, width, height] if confidence &gt; 0.5: center_x = int(detection[0] * w) center_y = int(detection[1] * h) width = int(detection[2] * w) height = int(detection[3] * h) left = int(center_x - width / 2) top = int(center_y - height / 2) classIds.append(classId) confidences.append(float(confidence)) boxes.append([left, top, width, height]) # 使用非最大抑制 indices = cv.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4) for i in indices: i = i[0] box = boxes[i] left = box[0] top = box[1] width = box[2] height = box[3] cv.rectangle(image, (left, top), (left+width, top+height), (0, 0, 255), 2, 8, 0) cv.putText(image, classes[classIds[i]], (left, top), cv.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 0), 2) c = cv.waitKey(1) if c == 27: break cv.imshow('YOLOv3-tiny-Detection-Demo', image)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>DNN 支持YOLOv3-tiny版本实时对象检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-128-DNN 直接调用tensorflow的导出模型]]></title>
    <url>%2F2019%2F05%2F26%2Fopencv-128%2F</url>
    <content type="text"><![CDATA[知识点OpenCV在DNN模块中支持直接调用tensorflow object detection训练导出的模型使用，支持的模型包括 SSD Faster-RCNN Mask-RCNN 三种经典的对象检测网络，这样就可以实现从tensorflow模型训练、导出模型、在OpenCV DNN调用模型网络实现自定义对象检测的技术链路，具有非常高的实用价值。以Faster-RCNN为例，模型下载地址如下：models/detection_model_zoo.md at master · tensorfl…对于这些模型没有与之匹配的graph.pbtxt文件，OpenCV DNN模块提供python脚本来生成，相关详细说明请看：tensorflow模型导出与OpenCV DNN中使用 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/dnn.hpp&gt;#include &lt;fstream&gt;using namespace cv;using namespace cv::dnn;using namespace std;string label_map = "D:/tensorflow/models/research/object_detection/data/mscoco_label_map.pbtxt";string model = "D:/tensorflow/faster_rcnn_resnet50_coco_2018_01_28/frozen_inference_graph.pb";string config = "D:/tensorflow/faster_rcnn_resnet50_coco_2018_01_28/graph.pbtxt";std::map&lt;int, String&gt; readLabelMaps();int main(int argc, char** argv) &#123; Mat src = imread("D:/images/person.jpg"); int width = src.cols; int height = src.rows; if (src.empty()) &#123; printf("could not load image...\n"); return 0; &#125; namedWindow("input", WINDOW_AUTOSIZE); imshow("input", src); map&lt;int, string&gt; names = readLabelMaps(); // 加载Faster-RCNN Net net = readNetFromTensorflow(model, config); Mat blob = blobFromImage(src, 1.0, Size(300, 300), Scalar(), true, false); net.setInput(blob); // 预测 Mat detection = net.forward(); Mat detectionMat(detection.size[2], detection.size[3], CV_32F, detection.ptr&lt;float&gt;()); float threshold = 0.5; // 处理输出数据，绘制预测框与文本 for (int row = 0; row &lt; detectionMat.rows; row++) &#123; float confidence = detectionMat.at&lt;float&gt;(row, 2); if (confidence &gt; threshold) &#123; // base zero int object_class = detectionMat.at&lt;float&gt;(row, 1) + 1; // predict box int left = detectionMat.at&lt;float&gt;(row, 3) * width; int top = detectionMat.at&lt;float&gt;(row, 4) * height; int right = detectionMat.at&lt;float&gt;(row, 5) * width; int bottom = detectionMat.at&lt;float&gt;(row, 6) * height; Rect rect; rect.x = left; rect.y = top; rect.width = (right - left); rect.height = (bottom - top); // render bounding box and label name rectangle(src, rect, Scalar(255, 0, 255), 4, 8, 0); map&lt;int, string&gt;::iterator it = names.find(object_class); printf("id : %d, display name : %s \n", object_class, (it-&gt;second).c_str()); putText(src, (it-&gt;second).c_str(), Point(left, top - 5), FONT_HERSHEY_SIMPLEX, 0.5, Scalar(255, 0, 0), 1); &#125; &#125; imshow("faster-rcnn-demo", src); waitKey(0); return 0;&#125;std::map&lt;int, string&gt; readLabelMaps()&#123; std::map&lt;int, string&gt; labelNames; std::ifstream fp(label_map); if (!fp.is_open()) &#123; printf("could not open file...\n"); exit(-1); &#125; string one_line; string display_name; while (!fp.eof()) &#123; std::getline(fp, one_line); std::size_t found = one_line.find("id:"); if (found != std::string::npos) &#123; int index = found; string id = one_line.substr(index + 4, one_line.length() - index); std::getline(fp, display_name); std::size_t found = display_name.find("display_name:"); index = found + 15; string name = display_name.substr(index, display_name.length() - index); name = name.replace(name.length() - 1, name.length(), ""); // printf("id : %d, name: %s \n", stoi(id.c_str()), name.c_str()); labelNames[stoi(id)] = name; &#125; &#125; fp.close(); return labelNames;&#125; 1234567891011121314151617181920212223242526272829303132333435363738"""DNN 直接调用tensorflow的导出模型"""import cv2 as cvinference_pb = "frozen_inference_graph.pb"graph_text = "graph.pbtxt"# load tensorflow modelnet = cv.dnn.readNetFromTensorflow(inference_pb, graph_text)image = cv.imread("images/dog_person_horse.jpg")h = image.shape[0]w = image.shape[1]# 获得所有层名称与索引layerNames = net.getLayerNames()lastLayerId = net.getLayerId(layerNames[-1])lastLayer = net.getLayer(lastLayerId)print(lastLayer.type)# 检测net.setInput(cv.dnn.blobFromImage(image, size=(300, 300), swapRB=True, crop=False))cvOut = net.forward()for detection in cvOut[0, 0, :, :]: score = float(detection[2]) if score &gt; 0.5: left = detection[3] * w top = detection[4] * h right = detection[5] * w bottom = detection[6] * h # 绘制 cv.rectangle(image, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), thickness=2)cv.imshow('faster-rcnn-demo', image)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>DNN 直接调用tensorflow的导出模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-127-DNN 基于残差网络的视频人脸检测]]></title>
    <url>%2F2019%2F05%2F26%2Fopencv-127%2F</url>
    <content type="text"><![CDATA[知识点OpenCV在DNN模块中提供了基于残差SSD网络训练的人脸检测模型，还支持单精度的fp16的检测准确度更好的Caffe模型加载与使用，这里实现了一个基于Caffe Model的视频实时人脸监测模型，基于Python与C++代码CPU运行，帧率均可以到达15以上。非常好用。 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/dnn.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace cv::dnn;using namespace std;const size_t width = 300;const size_t height = 300;String model_bin = "D:/projects/opencv_tutorial/data/models/face_detector/res10_300x300_ssd_iter_140000_fp16.caffemodel";String config_text = "D:/projects/opencv_tutorial/data/models/face_detector/deploy.prototxt";int main(int argc, char** argv) &#123; VideoCapture capture = VideoCapture(0); namedWindow("ssd-face-video", WINDOW_AUTOSIZE); Net net = readNetFromCaffe(config_text, model_bin); net.setPreferableBackend(DNN_BACKEND_INFERENCE_ENGINE); net.setPreferableTarget(DNN_TARGET_CPU); Mat frame; while (true) &#123; capture.read(frame); Mat blobImage = blobFromImage(frame, 1.0, Size(300, 300), Scalar(104.0, 177.0, 123.0), false, false); net.setInput(blobImage, "data"); Mat detection = net.forward("detection_out"); vector&lt;double&gt; layersTimings; double freq = getTickFrequency() / 1000; double time = net.getPerfProfile(layersTimings) / freq; printf("execute time : %.2f ms\n", time); Mat detectionMat(detection.size[2], detection.size[3], CV_32F, detection.ptr&lt;float&gt;()); float confidence_threshold = 0.5; for (int i = 0; i &lt; detectionMat.rows; i++) &#123; float confidence = detectionMat.at&lt;float&gt;(i, 2); if (confidence &gt; confidence_threshold) &#123; size_t objIndex = (size_t)(detectionMat.at&lt;float&gt;(i, 1)); float tl_x = detectionMat.at&lt;float&gt;(i, 3) * frame.cols; float tl_y = detectionMat.at&lt;float&gt;(i, 4) * frame.rows; float br_x = detectionMat.at&lt;float&gt;(i, 5) * frame.cols; float br_y = detectionMat.at&lt;float&gt;(i, 6) * frame.rows; Rect object_box((int)tl_x, (int)tl_y, (int)(br_x - tl_x), (int)(br_y - tl_y)); rectangle(frame, object_box, Scalar(0, 0, 255), 2, 8, 0); putText(frame, format(" confidence %.2f", confidence), Point(tl_x - 10, tl_y - 5), FONT_HERSHEY_SIMPLEX, 0.5, Scalar(255, 0, 0), 1, 8); &#125; &#125; char c = waitKey(5); if (c == 27) &#123; break; &#125; imshow("ssd-face-video", frame); &#125; waitKey(0); return 0;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354"""DNN 基于残差网络的视频人脸检测"""import cv2 as cvmodel_bin = "res10_300x300_ssd_iter_140000_fp16.caffemodel"config_text = "deploy.prototxt"# load caffe modelnet = cv.dnn.readNetFromCaffe(config_text, model_bin)# set back-endnet.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)net.setPreferableTarget(cv.dnn.DNN_TARGET_CPU)cap = cv.VideoCapture(0)while True: ret, image = cap.read() image = cv.flip(image, 1) if ret is False: break # 人脸检测 h, w = image.shape[:2] blobImage = cv.dnn.blobFromImage(image, 1.0, (300, 300), (104.0, 177.0, 123.0), False, False) net.setInput(blobImage) cvOut = net.forward() # Put efficiency information. t, _ = net.getPerfProfile() fps = 1000 / (t * 1000.0 / cv.getTickFrequency()) label = 'FPS: %.2f' % fps cv.putText(image, label, (0, 15), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0)) # 绘制检测矩形 for detection in cvOut[0,0,:,:]: score = float(detection[2]) objIndex = int(detection[1]) if score &gt; 0.5: left = detection[3]*w top = detection[4]*h right = detection[5]*w bottom = detection[6]*h # 绘制 cv.rectangle(image, (int(left), int(top)), (int(right), int(bottom)), (255, 0, 0), thickness=2) cv.putText(image, "score:%.2f"%score, (int(left), int(top)), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1) cv.imshow('face-detection-demo', image) c = cv.waitKey(2) if c == 27: breakcv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>DNN 基于残差网络的视频人脸检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-126-DNN 基于残差网络的人脸检测]]></title>
    <url>%2F2019%2F05%2F26%2Fopencv-126%2F</url>
    <content type="text"><![CDATA[知识点OpenCV在DNN模块中提供了基于残差SSD网络训练的人脸检测模型，该模型分别提供了tensorflow版本，caffe版本，torch版本模型文件，其中tensorflow版本的模型做了更加进一步的压缩优化，大小只有2MB左右，非常适合移植到移动端使用，实现人脸检测功能，而caffe版本的是fp16的浮点数模型，精准度更好。要先获得这些模型，只要下载OpenCV4.0源码之后，打开运行sources\samples\dnn\face_detector\download_weights.py该脚本即可。同样一张图像，在OpenCV HAAR与LBP级联检测器中必须通过不断调整参数才可以检测出全部人脸，而通过使用该模型，基本在Python语言中基于OpenCV后台的推断，在25毫秒均可以检测出结果，网络支持输入size大小为300x300。 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/dnn.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace cv::dnn;using namespace std;const size_t width = 300;const size_t height = 300;String model_bin = "D:/projects/opencv_tutorial/data/models/face_detector/opencv_face_detector_uint8.pb";String config_text = "D:/projects/opencv_tutorial/data/models/face_detector/opencv_face_detector.pbtxt";int main(int argc, char** argv) &#123; Mat frame = imread("D:/images/persons.png"); if (frame.empty()) &#123; printf("could not load image...\n"); return -1; &#125; namedWindow("input image", WINDOW_AUTOSIZE); imshow("input image", frame); Net net = readNetFromTensorflow(model_bin, config_text); net.setPreferableBackend(DNN_BACKEND_OPENCV); net.setPreferableTarget(DNN_TARGET_CPU); Mat blobImage = blobFromImage(frame, 1.0, Size(300, 300), Scalar(104.0, 177.0, 123.0), false, false); net.setInput(blobImage, "data"); Mat detection = net.forward("detection_out"); vector&lt;double&gt; layersTimings; double freq = getTickFrequency() / 1000; double time = net.getPerfProfile(layersTimings) / freq; printf("execute time : %.2f ms\n", time); Mat detectionMat(detection.size[2], detection.size[3], CV_32F, detection.ptr&lt;float&gt;()); float confidence_threshold = 0.5; for (int i = 0; i &lt; detectionMat.rows; i++) &#123; float confidence = detectionMat.at&lt;float&gt;(i, 2); if (confidence &gt; confidence_threshold) &#123; size_t objIndex = (size_t)(detectionMat.at&lt;float&gt;(i, 1)); float tl_x = detectionMat.at&lt;float&gt;(i, 3) * frame.cols; float tl_y = detectionMat.at&lt;float&gt;(i, 4) * frame.rows; float br_x = detectionMat.at&lt;float&gt;(i, 5) * frame.cols; float br_y = detectionMat.at&lt;float&gt;(i, 6) * frame.rows; Rect object_box((int)tl_x, (int)tl_y, (int)(br_x - tl_x), (int)(br_y - tl_y)); rectangle(frame, object_box, Scalar(0, 0, 255), 2, 8, 0); putText(frame, format(" confidence %.2f", confidence), Point(tl_x - 10, tl_y - 5), FONT_HERSHEY_SIMPLEX, 0.5, Scalar(255, 0, 0), 1, 8); &#125; &#125; imshow("ssd-face-detection", frame); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243"""DNN 基于残差网络的人脸检测"""import cv2 as cvmodel_bin ="opencv_face_detector_uint8.pb"config_text = "opencv_face_detector.pbtxt"# load tensorflow modelnet = cv.dnn.readNetFromTensorflow(model_bin, config=config_text)image = cv.imread("images/persons.jpg")h = image.shape[0]w = image.shape[1]# 人脸检测blobImage = cv.dnn.blobFromImage(image, 1.0, (300, 300), (104.0, 177.0, 123.0), False, False)net.setInput(blobImage)cvOut = net.forward()# Put efficiency information.t, _ = net.getPerfProfile()label = 'Inference time: %.2f ms' % (t * 1000.0 / cv.getTickFrequency())cv.putText(image, label, (0, 15), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0))# 绘制检测矩形for detection in cvOut[0,0,:,:]: score = float(detection[2]) objIndex = int(detection[1]) if score &gt; 0.5: left = detection[3]*w top = detection[4]*h right = detection[5]*w bottom = detection[6]*h # 绘制 cv.rectangle(image, (int(left), int(top)), (int(right), int(bottom)), (255, 0, 0), thickness=2) cv.putText(image, "score:%.2f"%score, (int(left), int(top)), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)cv.imshow('face-detection-demo', image)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>DNN 基于残差网络的人脸检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-130-DNN 支持YOLO对象检测网络运行]]></title>
    <url>%2F2019%2F05%2F26%2Fopencv-130%2F</url>
    <content type="text"><![CDATA[知识点OpenCV DNN模块支持YOLO对象检测网络，最新的OpenCV4.0支持YOLOv3版本的对象检测网络，YOLOv3版本同时还发布了移动端支持的网络模型YOLOv3-tiny版本，速度可以在CPU端实时运行的对象检测网络，OpenCV中通过对DarkNet框架集成支持实现YOLO网络加载与检测。因为YOLOv3对象检测网络是多个层的合并输出，所以在OpenCV中调用时候必须显示声明那些是输出层，这个对于对象检测网络，OpenCV提供了一个API来获取所有的输出层名称，该API为： 12# 该函数返回所有非连接的输出层。std::vector&lt;String&gt; cv::dnn::Net::getUnconnectedOutLayersNames()const 调用时候，必须显式通过输入参数完成推断，相关API如下： 123456void cv::dnn::Net::forward(OutputArrayOfArrays outputBlobs,const std::vector&lt; String &gt; &amp; outBlobNames)outputBlobs是调用之后的输出outBlobNames是所有输出层的名称 跟SSD/Faster-RCNN出来的结构不一样，YOLO的输出前四个为: [center_x, center_y, width, height]后面的是所有类别的得分，这个时候只要根据score大小就可以得到score最大的对应对象类别，解析检测结果。相关模型下载到YOLO作者的官方网站：YOLO: Real-Time Object Detection 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/dnn.hpp&gt;#include &lt;fstream&gt;#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;cstdlib&gt;using namespace std;using namespace cv;using namespace cv::dnn;String yolo_cfg = "D:/projects/pose_body/hand/yolov3.cfg";String yolo_model = "D:/projects/pose_body/hand/yolov3.weights";int main(int argc, char** argv)&#123; Net net = readNetFromDarknet(yolo_cfg, yolo_model); net.setPreferableBackend(DNN_BACKEND_INFERENCE_ENGINE); net.setPreferableTarget(DNN_TARGET_CPU); std::vector&lt;String&gt; outNames = net.getUnconnectedOutLayersNames(); for (int i = 0; i &lt; outNames.size(); i++) &#123; printf("output layer name : %s\n", outNames[i].c_str()); &#125; // 加载COCO数据集标签 vector&lt;string&gt; classNamesVec; ifstream classNamesFile("D:/projects/opencv_tutorial/data/models/object_detection_classes_yolov3.txt"); if (classNamesFile.is_open()) &#123; string className = ""; while (std::getline(classNamesFile, className)) classNamesVec.push_back(className); &#125; // 加载图像 Mat frame = imread("D:/images/pedestrian.png"); Mat inputBlob = blobFromImage(frame, 1 / 255.F, Size(416, 416), Scalar(), true, false); net.setInput(inputBlob); // 检测 std::vector&lt;Mat&gt; outs; net.forward(outs, outNames); vector&lt;double&gt; layersTimings; double freq = getTickFrequency() / 1000; double time = net.getPerfProfile(layersTimings) / freq; ostringstream ss; ss &lt;&lt; "detection time: " &lt;&lt; time &lt;&lt; " ms"; putText(frame, ss.str(), Point(20, 20), 0, 0.5, Scalar(0, 0, 255)); vector&lt;Rect&gt; boxes; vector&lt;int&gt; classIds; vector&lt;float&gt; confidences; for (size_t i = 0; i&lt;outs.size(); ++i) &#123; // Network produces output blob with a shape NxC where N is a number of // detected objects and C is a number of classes + 4 where the first 4 // numbers are [center_x, center_y, width, height] float* data = (float*)outs[i].data; for (int j = 0; j &lt; outs[i].rows; ++j, data += outs[i].cols) &#123; Mat scores = outs[i].row(j).colRange(5, outs[i].cols); Point classIdPoint; double confidence; minMaxLoc(scores, 0, &amp;confidence, 0, &amp;classIdPoint); if (confidence &gt; 0.5) &#123; int centerX = (int)(data[0] * frame.cols); int centerY = (int)(data[1] * frame.rows); int width = (int)(data[2] * frame.cols); int height = (int)(data[3] * frame.rows); int left = centerX - width / 2; int top = centerY - height / 2; classIds.push_back(classIdPoint.x); confidences.push_back((float)confidence); boxes.push_back(Rect(left, top, width, height)); &#125; &#125; &#125; // 非最大抑制操作 vector&lt;int&gt; indices; NMSBoxes(boxes, confidences, 0.5, 0.2, indices); for (size_t i = 0; i &lt; indices.size(); ++i) &#123; int idx = indices[i]; Rect box = boxes[idx]; String className = classNamesVec[classIds[idx]]; putText(frame, className.c_str(), box.tl(), FONT_HERSHEY_SIMPLEX, 1.0, Scalar(255, 0, 0), 2, 8); rectangle(frame, box, Scalar(0, 0, 255), 2, 8, 0); &#125; imshow("YOLOv3-Detections", frame); waitKey(0); return;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869"""DNN 支持YOLO对象检测网络运行"""import cv2 as cvimport numpy as npmodel_bin = "yolov3.weights"config_text = "yolov3.cfg"# Load names of classesclasses = Nonewith open("object_detection_classes_yolov3.txt", 'rt') as f: classes = f.read().rstrip('\n').split('\n')# load Darknet modelnet = cv.dnn.readNetFromDarknet(config_text, model_bin)image = cv.imread("images/dog_person_horse.jpg")h = image.shape[0]w = image.shape[1]# 预测blobImage = cv.dnn.blobFromImage(image, 1.0/255.0, (416, 416), None, True, False)outNames = net.getUnconnectedOutLayersNames()net.setInput(blobImage)outs = net.forward(outNames)# Put efficiency information.t, _ = net.getPerfProfile()label = 'Inference time: %.2f ms' % (t * 1000.0 / cv.getTickFrequency())cv.putText(image, label, (0, 15), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0))# 绘制检测矩形classIds = []confidences = []boxes = []for out in outs: for detection in out: scores = detection[5:] classId = np.argmax(scores) confidence = scores[classId] # numbers are [center_x, center_y, width, height] if confidence &gt; 0.5: center_x = int(detection[0] * w) center_y = int(detection[1] * h) width = int(detection[2] * w) height = int(detection[3] * h) left = int(center_x - width / 2) top = int(center_y - height / 2) classIds.append(classId) confidences.append(float(confidence)) boxes.append([left, top, width, height])indices = cv.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)for i in indices: i = i[0] box = boxes[i] left = box[0] top = box[1] width = box[2] height = box[3] cv.rectangle(image, (left, top), (left+width, top+height), (0, 0, 255), 2, 8, 0) cv.putText(image, classes[classIds[i]], (left, top), cv.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 0), 2)cv.namedWindow("YOLOv3-Detection-Demo", cv.WINDOW_NORMAL)cv.imshow('YOLOv3-Detection-Demo', image)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>DNN 支持YOLO对象检测网络运行</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-125-DNN 基于SSD实现实时视频检测]]></title>
    <url>%2F2019%2F05%2F26%2Fopencv-125%2F</url>
    <content type="text"><![CDATA[知识点SSD的mobilenet版本不仅可以检测图像，还可以检测视频，达到稳定实时的效果。 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/dnn.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace cv::dnn;using namespace std;const size_t width = 300;const size_t height = 300;String labelFile = "D:/projects/opencv_tutorial/data/models/ssd/labelmap_det.txt";String modelFile = "D:/projects/opencv_tutorial/data/models/ssd/MobileNetSSD_deploy.caffemodel";String model_text_file = "D:/projects/opencv_tutorial/data/models/ssd/MobileNetSSD_deploy.prototxt";String objNames[] = &#123; "background","aeroplane", "bicycle", "bird", "boat","bottle", "bus", "car", "cat", "chair","cow", "diningtable", "dog", "horse","motorbike", "person", "pottedplant","sheep", "sofa", "train", "tvmonitor" &#125;;int main(int argc, char** argv) &#123; // load model Net net = readNetFromCaffe(model_text_file, modelFile); net.setPreferableBackend(DNN_BACKEND_OPENCV); net.setPreferableTarget(DNN_TARGET_CPU); VideoCapture cap = VideoCapture(0); Mat frame; while (true) &#123; bool ret = cap.read(frame); if (!ret) break; Mat blobImage = blobFromImage(frame, 0.007843, Size(300, 300), Scalar(127.5, 127.5, 127.5), true, false); printf("blobImage width : %d, height: %d\n", blobImage.size[2], blobImage.size[3]); net.setInput(blobImage, "data"); Mat detection = net.forward("detection_out"); vector&lt;double&gt; layersTimings; double freq = getTickFrequency() / 1000; double time = net.getPerfProfile(layersTimings) / freq; printf("execute time : %.2f ms\n", time); Mat detectionMat(detection.size[2], detection.size[3], CV_32F, detection.ptr&lt;float&gt;()); float confidence_threshold = 0.5; for (int i = 0; i &lt; detectionMat.rows; i++) &#123; float confidence = detectionMat.at&lt;float&gt;(i, 2); if (confidence &gt; confidence_threshold) &#123; size_t objIndex = (size_t)(detectionMat.at&lt;float&gt;(i, 1)); float tl_x = detectionMat.at&lt;float&gt;(i, 3) * frame.cols; float tl_y = detectionMat.at&lt;float&gt;(i, 4) * frame.rows; float br_x = detectionMat.at&lt;float&gt;(i, 5) * frame.cols; float br_y = detectionMat.at&lt;float&gt;(i, 6) * frame.rows; Rect object_box((int)tl_x, (int)tl_y, (int)(br_x - tl_x), (int)(br_y - tl_y)); rectangle(frame, object_box, Scalar(0, 0, 255), 2, 8, 0); putText(frame, format(" confidence %.2f, %s", confidence, objNames[objIndex].c_str()), Point(tl_x - 10, tl_y - 5), FONT_HERSHEY_SIMPLEX, 0.7, Scalar(255, 0, 0), 2, 8); &#125; &#125; imshow("ssd-video-demo", frame); char c = waitKey(10); if (c == 27) &#123; break; &#125; &#125; waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455"""DNN 基于SSD实现实时视频检测"""import cv2 as cvmodel_bin = "MobileNetSSD_deploy.caffemodel"config_text = "MobileNetSSD_deploy.prototxt"objName = ["background", "aeroplane", "bicycle", "bird", "boat", "bottle", "bus", "car", "cat", "chair", "cow", "diningtable", "dog", "horse", "motorbike", "person", "pottedplant", "sheep", "sofa", "train", "tvmonitor"]# load caffe modelnet = cv.dnn.readNetFromCaffe(config_text, model_bin)# # 获得所有层名称与索引# layerNames = net.getLayerNames()# lastLayerId = net.getLayerId(layerNames[-1])# lastLayer = net.getLayer(lastLayerId)# print(lastLayer.type)# 检测cap = cv.VideoCapture(0)h, w = int(cap.get(cv.CAP_PROP_FRAME_HEIGHT)), int(cap.get(cv.CAP_PROP_FRAME_WIDTH))while True: ret, frame = cap.read() if ret is False: break # h, w = frame.shape[:2] blobImage = cv.dnn.blobFromImage(frame, 0.007843, (300, 300), (127.5, 127.5, 127.5), True, False) net.setInput(blobImage) cvOut = net.forward() for detection in cvOut[0, 0, :, :]: score = float(detection[2]) objIndex = int(detection[1]) if score &gt; 0.5: left = detection[3] * w top = detection[4] * h right = detection[5] * w bottom = detection[6] * h # 绘制 cv.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (255, 0, 0), thickness=2) cv.putText(frame, "score:%.2f, %s" % (score, objName[objIndex]), (int(left) - 10, int(top) - 5), cv.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2, 8); cv.imshow('video-ssd-demo', frame) c = cv.waitKey(10) if c == 27: breakcv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>DNN 基于SSD实现实时视频检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-122-DNN 实现图像分类]]></title>
    <url>%2F2019%2F05%2F25%2Fopencv-122%2F</url>
    <content type="text"><![CDATA[知识点使用ImageNet数据集支持1000分类的GoogleNet网络模型， 分别演示了Python与C++语言中的使用该模型实现图像分类标签预测。其中label标签是在一个单独的文本文件中读取，模型从上面的链接中下载即可。读取模型的API： 123456Net cv::dnn::readNetFromCaffe( const String &amp; prototxt, const String &amp; caffeModel = String() )prototxt表示模型的配置文件caffeModel表示模型的权重二进制文件 使用模型实现预测的时候，需要读取图像作为输入，网络模型支持的输入数据是四维的输入，所以要把读取到的Mat对象转换为四维张量，OpenCV的提供的API为如下： 12345678910111213141516Mat cv::dnn::blobFromImage( InputArray image, double scalefactor = 1.0, const Size &amp; size = Size(), const Scalar &amp; mean = Scalar(), bool swapRB = false, bool crop = false, int ddepth = CV_32F )image输入图像scalefactor 默认1.0size表示网络接受的数据大小mean表示训练时数据集的均值swapRB 是否互换Red与Blur通道crop剪切ddepth 数据类型 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/dnn.hpp&gt;#include &lt;iostream&gt;#include &lt;fstream&gt;/***************************************************************************************************************/using namespace cv;using namespace cv::dnn;using namespace std;String labels_txt_file = "D:/projects/opencv_tutorial/data/models/inception5h/imagenet_comp_graph_label_strings.txt";String tf_pb_file = "D:/projects/opencv_tutorial/data/models/inception5h/tensorflow_inception_graph.pb";vector&lt;String&gt; readClassNames();int main(int argc, char** argv) &#123; Mat src = imread("D:/images/space_shuttle.jpg"); if (src.empty()) &#123; printf("could not load image...\n"); return -1; &#125; namedWindow("input", WINDOW_AUTOSIZE); imshow("input", src); vector&lt;String&gt; labels = readClassNames(); Mat rgb; cvtColor(src, rgb, COLOR_BGR2RGB); int w = 224; int h = 224; // 加载网络 Net net = readNetFromTensorflow(tf_pb_file); if (net.empty()) &#123; printf("read caffe model data failure...\n"); return -1; &#125; Mat inputBlob = blobFromImage(src, 1.0f, Size(224, 224), Scalar(), true, false); inputBlob -= 117.0; // 均值 // 执行图像分类 Mat prob; net.setInput(inputBlob, "input"); prob = net.forward("softmax2"); // 得到最可能分类输出 Mat probMat = prob.reshape(1, 1); Point classNumber; double classProb; minMaxLoc(probMat, NULL, &amp;classProb, NULL, &amp;classNumber); int classidx = classNumber.x; printf("\n current image classification : %s, possible : %.2f", labels.at(classidx).c_str(), classProb); // 显示文本 putText(src, labels.at(classidx), Point(20, 20), FONT_HERSHEY_SIMPLEX, 1.0, Scalar(0, 0, 255), 2, 8); imshow("Image Classification", src); imwrite("D:/result.png", src); waitKey(0); return 0;&#125;std::vector&lt;String&gt; readClassNames()&#123; std::vector&lt;String&gt; classNames; std::ifstream fp(labels_txt_file); if (!fp.is_open()) &#123; printf("could not open file...\n"); exit(-1); &#125; std::string name; while (!fp.eof()) &#123; std::getline(fp, name); if (name.length()) classNames.push_back(name); &#125; fp.close(); return classNames;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546"""DNN 实现图像分类"""import cv2 as cvimport numpy as npbin_model = "bvlc_googlenet.caffemodel"protxt = "bvlc_googlenet.prototxt"# load names of classesclasses = Nonewith open("classification_classes_ILSVRC2012.txt", 'rt') as f: classes = f.read().rstrip('\n').split('\n')# load CNN modelnet = cv.dnn.readNetFromCaffe(protxt, bin_model)# read input dataimage = cv.imread("images/airplane.jpg")blob = cv.dnn.blobFromImage(image, 1.0, (224, 224), (104, 117, 123), False, False)result = np.copy(image)cv.imshow("input", image)# run a modelnet.setInput(blob)out = net.forward()# get a class with a highest scoreout = out.flatten()classId = np.argmax(out)confidence = out[classId]# put efficiency informationt, _ = net.getPerfProfile()label = 'Inference time: %.2f ms' % (t * 1000.0 / cv.getTickFrequency())cv.putText(result, label, (0, 15), cv.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0))# print predicted classlabel = '%s : %.4f' % (classes[classId], confidence)cv.putText(result, label, (50, 50), cv.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)cv.imshow("googlenet-demo", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>DNN 实现图像分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-123-DNN 为模型运行设置目标设备与计算后台]]></title>
    <url>%2F2019%2F05%2F25%2Fopencv-123%2F</url>
    <content type="text"><![CDATA[知识点OpenCV中加载网络模型之后，可以设置计算后台与计算目标设备，OpenCV DNN模块支持这两个设置的相关API如下： 1234567cv::dnn::Net::setPreferableBackend( int backendId)backendId 表示后台计算id，- DNN_BACKEND_DEFAULT (DNN_BACKEND_INFERENCE_ENGINE)表示默认使用intel的预测推断库(需要下载安装Intel® OpenVINO™ toolkit， 然后重新编译OpenCV源码，在CMake时候enable该选项方可)， 可加速计算！- DNN_BACKEND_OPENCV 一般情况都是使用opencv dnn作为后台计算， 123456789void cv::dnn::Net::setPreferableTarget( int targetId)常见的目标设备id如下：- DNN_TARGET_CPU其中表示使用CPU计算，默认是的- DNN_TARGET_OPENCL 表示使用OpenCL加速，一般情况速度都很扯- DNN_TARGET_OPENCL_FP16 可以尝试- DNN_TARGET_MYRIAD 树莓派上的 代码（python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748"""DNN 为模型运行设置目标设备与计算后台"""import cv2 as cvimport numpy as npbin_model = "bvlc_googlenet.caffemodel"protxt = "bvlc_googlenet.prototxt"# load names of classesclasses = Nonewith open("classification_classes_ILSVRC2012.txt", 'rt') as f: classes = f.read().rstrip('\n').split('\n')# load CNN modelnet = cv.dnn.readNetFromCaffe(protxt, bin_model)net.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)net.setPreferableTarget(cv.dnn.DNN_TARGET_CPU)# read input dataimage = cv.imread("images/airplane.jpg")blob = cv.dnn.blobFromImage(image, 1.0, (224, 224), (104, 117, 123), False, False)result = np.copy(image)cv.imshow("input", image)# run a modelnet.setInput(blob)out = net.forward()# get a class with a highest scoreout = out.flatten()classId = np.argmax(out)confidence = out[classId]# put efficiency informationt, _ = net.getPerfProfile()label = 'Inference time: %.2f ms' % (t * 1000.0 / cv.getTickFrequency())cv.putText(result, label, (0, 15), cv.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0))# print predicted classlabel = '%s : %.4f' % (classes[classId], confidence)cv.putText(result, label, (50, 50), cv.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)cv.imshow("googlenet-demo", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>DNN 为模型运行设置目标设备与计算后台</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-124-DNN 基于SSD实现对象检测]]></title>
    <url>%2F2019%2F05%2F25%2Fopencv-124%2F</url>
    <content type="text"><![CDATA[知识点OpenCV DNN模块支持常见得对象检测模型SSD， 以及它的移动版Mobile Net-SSD，特别是后者在端侧边缘设备上可以实时计算。 对对象检测网络来说：该API会返回一个四维的tensor，前两个维度是1，后面的两个维度，分别表示检测到BOX数量，以及每个BOX的坐标，对象类别，得分等信息。这里需要特别注意的是，这个坐标是浮点数的比率，不是像素值，所以必须转换为像素坐标才可以绘制BOX/矩形。 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/dnn.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace cv::dnn;using namespace std;const size_t width = 300;const size_t height = 300;String labelFile = "D:/projects/opencv_tutorial/data/models/ssd/labelmap_det.txt";String modelFile = "D:/projects/opencv_tutorial/data/models/ssd/MobileNetSSD_deploy.caffemodel";String model_text_file = "D:/projects/opencv_tutorial/data/models/ssd/MobileNetSSD_deploy.prototxt";String objNames[] = &#123; "background","aeroplane", "bicycle", "bird", "boat","bottle", "bus", "car", "cat", "chair","cow", "diningtable", "dog", "horse","motorbike", "person", "pottedplant","sheep", "sofa", "train", "tvmonitor" &#125;;int main(int argc, char** argv) &#123; Mat frame = imread("D:/images/dog.jpg"); if (frame.empty()) &#123; printf("could not load image...\n"); return -1; &#125; namedWindow("input image", WINDOW_AUTOSIZE); imshow("input image", frame); Net net = readNetFromCaffe(model_text_file, modelFile); net.setPreferableBackend(DNN_BACKEND_INFERENCE_ENGINE); net.setPreferableTarget(DNN_TARGET_CPU); Mat blobImage = blobFromImage(frame, 0.007843, Size(300, 300), Scalar(127.5, 127.5, 127.5), true, false); printf("blobImage width : %d, height: %d\n", blobImage.cols, blobImage.rows); net.setInput(blobImage, "data"); Mat detection = net.forward("detection_out"); vector&lt;double&gt; layersTimings; double freq = getTickFrequency() / 1000; double time = net.getPerfProfile(layersTimings) / freq; printf("execute time : %.2f ms\n", time); Mat detectionMat(detection.size[2], detection.size[3], CV_32F, detection.ptr&lt;float&gt;()); float confidence_threshold = 0.5; for (int i = 0; i &lt; detectionMat.rows; i++) &#123; float confidence = detectionMat.at&lt;float&gt;(i, 2); if (confidence &gt; confidence_threshold) &#123; size_t objIndex = (size_t)(detectionMat.at&lt;float&gt;(i, 1)); float tl_x = detectionMat.at&lt;float&gt;(i, 3) * frame.cols; float tl_y = detectionMat.at&lt;float&gt;(i, 4) * frame.rows; float br_x = detectionMat.at&lt;float&gt;(i, 5) * frame.cols; float br_y = detectionMat.at&lt;float&gt;(i, 6) * frame.rows; Rect object_box((int)tl_x, (int)tl_y, (int)(br_x - tl_x), (int)(br_y - tl_y)); rectangle(frame, object_box, Scalar(0, 0, 255), 2, 8, 0); putText(frame, format(" confidence %.2f, %s", confidence, objNames[objIndex].c_str()), Point(tl_x - 10, tl_y - 5), FONT_HERSHEY_SIMPLEX, 0.7, Scalar(255, 0, 0), 2, 8); &#125; &#125; imshow("ssd-demo", frame); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243"""DNN 基于SSD实现对象检测"""import cv2 as cvmodel_bin = "MobileNetSSD_deploy.caffemodel"config_text = "MobileNetSSD_deploy.prototxt"objName = ["background", "aeroplane", "bicycle", "bird", "boat", "bottle", "bus", "car", "cat", "chair", "cow", "diningtable", "dog", "horse", "motorbike", "person", "pottedplant", "sheep", "sofa", "train", "tvmonitor"]# load caffe model and read test iamgenet = cv.dnn.readNetFromCaffe(config_text, model_bin)image = cv.imread("images/dog.jpg")h = image.shape[0]w = image.shape[1]# 检测blobImage = cv.dnn.blobFromImage(image, 0.007843, (300, 300), (127.5, 127.5, 127.5), True, False);net.setInput(blobImage)cvOut = net.forward()print(cvOut)for detection in cvOut[0, 0, :, :]: score = float(detection[2]) objIndex = int(detection[1]) if score &gt; 0.5: left = detection[3] * w top = detection[4] * h right = detection[5] * w bottom = detection[6] * h # 绘制 cv.rectangle(image, (int(left), int(top)), (int(right), int(bottom)), (255, 0, 0), thickness=2) cv.putText(image, "score:%.2f, %s" % (score, objName[objIndex]), (int(left) - 10, int(top) - 5), cv.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2, 8)cv.imshow("ssd-demo", image)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>DNN 基于SSD实现对象检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-121-DNN模块 获取导入模型各层信息]]></title>
    <url>%2F2019%2F05%2F25%2Fopencv-121%2F</url>
    <content type="text"><![CDATA[知识点模型支持1000个类别的图像分类，OpenCV DNN模块支持下面框架的预训练模型的前馈网络(预测图)使用 Caffe Tensorflow Torch DLDT Darknet 同时还支持自定义层解析、非最大抑制操作、获取各层的信息等。OpenCV加载模型的通用API为： 12345Net cv::dnn::readNet( const String &amp; model, const String &amp; config = "", const String &amp; framework = "" ) model二进制训练好的网络权重文件，可能来自支持的网络框架，扩展名为如下：.caffemodel (Caffe, http://caffe.berkeleyvision.org/) .pb (TensorFlow, https://www.tensorflow.org/).t7 | .net (Torch, http://torch.ch/).weights (Darknet, https://pjreddie.com/darknet/) .bin (DLDT, https://software.intel.com/openvino-toolkit) config针对模型二进制的描述文件，不同的框架配置文件有不同扩展名.prototxt (Caffe, http://caffe.berkeleyvision.org/) .pbtxt (TensorFlow, https://www.tensorflow.org/).cfg (Darknet, https://pjreddie.com/darknet/) .xml (DLDT, https://software.intel.com/openvino-toolkit) framework显示声明参数，说明模型使用哪个框架训练出来的 代码（c++,python）123456789101112131415161718192021222324#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/dnn.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace cv::dnn;using namespace std;int main(int argc, char** argv) &#123; string bin_model = "D:/projects/opencv_tutorial/data/models/googlenet/bvlc_googlenet.caffemodel"; string protxt = "D:/projects/opencv_tutorial/data/models/googlenet/bvlc_googlenet.prototxt"; // load CNN model Net net = dnn::readNet(bin_model, protxt); // 获取各层信息 vector&lt;String&gt; layer_names = net.getLayerNames(); for (int i = 0; i &lt; layer_names.size(); i++) &#123; int id = net.getLayerId(layer_names[i]); auto layer = net.getLayer(id); printf("layer id:%d, type: %s, name:%s \n", id, layer-&gt;type.c_str(), layer-&gt;name.c_str()); &#125; return 0;&#125; 12345678910111213141516171819202122232425"""DNN模块 获取导入模型各层信息"""import cv2 as cvimport numpy as npbin_model = "bvlc_googlenet.caffemodel"protxt = "bvlc_googlenet.prototxt"# load CNN modelnet = cv.dnn.readNet(bin_model, protxt)# 获取各层信息layer_names = net.getLayerNames()for name in layer_names: id = net.getLayerId(name) layer = net.getLayer(id) print("layer id : &#123;&#125;, type : &#123;&#125;, name : &#123;&#125;" .format(id, layer.type, layer.name))print("successfully loaded model...")cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>DNN模块 获取导入模型各层信息</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-117-图像均值漂移分割]]></title>
    <url>%2F2019%2F05%2F25%2Fopencv-117%2F</url>
    <content type="text"><![CDATA[知识点图像均值漂移分割是一种无监督的图像分割方法，前面我们在跟踪相关的内容介绍过均值迁移算法，知道均值迁移可以找到图像中特征直方图空间的峰值分布，这里我们还是使用均值迁移，让它去不断分割找到空间颜色分布的峰值，然后根据峰值进行相似度合并，解决过度分割问题，得到最终的分割图像，对于图像多维度数据颜色值(RGB)与空间位置(x,y)，所以需要两个窗口半径，一个是空间半径、另外一个是颜色半径，经过均值漂移窗口的所有的像素点会具有相同的像素值，OpenCV中均值漂移分割的API如下： 1234567891011121314void cv::pyrMeanShiftFiltering( InputArray src, OutputArray dst, double sp, double sr, int maxLevel = 1, TermCriteria termcrit = TermCriteria(TermCriteria::MAX_ITER+TermCriteria::EPS, 5, 1) )src 输入图像dst输出结果sp 表示空间窗口大小sr 表示表示颜色空间maxLevel表示金字塔层数，总层数为maxlevel+1tesrmcrit表示停止条件 代码（c++,python）12345678910111213141516#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace std;int main(int argc, char** argv) &#123; Mat src = imread("D:/images/yuan_test.png"); imshow("input", src); Mat dst; TermCriteria tc = TermCriteria(TermCriteria::MAX_ITER + TermCriteria::EPS, 10, 0.1); pyrMeanShiftFiltering(src, dst, 20, 40, 2, tc); imshow("mean shift segementation demo", dst); waitKey(0); return 0;&#125; 12345678910111213"""图像均值漂移分割"""import cv2 as cvsrc = cv.imread("images/yuan_test.png")cv.imshow("input", src)dst = cv.pyrMeanShiftFiltering(src, 25, 40, None, 2)cv.imshow("result", dst)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像均值漂移分割</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-118-Grabcut图像分割]]></title>
    <url>%2F2019%2F05%2F25%2Fopencv-118%2F</url>
    <content type="text"><![CDATA[知识点Grabcut是基于图割(graph cut)实现的图像分割算法，它需要用户输入一个bounding box作为分割目标位置，实现对目标与背景的分离/分割，这个跟KMeans与MeanShift等图像分割方法有很大的不同，但是Grabcut分割速度快，效果好，支持交互操作，因此在很多APP图像分割/背景虚化的软件中可以看到其身影。 1234567891011121314151617181920void cv::grabCut( InputArray img, InputOutputArray mask, Rect rect, InputOutputArray bgdModel, InputOutputArray fgdModel, int iterCount, int mode = GC_EVAL )img输入的三通道图像mask输入的单通道图像，初始化方式为GC_INIT_WITH_RECT表示ROI区域可以被初始化为：GC_BGD 定义为明显的背景像素 0GC_FGD 定义为明显的前景像素 1GC_PR_BGD 定义为可能的背景像素 2GC_PR_FGD 定义为可能的前景像素 3rect 表示roi区域bgdModel表示临时背景模型数组fgdModel表示临时前景模型数组iterCount表示图割算法迭代次数mode当使用用户提供的roi时候使用GC_INIT_WITH_RECT 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace std;int main(int argc, char** argv) &#123; Mat src = imread("D:/images/master.jpg"); if (src.empty()) &#123; printf("could not load image...\n"); return 0; &#125; namedWindow("input", WINDOW_AUTOSIZE); imshow("input", src); Mat mask = Mat::zeros(src.size(), CV_8UC1); Rect rect(180, 20, 180, 220); Mat bgdmodel = Mat::zeros(1, 65, CV_64FC1); Mat fgdmodel = Mat::zeros(1, 65, CV_64FC1); grabCut(src, mask, rect, bgdmodel, fgdmodel, 5, GC_INIT_WITH_RECT); Mat result; for (int row = 0; row &lt; mask.rows; row++) &#123; for (int col = 0; col &lt; mask.cols; col++) &#123; int pv = mask.at&lt;uchar&gt;(row, col); if (pv == 1 || pv == 3) &#123; mask.at&lt;uchar&gt;(row, col) = 255; &#125; else &#123; mask.at&lt;uchar&gt;(row, col) = 0; &#125; &#125; &#125; bitwise_and(src, src, result, mask); imshow("grabcut result", result); waitKey(0); return 0; &#125; 123456789101112131415161718192021222324"""Grabcut图像分割"""import cv2 as cvimport numpy as npsrc = cv.imread("images/master.jpg")cv.imshow("input", src)mask = np.zeros(src.shape[:2], dtype=np.uint8)rect = (53, 12, 356, 622)iterCount = 5bgdmodel = np.zeros((1, 13 * iterCount), np.float64)fgdmodel = np.zeros((1, 13 * iterCount), np.float64)cv.grabCut(src, mask, rect, bgdmodel, fgdmodel, iterCount, mode=cv.GC_INIT_WITH_RECT)mask2 = np.where((mask == 1) + (mask == 3), 255, 0).astype('uint8')print(mask2.shape)result = cv.bitwise_and(src, src, mask=mask2)cv.imshow("result", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>Grabcut图像分割</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-119-利用Grabcut图像分割进行背景替换]]></title>
    <url>%2F2019%2F05%2F25%2Fopencv-119%2F</url>
    <content type="text"><![CDATA[知识点使用Grabcut实现图像对象提取，通过背景图像替换，实现图像合成，通过对背景图像高斯模糊实现背景虚化效果，完整的步骤如下： ROI区域选择 Grabcut对象分割 Mask生成 使用mask，实现背景与前景的高斯权重融合 代码（python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647"""利用Grabcut图像分割进行背景替换"""import cv2 as cvimport numpy as npsrc = cv.imread("images/master.jpg")h, w = src.shape[:2]background = cv.imread("images/land.jpg")background = cv.resize(background, (w, h))cv.imshow("input", src)cv.imshow("background", background)# 分割，得到mask区域h, w, ch = src.shapemask = np.zeros(src.shape[:2], dtype=np.uint8)rect = (53,12,w-100,h-12)bgdmodel = np.zeros((1,65),np.float64)fgdmodel = np.zeros((1,65),np.float64)cv.grabCut(src,mask,rect,bgdmodel,fgdmodel,5,mode=cv.GC_INIT_WITH_RECT)mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')# 高斯模糊se = cv.getStructuringElement(cv.MORPH_RECT, (3, 3))cv.dilate(mask2, se, mask2)mask2 = cv.GaussianBlur(mask2, (5, 5), 0)cv.imshow('background-mask',mask2)# 虚化背景background = cv.GaussianBlur(background, (0, 0), 15)# 混合图像result = np.zeros((h, w, ch), dtype=np.uint8)for row in range(h): for col in range(w): w1 = mask2[row, col] / 255.0 b, g, r = src[row, col] b1,g1,r1 = background[row, col] b = (1.0-w1) * b1 + b * w1 g = (1.0-w1) * g1 + g * w1 r = (1.0-w1) * r1 + r * w1 result[row, col] = (b, g, r)cv.imshow("result", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>利用Grabcut图像分割进行背景替换</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-120-二维码检测与识别]]></title>
    <url>%2F2019%2F05%2F25%2Fopencv-120%2F</url>
    <content type="text"><![CDATA[知识点OpenCV在对象检测模块中QRCodeDetector有两个相关API分别实现二维码检测与二维码解析: 12345678910111213141516171819202122232425# 检测二维码bool cv::QRCodeDetector::detect( InputArray img, OutputArray points )constimg输入图像，灰度或者彩色图像points 得到的二维码四个点的坐标信息# 解析二维码std::string cv::QRCodeDetector::decode( InputArray img, InputArray points, OutputArray straight_qrcode = noArray() )img输入图像，灰度或者彩色图像points 二维码ROI最小外接矩形顶点坐标qrcode 输出的是二维码区域ROI图像信息返回的二维码utf-8字符串上述两个API功能，可以通过一个API调用实现，该API如下：std::string cv::QRCodeDetector::detectAndDecode( InputArray img, OutputArray points = noArray(), OutputArray straight_qrcode = noArray() ) 代码（c++,python）123456789101112131415161718192021222324252627#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace std;int main(int argc, char** argv) &#123; Mat src = imread("D:/images/qrcode.png"); imshow("image", src); Mat gray, qrcode_roi; cvtColor(src, gray, COLOR_BGR2GRAY); QRCodeDetector qrcode_detector; vector&lt;Point&gt; pts; string detect_info; bool det_result = qrcode_detector.detect(gray, pts); if (det_result) &#123; detect_info = qrcode_detector.decode(gray, pts, qrcode_roi); &#125; vector&lt; vector&lt;Point&gt; &gt; contours; contours.push_back(pts); drawContours(src, contours, 0, Scalar(0, 0, 255), 2); putText(src, detect_info.c_str(), Point(20, 200), FONT_HERSHEY_SIMPLEX, 1.0, Scalar(0, 0, 255), 2, 8); printf("qrcode info %s \n", detect_info.c_str()); imshow("result", src); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223"""二维码检测与识别"""import cv2 as cvimport numpy as npsrc = cv.imread("images/fanfan.jpg")cv.imshow("image", src)gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)qrcoder = cv.QRCodeDetector()codeinfo, points, straight_qrcode = qrcoder.detectAndDecode(gray)print(points)result = np.copy(src)cv.drawContours(result, [np.int32(points)], 0, (0, 0, 255), 2)print("qrcode : %s" % codeinfo)cv.imshow("result", result)code_roi = np.uint8(straight_qrcode)cv.namedWindow("qrcode roi", cv.WINDOW_NORMAL)cv.imshow("qrcode roi", code_roi)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>二维码检测与识别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-116-决策树算法 介绍与使用]]></title>
    <url>%2F2019%2F05%2F25%2Fopencv-116%2F</url>
    <content type="text"><![CDATA[知识点OpenCV中机器学习模块的决策树算法分为两个类别，一个是随机森林(Random Trees)、另外一个强化分类(Boosting Classification)。这两个算法都属于决策树算法。 1234567virtual float cv::ml::StatModel::predict( InputArray samples, OutputArray results = noArray(), int flags = 0 )constsample输入样本result预测结果 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace cv::ml;using namespace std;int main(int argc, char** argv) &#123; Mat data = imread("D:/projects/opencv_tutorial/data/images/digits.png"); Mat gray; cvtColor(data, gray, COLOR_BGR2GRAY); // 分割为5000个cells Mat images = Mat::zeros(5000, 400, CV_8UC1); Mat labels = Mat::zeros(5000, 1, CV_8UC1); int index = 0; Rect roi; roi.x = 0; roi.height = 1; roi.width = 400; for (int row = 0; row &lt; 50; row++) &#123; int label = row / 5; int offsety = row * 20; for (int col = 0; col &lt; 100; col++) &#123; int offsetx = col * 20; Mat digit = Mat::zeros(Size(20, 20), CV_8UC1); for (int sr = 0; sr &lt; 20; sr++) &#123; for (int sc = 0; sc &lt; 20; sc++) &#123; digit.at&lt;uchar&gt;(sr, sc) = gray.at&lt;uchar&gt;(sr + offsety, sc + offsetx); &#125; &#125; Mat one_row = digit.reshape(1, 1); printf("index : %d \n", index); roi.y = index; one_row.copyTo(images(roi)); labels.at&lt;uchar&gt;(index, 0) = label; index++; &#125; &#125; printf("load sample hand-writing data...\n"); imwrite("D:/result.png", images); // 转换为浮点数 images.convertTo(images, CV_32FC1); labels.convertTo(labels, CV_32SC1); printf("load sample hand-writing data...\n"); // 开始训练 printf("Start to Random Trees train...\n"); Ptr&lt;RTrees&gt; model = RTrees::create(); /*model-&gt;setMaxDepth(10); model-&gt;setMinSampleCount(10); model-&gt;setRegressionAccuracy(0); model-&gt;setUseSurrogates(false); model-&gt;setMaxCategories(15); model-&gt;setPriors(Mat()); model-&gt;setCalculateVarImportance(true); model-&gt;setActiveVarCount(4); */ TermCriteria tc = TermCriteria(TermCriteria::MAX_ITER + TermCriteria::EPS, 100, 0.01); model-&gt;setTermCriteria(tc); Ptr&lt;ml::TrainData&gt; tdata = ml::TrainData::create(images, ml::ROW_SAMPLE, labels); model-&gt;train(tdata); model-&gt;save("D:/vcworkspaces/rtrees_knowledge.yml"); printf("Finished Random trees...\n"); waitKey(0); return true;&#125; 12345678910111213141516171819202122232425262728293031323334"""决策树算法 介绍与使用"""import cv2 as cvimport numpy as np# 读取数据img = cv.imread('images/digits.png')gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)cells = [np.hsplit(row, 100) for row in np.vsplit(gray, 50)]x = np.array(cells)# 创建训练与测试数据train = x[:, :50].reshape(-1, 400).astype(np.float32)test = x[:, 50:100].reshape(-1, 400).astype(np.float32)k = np.arange(10)train_labels = np.repeat(k, 250)[:, np.newaxis]test_labels = train_labels.copy()# 训练随机树dt = cv.ml.RTrees_create()dt.train(train, cv.ml.ROW_SAMPLE, train_labels)retval, results = dt.predict(test)# 计算准确率matches = results == test_labelscorrect = np.count_nonzero(matches)accuracy = correct / results.sizeprint(accuracy)cv.waitKey(0)cv.destroyAllWindows() 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>决策树算法 介绍与使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-113-利用KMeans图像分割进行主色彩提取]]></title>
    <url>%2F2019%2F05%2F24%2Fopencv-113%2F</url>
    <content type="text"><![CDATA[知识点KMeans分割会计算出每个聚类的像素平均值，根据这个可以得到图像的主色彩RGB分布成分多少，得到各种色彩在图像中的比重，绘制出图像对应的取色卡！这个方面在纺织与填色方面特别有用！主要步骤显示如下： 读入图像建立KMenas样本 使用KMeans图像分割，指定分类数目 统计各个聚类占总像素比率，根据比率建立色卡！ 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace std;int main(int argc, char** argv) &#123; Mat src = imread("D:/images/master.jpg"); if (src.empty()) &#123; printf("could not load image...\n"); return -1; &#125; namedWindow("input image", WINDOW_AUTOSIZE); imshow("input image", src); int width = src.cols; int height = src.rows; int dims = src.channels(); // 初始化定义 int sampleCount = width*height; int clusterCount = 4; Mat labels; Mat centers; // RGB 数据转换到样本数据 Mat sample_data = src.reshape(3, sampleCount); Mat data; sample_data.convertTo(data, CV_32F); // 运行K-Means TermCriteria criteria = TermCriteria(TermCriteria::EPS + TermCriteria::COUNT, 10, 0.1); kmeans(data, clusterCount, labels, criteria, clusterCount, KMEANS_PP_CENTERS, centers); Mat card = Mat::zeros(Size(width, 50), CV_8UC3); vector&lt;float&gt; clusters(clusterCount); for (int i = 0; i &lt; labels.rows; i++) &#123; clusters[labels.at&lt;int&gt;(i, 0)]++; &#125; for (int i = 0; i &lt; clusters.size(); i++) &#123; clusters[i] = clusters[i] / sampleCount; &#125; int x_offset = 0; for (int x = 0; x &lt; clusterCount; x++) &#123; Rect rect; rect.x = x_offset; rect.y = 0; rect.height = 50; rect.width = round(clusters[x] * width); x_offset += rect.width; int b = centers.at&lt;float&gt;(x, 0); int g = centers.at&lt;float&gt;(x, 1); int r = centers.at&lt;float&gt;(x, 2); rectangle(card, rect, Scalar(b, g, r), -1, 8, 0); &#125; imshow("Image Color Card", card); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142"""利用KMeans图像分割进行主色彩提取"""import cv2 as cvimport numpy as npimage = cv.imread('images/toux.jpg')cv.imshow("input", image)h, w, ch = image.shape# 构建图像数据data = image.reshape((-1, 3))data = np.float32(data)# 图像分割criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 1.0)num_clusters = 4ret, label, center = cv.kmeans(data, num_clusters, None, criteria, num_clusters, cv.KMEANS_RANDOM_CENTERS)# 生成主色彩条形卡片card = np.zeros((50, w, 3), dtype=np.uint8)clusters = np.zeros([4], dtype=np.int32)for i in range(len(label)): clusters[label[i][0]] += 1# 计算各类别像素的比率clusters = np.float32(clusters) / float(h*w)center = np.int32(center)x_offset = 0for c in range(num_clusters): dx = np.int(clusters[c] * w) b = center[c][0] g = center[c][1] r = center[c][2] cv.rectangle(card, (x_offset, 0), (x_offset+dx, 50), (int(b),int(g),int(r)), -1) x_offset += dxcv.imshow("color table", card)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>利用KMeans图像分割进行主色彩提取</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-114-KNN算法介绍]]></title>
    <url>%2F2019%2F05%2F24%2Fopencv-114%2F</url>
    <content type="text"><![CDATA[知识点OpenCV中机器学习模块的最近邻算法KNN， 使用KNN算法实现手写数字识别，OpenCV在sample/data中有一张自带的手写数字数据集图像，0~9 每个有500个样本，总计有5000个数字。图像大小为1000x2000的大小图像，分割为20x20大小的单个数字图像，每个样本400个像素。然后使用KNN相关API实现训练与结果的保存。大致的顺序如下： 读入测试图像digit.png(可以在我的github下载，不知道地址看置顶帖子) 构建样本数据与标签 创建KNN训练并保存训练结果 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace cv::ml;using namespace std;int main(int argc, char** argv) &#123; Mat data = imread("D:/projects/opencv_tutorial/data/images/digits.png"); Mat gray; cvtColor(data, gray, COLOR_BGR2GRAY); // 分割为5000个cells Mat images = Mat::zeros(5000, 400, CV_8UC1); Mat labels = Mat::zeros(5000, 1, CV_8UC1); Rect rect; rect.height = 20; rect.width = 20; int index = 0; Rect roi; roi.x = 0; roi.height = 1; roi.width = 400; for (int row = 0; row &lt; 50; row++) &#123; int label = row / 5; for (int col = 0; col &lt; 100; col++) &#123; Mat digit = Mat::zeros(20, 20, CV_8UC1); index = row * 100 + col; rect.x = col * 20; rect.y = row * 20; gray(rect).copyTo(digit); Mat one_row = digit.reshape(1, 1); roi.y = index; one_row.copyTo(images(roi)); labels.at&lt;uchar&gt;(index, 0) = label; &#125; &#125; printf("load sample hand-writing data...\n"); // 转换为浮点数 images.convertTo(images, CV_32FC1); labels.convertTo(labels, CV_32SC1); // 开始KNN训练 printf("Start to knn train...\n"); Ptr&lt;KNearest&gt; knn = KNearest::create(); knn-&gt;setDefaultK(5); knn-&gt;setIsClassifier(true); Ptr&lt;ml::TrainData&gt; tdata = ml::TrainData::create(images, ml::ROW_SAMPLE, labels); knn-&gt;train(tdata); knn-&gt;save("D:/vcworkspaces/knn_knowledge.yml"); printf("Finished KNN...\n"); return true;&#125; 12345678910111213141516171819202122232425262728293031"""KNN算法介绍"""import cv2 as cvimport numpy as np# 读取数据img = cv.imread("images/digits.png")gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)cells = [np.hsplit(row, 100) for row in np.vsplit(gray, 50)]x = np.array(cells)# 创建训练与测试数据train = x[:, :50].reshape(-1, 400).astype(np.float32)test = x[:, 50:100].reshape(-1, 400).astype(np.float32)k = np.arange(10)train_labels = np.repeat(k,250)[:, np.newaxis]test_labels = train_labels.copy()# 训练KNNknn = cv.ml.KNearest_create()knn.train(train, cv.ml.ROW_SAMPLE, train_labels)ret, result, neighbours, dist = knn.findNearest(test, k=5)# 计算准确率matches = result == test_labelscorrect = np.count_nonzero(matches)acc = correct * 100.0 / result.sizeprint(acc)# 预测准确率： 91.76 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>KNN算法介绍</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-115-KNN算法的使用]]></title>
    <url>%2F2019%2F05%2F24%2Fopencv-115%2F</url>
    <content type="text"><![CDATA[知识点OpenCV中机器学习模块的最近邻算法KNN， 对使用KNN训练好的XML文件，可以通过算法接口的load方法加载成为KNN分类器，使用findNearest方法进行预测。OpenCV KNN预测方法参数解释如下： 123456789101112virtual float cv::ml::KNearest::findNearest( InputArray samples, int k, OutputArray results, OutputArray neighborResponses = noArray(), OutputArray dist = noArray() )其中sample是待预测的数据样本K表示选择最近邻的数目Result表示预测结果neighborResponses表示每个样本的前k个邻居dist表示每个样本前k的邻居的距离 注意：Python中API没有load方法 代码（c++）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace cv::ml;using namespace std;void knn_test(Mat&amp; data, Mat&amp; labels);int main(int argc, char** argv) &#123; Mat data = imread("D:/projects/opencv_tutorial/data/images/digits.png"); Mat gray; cvtColor(data, gray, COLOR_BGR2GRAY); // 分割为5000个cells Mat images = Mat::zeros(5000, 400, CV_8UC1); Mat labels = Mat::zeros(5000, 1, CV_8UC1); int index = 0; Rect roi; roi.x = 0; roi.height = 1; roi.width = 400; for (int row = 0; row &lt; 50; row++) &#123; int label = row / 5; int offsety = row * 20; for (int col = 0; col &lt; 100; col++) &#123; int offsetx = col * 20; Mat digit = Mat::zeros(Size(20, 20), CV_8UC1); for (int sr = 0; sr &lt; 20; sr++) &#123; for (int sc = 0; sc &lt; 20; sc++) &#123; digit.at&lt;uchar&gt;(sr, sc) = gray.at&lt;uchar&gt;(sr + offsety, sc + offsetx); &#125; &#125; Mat one_row = digit.reshape(1, 1); printf("index : %d \n", index); roi.y = index; one_row.copyTo(images(roi)); labels.at&lt;uchar&gt;(index, 0) = label; index++; &#125; &#125; printf("load sample hand-writing data...\n"); imwrite("D:/result.png", images); // 转换为浮点数 images.convertTo(images, CV_32FC1); labels.convertTo(labels, CV_32SC1); printf("load sample hand-writing data...\n"); // 开始KNN训练 printf("Start to knn train...\n"); Ptr&lt;KNearest&gt; knn = KNearest::create(); knn-&gt;setDefaultK(5); knn-&gt;setIsClassifier(true); Ptr&lt;ml::TrainData&gt; tdata = ml::TrainData::create(images, ml::ROW_SAMPLE, labels); knn-&gt;train(tdata); knn-&gt;save("D:/vcworkspaces/knn_knowledge.yml"); printf("Finished KNN...\n"); // 测试KNN printf("start to test knn...\n"); knn_test(images, labels); waitKey(0); return true;&#125;void knn_test(Mat&amp; data, Mat&amp; labels) &#123; // 加载KNN分类器 Ptr&lt;ml::KNearest&gt; knn = Algorithm::load&lt;ml::KNearest&gt;("D:/vcworkspaces/knn_knowledge.yml"); Mat result; knn-&gt;findNearest(data, 5, result); float count = 0; for (int row = 0; row &lt; result.rows; row++) &#123; int predict = result.at&lt;float&gt;(row, 0); if (labels.at&lt;int&gt;(row, 0) == predict) &#123; count++; &#125; &#125; printf("test acc of digit numbers : %.2f \n ", (count / result.rows)); // real test it Mat t1 = imread("D:/images/knn_01.png", IMREAD_GRAYSCALE); Mat t2 = imread("D:/images/knn_02.png", IMREAD_GRAYSCALE); imshow("t1", t1); imshow("t2", t2); Mat m1, m2; resize(t1, m1, Size(20, 20)); resize(t2, m2, Size(20, 20)); Mat testdata = Mat::zeros(2, 400, CV_8UC1); Mat testlabels = Mat::zeros(2, 1, CV_32SC1); Rect rect; rect.x = 0; rect.y = 0; rect.height = 1; rect.width = 400; Mat one = m1.reshape(1, 1); Mat two = m2.reshape(1, 1); one.copyTo(testdata(rect)); rect.y = 1; two.copyTo(testdata(rect)); testlabels.at&lt;int&gt;(0, 0) = 1; testlabels.at&lt;int&gt;(1, 0) = 2; testdata.convertTo(testdata, CV_32F); Mat result2; knn-&gt;findNearest(testdata, 5, result2); for (int i = 0; i&lt; result2.rows; i++) &#123; int predict = result2.at&lt;float&gt;(i, 0); printf("knn t%d predict : %d, actual label ：%d \n",(i+1), predict, testlabels.at&lt;int&gt;(i, 0)); &#125;&#125; 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>KNN算法使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-112-利用KMeans图像分割进行背景替换]]></title>
    <url>%2F2019%2F05%2F24%2Fopencv-112%2F</url>
    <content type="text"><![CDATA[知识点KMeans可以实现简单的证件照片的背景分割提取与替换，大致可以分为如下几步实现 读入图像建立KMenas样本 使用KMeans图像分割，指定指定分类数目 取左上角的label得到背景cluster index 生成mask区域，然后高斯模糊进行背景替换 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace std;int main(int argc, char** argv) &#123; Mat src = imread("D:/projects/opencv_tutorial/data/images/toux.jpg"); if (src.empty()) &#123; printf("could not load image...\n"); return -1; &#125; namedWindow("input image", WINDOW_AUTOSIZE); imshow("input image", src); int width = src.cols; int height = src.rows; int dims = src.channels(); // 初始化定义 int sampleCount = width*height; int clusterCount = 3; Mat labels; Mat centers; // RGB 数据转换到样本数据 Mat sample_data = src.reshape(3, sampleCount); Mat data; sample_data.convertTo(data, CV_32F); // 运行K-Means TermCriteria criteria = TermCriteria(TermCriteria::EPS + TermCriteria::COUNT, 10, 0.1); kmeans(data, clusterCount, labels, criteria, clusterCount, KMEANS_PP_CENTERS, centers); Mat mask = Mat::zeros(src.size(), CV_8UC1); int index = labels.at&lt;int&gt;(0, 0); labels = labels.reshape(1, height); for (int row = 0; row &lt; height; row++) &#123; for (int col = 0; col &lt; width; col++) &#123; int c = labels.at&lt;int&gt;(row, col); if (c == index) &#123; mask.at&lt;uchar&gt;(row, col) = 255; &#125; &#125; &#125; Mat se = getStructuringElement(MORPH_RECT, Size(3, 3), Point(-1, -1)); dilate(mask, mask, se); GaussianBlur(mask, mask, Size(5, 5), 0); Mat result = Mat::zeros(src.size(), CV_8UC3); for (int row = 0; row &lt; height; row++) &#123; for (int col = 0; col &lt; width; col++) &#123; float w1 = mask.at&lt;uchar&gt;(row, col) / 255.0; Vec3b bgr = src.at&lt;Vec3b&gt;(row, col); bgr[0] = w1 * 255.0 + bgr[0] * (1.0 - w1); bgr[1] = w1 * 0 + bgr[1] * (1.0 - w1); bgr[2] = w1 * 255.0 + bgr[2] * (1.0 - w1); result.at&lt;Vec3b&gt;(row, col) = bgr; &#125; &#125; imshow("KMeans-image-Demo", result); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748"""利用KMeans图像分割进行背景替换"""import cv2 as cvimport numpy as npimage = cv.imread('images/toux.jpg')cv.imshow("input", image)h, w, ch = image.shape# 构建图像数据data = image.reshape((-1, 3))data = np.float32(data)# 图像分割criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 1.0)num_clusters = 4ret, label, center = cv.kmeans(data, num_clusters, None, criteria, num_clusters, cv.KMEANS_RANDOM_CENTERS)# 生成mask区域index = label[0][0]center = np.uint8(center)color = center[0]mask = np.zeros((h, w), dtype=np.uint8)label = np.reshape(label, (h, w))mask[label == index] = 255# 高斯模糊se = cv.getStructuringElement(cv.MORPH_RECT, (3, 3))cv.dilate(mask, se, mask)mask = cv.GaussianBlur(mask, (5, 5), 0)cv.imshow("background-mask", mask)# 背景替换result = np.zeros((h, w, ch), dtype=np.uint8)for row in range(h): for col in range(w): w1 = mask[row, col] / 255.0 b, g, r = image[row, col] b = w1 * 255 + b * (1.0 - w1) g = w1 * 0 + g * (1.0 - w1) r = w1 * 255 + r * (1.0 - w1) result[row, col] = (b, g, r)cv.imshow("background-substitution", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>利用KMeans图像分割进行背景替换</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python 列表生成器简单案例]]></title>
    <url>%2F2019%2F05%2F24%2Fpython_list_generation%2F</url>
    <content type="text"><![CDATA[场景一将文件中逐行读取的一个完整语句，按逗号分割单词，去掉首位的空字符，并过滤掉长度小于 3 的单词，最后返回由单词组成的列表。 123text = " today, is , sunday"text_list = [s.strip() for s in text.split(',') if len(s.strip()) &gt; 3]print(text_list) # out:['today', 'sunday'] 场景二给定下面两个列表 attributes 和 values，要求针对 values 中每一组子列表 value，输出其和 attributes 中的键对应后的字典，最后返回字典组成的列表。分别用一行代码和多行代码来写。 123456789attributes = ['name', 'dob', 'gender']values = [['jason', '2000-01-01', 'male'], ['mike', '1999-01-01', 'male'], ['nancy', '2001-02-01', 'female']]# expected outout:[&#123;'name': 'jason', 'dob': '2000-01-01', 'gender': 'male'&#125;, &#123;'name': 'mike', 'dob': '1999-01-01', 'gender': 'male'&#125;, &#123;'name': 'nancy', 'dob': '2001-02-01', 'gender': 'female'&#125;] 1234567891011121314151617181920# 多行代码l1 = []for value in values: dict1 = &#123;&#125; for i, attr in enumerate(attributes): dict1[attr] = value[i] l1.append(dict1)l2 = []for value in values: dict1 = dict(zip(attributes, value)) l2.append(dict1)# 一行代码l3 = [&#123;attr: value[i] for i, attr in enumerate(attributes)&#125; for value in values ]l4 = [dict(zip(attributes, value)) for value in values]print(l1 == l2 == l3 == l4)# out: True]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>列表生成器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-111-KMeans图像分割]]></title>
    <url>%2F2019%2F05%2F23%2Fopencv-111%2F</url>
    <content type="text"><![CDATA[知识点KMean不光可以对数据进行分类，还可以实现对图像分割，什么图像分割，简单的说就要图像的各种像素值，分割为几个指定类别颜色值，这种分割有两个应用，一个可以实现图像主色彩的简单提取，另外针对特定的应用场景可以实现证件照片的背景替换效果，这个方面早期最好的例子就是证件之星上面的背景替换。当然要想实现类似的效果，绝对不是简单的KMeans就可以做到的，还有一系列后续的交互操作需要完成。对图像数据来说，要把每个像素点作为单独的样本，按行组织。 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace std;int main(int argc, char** argv) &#123; Mat src = imread("D:/projects/opencv_tutorial/data/images/toux.jpg"); if (src.empty()) &#123; printf("could not load image...\n"); return -1; &#125; namedWindow("input image", WINDOW_AUTOSIZE); imshow("input image", src); Scalar colorTab[] = &#123; Scalar(0, 0, 255), Scalar(0, 255, 0), Scalar(255, 0, 0), Scalar(0, 255, 255), Scalar(255, 0, 255) &#125;; int width = src.cols; int height = src.rows; int dims = src.channels(); // 初始化定义 int sampleCount = width*height; int clusterCount = 3; Mat labels; Mat centers; // RGB 数据转换到样本数据 Mat sample_data = src.reshape(3, sampleCount); Mat data; sample_data.convertTo(data, CV_32F); // 运行K-Means TermCriteria criteria = TermCriteria(TermCriteria::EPS + TermCriteria::COUNT, 10, 0.1); kmeans(data, clusterCount, labels, criteria, clusterCount, KMEANS_PP_CENTERS, centers); // 显示图像分割结果 int index = 0; Mat result = Mat::zeros(src.size(), src.type()); for (int row = 0; row &lt; height; row++) &#123; for (int col = 0; col &lt; width; col++) &#123; index = row*width + col; int label = labels.at&lt;int&gt;(index, 0); result.at&lt;Vec3b&gt;(row, col)[0] = colorTab[label][0]; result.at&lt;Vec3b&gt;(row, col)[1] = colorTab[label][1]; result.at&lt;Vec3b&gt;(row, col)[2] = colorTab[label][2]; &#125; &#125; imshow("KMeans-image-Demo", result); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324252627"""KMeans 图像分割"""import cv2 as cvimport numpy as npimage = cv.imread('images/toux.jpg')cv.imshow("input", image)# 构建图像数据data = image.reshape((-1, 3))data = np.float32(data)# 图像分割criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 1.0)num_clusters = 4ret, label, center = cv.kmeans(data, num_clusters, None, criteria, num_clusters, cv.KMEANS_RANDOM_CENTERS)center = np.uint8(center)res = center[label.flatten()]# 显示result = res.reshape((image.shape))cv.imshow("result", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>KMeans图像分割</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-109-BLOB特征分析(simpleblobdetector使用)]]></title>
    <url>%2F2019%2F05%2F22%2Fopencv-109%2F</url>
    <content type="text"><![CDATA[知识点BLOB是图像中灰度块的一种专业称呼，更加变通一点的可以说它跟我们前面二值图像分析的联通组件类似，通过特征提取中的SimpleBlobDetector可以实现常见的各种灰度BLOB对象组件检测与分离。使用该检测器的时候，可以根据需要输入不同参数，得到的结果跟输入的参数息息相关。常见的BLOB分析支持如下： 根据BLOB面积过滤 根据灰度/颜色值过滤 根据圆度过滤 根据长轴与短轴过滤 根据凹凸进行过滤 参数列表 12345678910111213141516SimpleBlobDetector::Params::Params()bool filterByAreabool filterByCircularitybool filterByColorbool filterByConvexitybool filterByInertiafloat maxAreafloat maxCircularityfloat maxConvexityfloat maxInertiaRatiofloat maxThresholdfloat minAreafloat minCircularityfloat minConvexityfloat minDistBetweenBlobsfloat minInertiaRatio 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637#include "opencv2/opencv.hpp"#include &lt;iostream&gt;using namespace cv;using namespace std;int main(int argc, char** argv)&#123; // 加载图像 Mat src = imread("D:/images/blob2.png"); Mat gray; cvtColor(src, gray, COLOR_BGR2GRAY); // 初始化参数设置 SimpleBlobDetector::Params params; params.minThreshold = 10; params.maxThreshold = 200; params.filterByArea = true; params.minArea = 100; params.filterByCircularity = true; params.minCircularity = 0.1; params.filterByConvexity = true; params.minConvexity = 0.87; params.filterByInertia = true; params.minInertiaRatio = 0.01; // 创建BLOB Detetor Ptr&lt;SimpleBlobDetector&gt; detector = SimpleBlobDetector::create(params); // BLOB分析与显示 Mat result; vector&lt;KeyPoint&gt; keypoints; detector-&gt;detect(gray, keypoints); drawKeypoints(src, keypoints, result, Scalar(0, 0, 255), DrawMatchesFlags::DRAW_RICH_KEYPOINTS); imshow("Blob Detection Demo", result); waitKey(0);&#125; 12345678910111213141516171819202122232425262728293031323334353637383940"""BLOB特征分析(simpleblobdetector使用)"""import cv2 as cvframe = cv.imread("images/zhifang_ball.png")cv.imshow("input", frame)gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)params = cv.SimpleBlobDetector_Params()# change thresholdsparams.minThreshold = 0params.maxThreshold = 256# filter by areaparams.filterByArea = Trueparams.minArea = 100# filter by circularityparams.filterByCircularity = Trueparams.minCircularity = 0.1# Filter by Convexityparams.filterByConvexity = Trueparams.minConvexity = 0.5# Filter by Inertiaparams.filterByInertia = Trueparams.minInertiaRatio = 0.5# 提取关键点detector = cv.SimpleBlobDetector_create(params)keypoints = detector.detect(gray)for marker in keypoints: result = cv.drawMarker(frame, tuple(int(i) for i in marker.pt), color=(0, 255, 0))cv.imshow("result", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>BLOB特征分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-108-特征提取之关键点检测(GFTTDetector)]]></title>
    <url>%2F2019%2F05%2F22%2Fopencv-108%2F</url>
    <content type="text"><![CDATA[知识点该方法是基于shi-tomas角点检测变化而来的一种特征提取方法，OpenCV创建该检测器的API与goodfeaturetotrack的API参数极其类似： 1234567891011Ptr&lt;GFTTDetector&gt; cv::GFTTDetector::create(int maxCorners = 1000,double qualityLevel = 0.01,double minDistance = 1,int blockSize = 3,bool useHarrisDetector = false,double k = 0.04 )唯一不同的，该方法返回一个指针。该方法无法提取描述子，只支持提取关键点！ 代码（c++,python）123456789101112131415161718#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace std;int main(int argc, char** argv) &#123; Mat src = imread("D:/images/test1.png"); auto keypoint_detector = GFTTDetector::create(1000, 0.01, 1.0, 3, false, 0.04); vector&lt;KeyPoint&gt; kpts; keypoint_detector-&gt;detect(src, kpts); Mat result = src.clone(); drawKeypoints(src, kpts, result, Scalar::all(-1), DrawMatchesFlags::DEFAULT); imshow("GFTT-Keypoint-Detect", result); imwrite("D:/result.png", result); waitKey(0); return 0;&#125; 123456789101112131415161718"""特征提取之关键点检测(GFTTDetector)"""import cv2 as cvimage = cv.imread("images/test4.jpg")cv.imshow("input", image)# 创建GFTT特征检测器gftt = cv.GFTTDetector_create(100, 0.01, 1, 3, False, 0.04)kp1 = gftt.detect(image, None)for marker in kp1: result = cv.drawMarker(image, tuple(int(i) for i in marker.pt), color=(0, 255, 0))cv.imshow("GFTT-Keypoint-Detect", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>特征提取之关键点检测(GFTTDetector)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-110-KMeans进行数据分类]]></title>
    <url>%2F2019%2F05%2F22%2Fopencv-110%2F</url>
    <content type="text"><![CDATA[知识点K-Means算法的作者是MacQueen， K-Means的算法是对数据进行分类的算法，采用的硬分类方式，是属于非监督学习的算法，预先要求知道分为几个类别，然后每个类别有一个中心点，根据距离度量来决定每个数据点属于哪个类别标签，一次循环实现对所有数据点分类之后，会根据标签重新计算各个类型的中心位置，然后继续循环数据集再次分类标签样本数据，如此不断迭代，直到指定的循环数目或者前后两次delta小于指定阈值，停止计算，得到最终各个样本数据的标签。 API 12345678910111213141516171819double cv::kmeans( InputArray data, int K, InputOutputArray bestLabels, TermCriteria criteria, int attempts, int flags, OutputArray centers = noArray() )data表示输入的样本数据，必须是按行组织样本，每一行为一个样本数据，列表示样本的维度K表示最终的分类数目bestLabels 表示最终分类每个样本的标签criteria 表示KMeans分割的停止条件attempts 表示采样不同初始化标签尝试次数flag表示中心初始化方法- KMEANS_RANDOM_CENTERS - KMEANS_PP_CENTERS - KMEANS_USE_INITIAL_LABELScenters表示最终分割以后的每个cluster的中心位置 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace std;int main(int argc, char** argv) &#123; Mat img(500, 500, CV_8UC3); RNG rng(12345); Scalar colorTab[] = &#123; Scalar(0, 0, 255), Scalar(255, 0, 0), &#125;; int numCluster = 2; int sampleCount = rng.uniform(5, 500); Mat points(sampleCount, 1, CV_32FC2); // 生成随机数 for (int k = 0; k &lt; numCluster; k++) &#123; Point center; center.x = rng.uniform(0, img.cols); center.y = rng.uniform(0, img.rows); Mat pointChunk = points.rowRange(k*sampleCount / numCluster, k == numCluster - 1 ? sampleCount : (k + 1)*sampleCount / numCluster); rng.fill(pointChunk, RNG::NORMAL, Scalar(center.x, center.y), Scalar(img.cols*0.05, img.rows*0.05)); &#125; randShuffle(points, 1, &amp;rng); // 使用KMeans Mat labels; Mat centers; kmeans(points, numCluster, labels, TermCriteria(TermCriteria::EPS + TermCriteria::COUNT, 10, 0.1), 3, KMEANS_PP_CENTERS, centers); // 用不同颜色显示分类 img = Scalar::all(255); for (int i = 0; i &lt; sampleCount; i++) &#123; int index = labels.at&lt;int&gt;(i); Point p = points.at&lt;Point2f&gt;(i); circle(img, p, 2, colorTab[index], -1, 8); &#125; // 每个聚类的中心来绘制圆 for (int i = 0; i &lt; centers.rows; i++) &#123; int x = centers.at&lt;float&gt;(i, 0); int y = centers.at&lt;float&gt;(i, 1); printf("c.x= %d, c.y=%d", x, y); circle(img, Point(x, y), 40, colorTab[i], 1, LINE_AA); &#125; imshow("KMeans-Data-Demo", img); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526272829303132333435363738"""KMeans进行数据分类"""import cv2 as cvimport numpy as npfrom matplotlib import pyplot as pltX = np.random.randint(25, 50, (25, 2))Y = np.random.randint(60, 85, (25, 2))pts = np.vstack((X, Y))# 初始化数据data = np.float32(pts)print(data.shape)# 定义停止条件criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 1.0)# kmeans分类ret, label, center = cv.kmeans(data, 2, None, criteria, 2, cv.KMEANS_RANDOM_CENTERS)print(label.shape)print(center)# 获取不同标签的点A = data[label.ravel() == 0]B = data[label.ravel() == 1]# plot the dataplt.scatter(A[:, 0], A[:, 1])plt.scatter(B[:, 0], B[:, 1], c='r')plt.scatter(center[:, 0], center[:, 1], s=80, c='y', marker='s')plt.xlabel("x1")plt.ylabel("x2")plt.show()cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>KMeans进行数据分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-106-AKAZE特征与描述子]]></title>
    <url>%2F2019%2F05%2F21%2Fopencv-106%2F</url>
    <content type="text"><![CDATA[知识点AKAZE特征提取算法是局部特征描述子算法，可以看成是SIFT算法的改进、采用非线性扩散滤波迭代来提取与构建尺度空间、采用与SIFT类似的方法寻找特征点、在描述子生成阶段采用ORB类似的方法生成描述子，但是描述子比ORB多了旋转不变性特征。ORB采用LDB方法，AKAZE采用 M-LDB。 代码（c++,python）12345678910111213141516171819202122232425262728293031#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace std;int main(int argc, char** argv) &#123; Mat box = imread("D:/images/box.png"); Mat box_in_sence = imread("D:/images/box_in_scene.png"); // 创建AKAZE auto akaze_detector = AKAZE::create(); vector&lt;KeyPoint&gt; kpts_01, kpts_02; Mat descriptors1, descriptors2; akaze_detector-&gt;detectAndCompute(box, Mat(), kpts_01, descriptors1); akaze_detector-&gt;detectAndCompute(box_in_sence, Mat(), kpts_02, descriptors2); // 定义描述子匹配 - 暴力匹配 Ptr&lt;DescriptorMatcher&gt; matcher = DescriptorMatcher::create(DescriptorMatcher::BRUTEFORCE); std::vector&lt; DMatch &gt; matches; matcher-&gt;match(descriptors1, descriptors2, matches); // 绘制匹配 Mat img_matches; drawMatches(box, kpts_01, box_in_sence, kpts_02, matches, img_matches); imshow("AKAZE-Matches", img_matches); imwrite("D:/result.png", img_matches); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526"""AKAZE特征与描述子"""import cv2 as cvbox = cv.imread("images/box.png")box_in_scene = cv.imread("images/box_in_scene.png")# 创建AKAZE特征检测器akaze = cv.AKAZE_create()# 得到特征关键点和描述子kp1, des1 = akaze.detectAndCompute(box, None)kp2, des2 = akaze.detectAndCompute(box_in_scene, None)# 暴力匹配bf = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True)matchers = bf.match(des1, des2)# 绘制匹配result = cv.drawMatches(box, kp1, box_in_scene, kp2, matchers, None)cv.imshow("orb-match", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>AKAZE特征与描述子</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-107-Brisk特征提取与描述子匹配]]></title>
    <url>%2F2019%2F05%2F21%2Fopencv-107%2F</url>
    <content type="text"><![CDATA[知识点BRISK(Binary robust invariant scalable keypoints)是一种基于尺度空间不变性类似ORB特征描述子的特征提取算法。BRISK主要步骤可以分为如下两步： 构建尺度空间金字塔实现关键点定位 根据关键点生成描述子 代码（c++,python）12345678910111213141516171819202122232425262728293031#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace std;int main(int argc, char** argv) &#123; Mat box = imread("D:/images/box.png"); Mat box_in_sence = imread("D:/images/box_in_scene.png"); // 创建BRISK auto brisk_detector = BRISK::create(); vector&lt;KeyPoint&gt; kpts_01, kpts_02; Mat descriptors1, descriptors2; brisk_detector-&gt;detectAndCompute(box, Mat(), kpts_01, descriptors1); brisk_detector-&gt;detectAndCompute(box_in_sence, Mat(), kpts_02, descriptors2); // 定义描述子匹配 - 暴力匹配 Ptr&lt;DescriptorMatcher&gt; matcher = DescriptorMatcher::create(DescriptorMatcher::BRUTEFORCE); std::vector&lt; DMatch &gt; matches; matcher-&gt;match(descriptors1, descriptors2, matches); // 绘制匹配 Mat img_matches; drawMatches(box, kpts_01, box_in_sence, kpts_02, matches, img_matches); imshow("AKAZE-Matches", img_matches); imwrite("D:/result.png", img_matches); waitKey(0); return 0;&#125; 123456789101112131415161718192021import cv2 as cvbox = cv.imread("D:/images/box.png");box_in_sence = cv.imread("D:/images/box_in_scene.png");cv.imshow("box", box)cv.imshow("box_in_sence", box_in_sence)# 创建BRISK特征检测器brisk = cv.BRISK_create()kp1, des1 = brisk.detectAndCompute(box,None)kp2, des2 = brisk.detectAndCompute(box_in_sence,None)# 暴力匹配bf = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True)matches = bf.match(des1,des2)# 绘制匹配result = cv.drawMatches(box, kp1, box_in_sence, kp2, matches, None)cv.imshow("orb-match", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>Brisk特征提取与描述子匹配</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-105-使用HOG进行对象检测]]></title>
    <url>%2F2019%2F05%2F21%2Fopencv-105%2F</url>
    <content type="text"><![CDATA[知识点对于已经训练好的HOG+SVM的模型，我们可以通过开窗实现对象检测，从而完成自定义对象检测。以电表检测为例，这样我们就实现HOG+SVM对象检测全流程。OpenCV中实现对每个窗口像素块预测，需要首先加载SVM模型文件，然后使用predict方法实现预测。这种方法的缺点就是开窗检测是从左到右、从上到下，是一个高耗时的操作，所以步长选择一般会选择HOG窗口默认步长的一半，这样可以减少检测框的数目，同时在predict时候会发现多个重复框，求取它们的平均值即可得到最终的检测框。 代码（python）1234567891011121314151617181920212223242526272829303132333435363738394041"""使用HOG进行对象检测"""import cv2 as cvimport numpy as npimage = cv.imread("images/elec_watch/test/scene_01.jpg")test_image = cv.resize(image, (0, 0), fx=0.2, fy=0.2)cv.imshow("input", test_image)gray = cv.cvtColor(test_image, cv.COLOR_BGR2GRAY)print(test_image.shape)h, w = test_image.shape[:2]svm = cv.ml.SVM_load("svm_data.dat")sum_x = 0sum_y = 0count = 0hog = cv.HOGDescriptor()for row in range(64, h-64, 4): for col in range(32, w-32, 4): win_roi = gray[row-64:row+64,col-32:col+32] hog_desc = hog.compute(win_roi, winStride=(8, 8), padding=(0, 0)) one_fv = np.zeros([len(hog_desc)], dtype=np.float32) for i in range(len(hog_desc)): one_fv[i] = hog_desc[i][0] one_fv = np.reshape(one_fv, [-1, len(hog_desc)]) result = svm.predict(one_fv)[1] if result[0][0] &gt; 0: sum_x += (col-32) sum_y += (row-64) count += 1x = sum_x // county = sum_y // countcv.rectangle(test_image, (x, y), (x+64, y+128), (0, 0, 255), 2, 8, 0)cv.imshow("result", test_image)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>使用HOG进行对象检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-103-HOG特征描述子之使用描述子特征生成样本数据]]></title>
    <url>%2F2019%2F05%2F21%2Fopencv-103%2F</url>
    <content type="text"><![CDATA[知识点对于HOG特征，我们可以通过预先训练的特征数据，进行多尺度的对象检测，OpenCV中基于HOG的行人检测是一个典型案例，同时我们还可以实现自定义对象的检测，这种自定义对象检测，可以分为两个部分，第一部分：通过提取样本的HOG描述子，生成样本的特征数据，第二部分通过SVM进行分类学习与训练，保存为模型。这样我们以后就可以通过模型来实现自定义对象检测啦。今天我们分享第二部分，使用HOG描述子特征数据生成数据集，进行SVM分类训练，实现对象分类识别。 这里我已一个很常见的应用，电表检测为例，这类问题早期主要通过特征匹配实现，但是这个方法比较容易受到各种因素干扰，不是很好，通过提取HOG特征、进行SVM特征分类、然后开窗检测，是一个很好的解决方法。 在OpenCV中训练SVM模型，其数据格式常见的是“行模式”就是一行（多列向量）是一个样本，对应一个整数标签(label)。这里采用默认的窗口大小为64x128 提取HOG特征向量，得到的每个样本的向量数目等于7x15x36=3780，有多少个样本就有多少行， 对于的标签是每一行对应自己的标签，有多少个训练样本，标签就有多少行！ 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace cv::ml;using namespace std;string positive_dir = "D:/images/train_data/elec_watch/positive/";string negative_dir = "D:/images/train_data/elec_watch/negative/";void get_hog_descripor(Mat &amp;image, vector&lt;float&gt; &amp;desc);void generate_dataset(Mat &amp;trainData, Mat &amp;labels);int main(int argc, char** argv) &#123; Mat trainData = Mat::zeros(Size(3780, 26), CV_32FC1); Mat labels = Mat::zeros(Size(1, 26), CV_32SC1); generate_dataset(trainData, labels); waitKey(0); return 0;&#125;void get_hog_descripor(Mat &amp;image, vector&lt;float&gt; &amp;desc) &#123; HOGDescriptor hog; int h = image.rows; int w = image.cols; float rate = 64.0 / w; Mat img, gray; resize(image, img, Size(64, int(rate*h))); cvtColor(img, gray, COLOR_BGR2GRAY); Mat result = Mat::zeros(Size(64, 128), CV_8UC1); result = Scalar(127); Rect roi; roi.x = 0; roi.width = 64; roi.y = (128 - gray.rows) / 2; roi.height = gray.rows; gray.copyTo(result(roi)); hog.compute(result, desc, Size(8, 8), Size(0, 0));&#125;void generate_dataset(Mat &amp;trainData, Mat &amp;labels) &#123; vector&lt;string&gt; images; glob(positive_dir, images); int pos_num = images.size(); for (int i = 0; i &lt; images.size(); i++) &#123; Mat image = imread(images[i].c_str()); vector&lt;float&gt; fv; imshow("image", image); waitKey(0); get_hog_descripor(image, fv); printf("image path : %s, feature data length: %d \n", images[i].c_str(), fv.size()); for (int j = 0; j &lt; fv.size(); j++) &#123; trainData.at&lt;float&gt;(i, j) = fv[j]; &#125; labels.at&lt;int&gt;(i, 0) = 1; &#125; images.clear(); glob(negative_dir, images); for (int i = 0; i &lt; images.size(); i++) &#123; Mat image = imread(images[i].c_str()); vector&lt;float&gt; fv; imshow("image", image); waitKey(0); get_hog_descripor(image, fv); printf("image path : %s, feature data length: %d \n", images[i].c_str(), fv.size()); for (int j = 0; j &lt; fv.size(); j++) &#123; trainData.at&lt;float&gt;(i+pos_num, j) = fv[j]; &#125; labels.at&lt;int&gt;(i+ pos_num, 0) = -1; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556"""HOG特征描述子之使用描述子特征生成样本数据"""import cv2 as cvimport osimport numpy as npdef get_hog_descriptor(image): hog = cv.HOGDescriptor() h, w = image.shape[:2] # 预处理输入图像 rate = 64 / w image = cv.resize(image, (64, np.int(rate * h))) gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY) bg = np.zeros((128, 64), dtype=np.uint8) bg[:,:] = 127 h, w = gray.shape dy = (128 - h) // 2 bg[dy:h+dy,:] = gray cv.waitKey(0) # 64x128 = 3780 fv = hog.compute(bg, winStride=(8, 8), padding=(0, 0)) return fvdef generate_dataset(pdir, ndir): train_data = [] labels = [] for file_name in os.listdir(pdir): img_dir = os.path.join(pdir, file_name) img = cv.imread(img_dir) hog_desc = get_hog_descriptor(img) one_fv = np.zeros([len(hog_desc)], dtype=np.float32) for i in range(len(hog_desc)): one_fv[i] = hog_desc[i][0] train_data.append(one_fv) labels.append(1) for file_name in os.listdir(ndir): img_dir = os.path.join(ndir, file_name) img = cv.imread(img_dir) hog_desc = get_hog_descriptor(img) one_fv = np.zeros([len(hog_desc)], dtype=np.float32) for i in range(len(hog_desc)): one_fv[i] = hog_desc[i][0] train_data.append(one_fv) labels.append(-1) return np.array(train_data, dtype=np.float32), np.array(labels, dtype=np.int32)if __name__ == '__main__': generate_dataset("images/elec_watch/positive/", "images/elec_watch/negative/") cv.destroyAllWindows() 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>HOG特征描述子之使用描述子特征生成样本数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-104-HOG-SVM分类训练]]></title>
    <url>%2F2019%2F05%2F21%2Fopencv-104%2F</url>
    <content type="text"><![CDATA[知识点对于得到的结构化HOG特征数据，我们就可以通过初始化SVM进行回归分类训练，这里采用的训练器是SVM线性分类器，SVM还有另外一个分类器就是对于线性不可分数据的径向分类器。OpenCV中使用径向分类器SVM有时候会训练很长时间，而且结果很糟糕，甚至会报一些莫名其妙的错误，感觉不是特别好。所以推荐大家真对线性不可分的问题可以选择神经网络ANN模块。在训练之前，首先简单的认识一下SVM，我们这边是通过二分类来完成，是很典型的线性可分离的SVM。 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace cv::ml;using namespace std;string positive_dir = "D:/images/train_data/elec_watch/positive/";string negative_dir = "D:/images/train_data/elec_watch/negative/";void get_hog_descripor(Mat &amp;image, vector&lt;float&gt; &amp;desc);void generate_dataset(Mat &amp;trainData, Mat &amp;labels);void svm_train(Mat &amp;trainData, Mat &amp;labels);int main(int argc, char** argv) &#123; Mat trainData = Mat::zeros(Size(3780, 26), CV_32FC1); Mat labels = Mat::zeros(Size(1, 26), CV_32SC1); generate_dataset(trainData, labels); svm_train(trainData, labels); Ptr&lt;SVM&gt; svm = SVM::load("D:/vcworkspaces/hog_elec.yml"); Mat test_img = imread("D:/images/train_data/elec_watch/test/test_01.png"); // Mat test_img = imread("D:/images/train_data/elec_watch/positive/box_01.bmp"); imshow("test image", test_img); vector&lt;float&gt; fv; get_hog_descripor(test_img, fv); Mat one_row = Mat::zeros(Size(fv.size(), 1), CV_32FC1); for (int i = 0; i &lt; fv.size(); i++) &#123; one_row.at&lt;float&gt;(0, i) = fv[i]; &#125; float result = svm-&gt;predict(one_row); printf("\n prediction result : %.2f \n", result); waitKey(0); return 0;&#125;void svm_train(Mat &amp;trainData, Mat &amp;labels) &#123; printf("\n start SVM training... \n"); Ptr&lt; SVM &gt; svm = SVM::create(); /* Default values to train SVM */ svm-&gt;setGamma(5.383); svm-&gt;setKernel(SVM::LINEAR); svm-&gt;setC(2.67); svm-&gt;setType(SVM::C_SVC); svm-&gt;train(trainData, ROW_SAMPLE, labels); clog &lt;&lt; "...[done]" &lt;&lt; endl; // save xml svm-&gt;save("D:/vcworkspaces/hog_elec.yml");&#125;void get_hog_descripor(Mat &amp;image, vector&lt;float&gt; &amp;desc) &#123; HOGDescriptor hog; int h = image.rows; int w = image.cols; float rate = 64.0 / w; Mat img, gray; resize(image, img, Size(64, int(rate*h))); cvtColor(img, gray, COLOR_BGR2GRAY); Mat result = Mat::zeros(Size(64, 128), CV_8UC1); result = Scalar(127); Rect roi; roi.x = 0; roi.width = 64; roi.y = (128 - gray.rows) / 2; roi.height = gray.rows; gray.copyTo(result(roi)); hog.compute(result, desc, Size(8, 8), Size(0, 0));&#125;void generate_dataset(Mat &amp;trainData, Mat &amp;labels) &#123; vector&lt;string&gt; images; glob(positive_dir, images); int pos_num = images.size(); for (int i = 0; i &lt; images.size(); i++) &#123; Mat image = imread(images[i].c_str()); vector&lt;float&gt; fv; get_hog_descripor(image, fv); printf("image path : %s, feature data length: %d \n", images[i].c_str(), fv.size()); for (int j = 0; j &lt; fv.size(); j++) &#123; trainData.at&lt;float&gt;(i, j) = fv[j]; &#125; labels.at&lt;int&gt;(i, 0) = 1; &#125; images.clear(); glob(negative_dir, images); for (int i = 0; i &lt; images.size(); i++) &#123; Mat image = imread(images[i].c_str()); vector&lt;float&gt; fv; get_hog_descripor(image, fv); printf("image path : %s, feature data length: %d \n", images[i].c_str(), fv.size()); for (int j = 0; j &lt; fv.size(); j++) &#123; trainData.at&lt;float&gt;(i + pos_num, j) = fv[j]; &#125; labels.at&lt;int&gt;(i + pos_num, 0) = -1; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384"""HOG-SVM分类训练"""import cv2 as cvimport osimport numpy as npdef get_hog_descriptor(image): hog = cv.HOGDescriptor() h, w = image.shape[:2] rate = 64 / w image = cv.resize(image, (64, np.int(rate*h))) gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY) bg = np.zeros((128, 64), dtype=np.uint8) bg[:,:] = 127 h, w = gray.shape dy = (128 - h) // 2 bg[dy:h+dy,:] = gray cv.waitKey(0) # 64x128 = 3780 fv = hog.compute(bg, winStride=(8, 8), padding=(0, 0)) return fvdef generate_dataset(pdir, ndir): train_data = [] labels = [] for file_name in os.listdir(pdir): img_dir = os.path.join(pdir, file_name) img = cv.imread(img_dir) hog_desc = get_hog_descriptor(img) one_fv = np.zeros([len(hog_desc)], dtype=np.float32) for i in range(len(hog_desc)): one_fv[i] = hog_desc[i][0] train_data.append(one_fv) labels.append(1) for file_name in os.listdir(ndir): img_dir = os.path.join(ndir, file_name) img = cv.imread(img_dir) hog_desc = get_hog_descriptor(img) one_fv = np.zeros([len(hog_desc)], dtype=np.float32) for i in range(len(hog_desc)): one_fv[i] = hog_desc[i][0] train_data.append(one_fv) labels.append(-1) return np.array(train_data, dtype=np.float32), np.array(labels, dtype=np.int32)def svm_train(positive_dir, negative_dir): svm = cv.ml.SVM_create() svm.setKernel(cv.ml.SVM_LINEAR) svm.setType(cv.ml.SVM_C_SVC) svm.setC(2.67) svm.setGamma(5.383) trainData, responses = generate_dataset(positive_dir, negative_dir) responses = np.reshape(responses, [-1, 1]) svm.train(trainData, cv.ml.ROW_SAMPLE, responses) svm.save('svm_data.dat')def elec_detect(image): hog_desc = get_hog_descriptor(image) print(len(hog_desc)) one_fv = np.zeros([len(hog_desc)], dtype=np.float32) for i in range(len(hog_desc)): one_fv[i] = hog_desc[i][0] one_fv = np.reshape(one_fv, [-1, len(hog_desc)]) print(len(one_fv), len(one_fv[0])) svm = cv.ml.SVM_load('svm_data.dat') result = svm.predict(one_fv)[1] print(result)if __name__ == '__main__': svm_train("images/elec_watch/positive/", "images/elec_watch/negative/") cv.waitKey(0) # test_img = cv.imread("images/elec_watch/test/scene_01.jpg") # elec_detect(test_img) # cv.destroyAllWindows() 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>HOG-SVM分类训练</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-102-HOG特征描述子之提取描述子]]></title>
    <url>%2F2019%2F05%2F21%2Fopencv-102%2F</url>
    <content type="text"><![CDATA[知识点对于HOG特征，我们可以通过预先训练的特征数据，进行多尺度的对象检测，OpenCV中基于HOG的行人检测是一个典型案例，同时我们还可以实现自定义对象的检测，这种自定义对象检测，可以分为两个部分，第一部分：通过提取样本的HOG描述子，生成样本的特征数据，第二部分通过SVM进行分类学习与训练，保存为模型。这样我们以后就可以通过模型来实现自定义对象检测啦。今天我们首先分享第一部分，提取HOG描述子。 OpenCV中提取HOG描述子的API表示如下： 12345678910111213141516virtual void cv::HOGDescriptor::compute( InputArray img, std::vector&lt; float &gt; &amp; descriptors, Size winStride = Size(), Size padding = Size(), const std::vector&lt; Point &gt; &amp; locations = std::vector&lt; Point &gt;() )输入图像大小WxH=72x128默认的HOG描述子窗口大小为64x128，窗口移动的步长8x8对于每个窗口内部，每个Cell大小是8x8的，所以窗口可以划分为8x16的Cells大小对于每个Block区域来说，每次移动步长是一个Cell，8x16Cells可以得到总数7x15个Block每个Block都是4个Cell, 36个向量，所以对于输入图像得到：7x15x36x2 = 7560个特征描述子，这些描述子可以作为浮点数特征数据.对于需要输入的样本图像来说，需要首先执行以下预处理，把图像大小resize为跟窗口大小一致或者把窗口resize跟图像大小s一致，这样有利于下一步处理。 代码（c++,python）12345678910111213141516171819202122232425#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace std;int main(int argc, char** argv) &#123; Mat src = imread("D:/images/gaoyy_min.png"); if (src.empty()) &#123; printf("could not load image..\n"); return -1; &#125; imshow("input", src); HOGDescriptor hog; vector&lt;float&gt; features; hog.compute(src, features, Size(8, 8), Size(0, 0)); printf("feature sum size :%d \n", features.size()); for (int i = 0; i &lt; features.size(); i++) &#123; printf("v: %.2f\n ", features[i]); &#125; imshow("result", src); waitKey(0); return 0;&#125; 123456789101112131415161718192021"""HOG特征描述子之提取描述子"""import cv2 as cvsrc = cv.imread("images/test.png")# 对输入图像预处理src = cv.resize(src, (72, 128))print("shape of image: ", src.shape)hog = cv.HOGDescriptor()# 先变成灰度图像再进行描述子计算gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)des = hog.compute(gray, winStride=(8, 8), padding=(0, 0))print("提取的描述子数量：", len(des))print("描述子：")print(des)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>HOG特征描述子之提取描述子</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-101-HOG特征描述子之多尺度检测]]></title>
    <url>%2F2019%2F05%2F17%2Fopencv-101%2F</url>
    <content type="text"><![CDATA[知识点HOG(Histogram of Oriented Gradient)特征本身不支持旋转不变性，通过金字塔可以支持多尺度检测实现尺度空间不变性，OpenCV中支持HOG描述子多尺度检测的相关API如下： 1234567891011121314151617181920virtual void cv::HOGDescriptor::detectMultiScale( InputArray img, std::vector&amp;lt; Rect &amp;gt; &amp; foundLocations, double hitThreshold = 0, Size winStride = Size(), Size padding = Size(), double scale = 1.05, double finalThreshold = 2.0, bool useMeanshiftGrouping = false )Img表示输入图像foundLocations表示发现对象矩形框hitThreshold表示SVM距离度量，默认0表示，表示特征与SVM分类超平面之间winStride表示窗口步长padding表示填充scale表示尺度空间finalThreshold 最终阈值，默认为2.0useMeanshiftGrouping 不建议使用，速度太慢拉这个其中窗口步长与Scale对结果影响最大，特别是Scale，小的尺度变化有利于检出低分辨率对象，同事也会导致FP发生，高的可以避免FP但是会产生FN（有对象漏检）。窗口步长是一个或者多个block区域，关于Block区域可以看下图 代码（c++,python）123456789101112131415161718192021222324#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace std;int main(int argc, char** argv) &#123; Mat src = imread("D:/images/pedestrian_02.png"); if (src.empty()) &#123; printf("could not load image..\n"); return -1; &#125; imshow("input", src); HOGDescriptor *hog = new HOGDescriptor(); hog-&gt;setSVMDetector(hog-&gt;getDefaultPeopleDetector()); vector&lt;Rect&gt; objects; hog-&gt;detectMultiScale(src, objects, 0.0, Size(4, 4), Size(8, 8), 1.05); for (int i = 0; i &lt; objects.size(); i++) &#123; rectangle(src, objects[i], Scalar(0, 0, 255), 2, 8, 0); &#125; imshow("result", src); waitKey(0); return 0;&#125; 123456789101112131415161718192021"""HOG特征描述子之多尺度检测"""import cv2 as cvsrc = cv.imread("images/pedestrian.png")hog = cv.HOGDescriptor()hog.setSVMDetector(cv.HOGDescriptor_getDefaultPeopleDetector())# detect people in image(rects, weights) = hog.detectMultiScale(src, winStride=(4, 4), padding=(8, 8), scale=1.55, useMeanshiftGrouping=False)for (x, y, w, h) in rects: cv.rectangle(src, (x, y), (x + w, y + h), (0, 255, 0), 2)cv.imshow("hog-detector", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>HOG特征描述子之多尺度检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-099-SIFT特征提取之描述子生成]]></title>
    <url>%2F2019%2F05%2F16%2Fopencv-099%2F</url>
    <content type="text"><![CDATA[知识点SIFT特征提取是图像特征提取中最经典的一个算法，归纳起来SIFT特征提取主要有如下几步： 构建高斯多尺度金字塔 关键点查找/过滤与精准定位 窗口区域角度方向直方图 描述子生成 前面我们已经详细解释了SIFT特征点是如何提取的，有了特征点之后，我们对特征点周围的像素块计算角度方向直方图，在计算直方图之前首先需要对图像进行梯度计算，这里可以使用SOBEL算子，然后根据dx与dy计算梯度和与角度。 SIFT特征提取具有空间尺度不变性、迁移不变性、光照不变性，一定要理解SIFT的精髓，如何实现了这几种不变性。 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/xfeatures2d.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace cv::xfeatures2d;using namespace std;void find_known_object(Mat &amp;box, Mat &amp;box_scene);int main(int argc, char** argv) &#123; Mat box = imread("D:/images/box.bmp"); Mat scene = imread("D:/images/scene.jpg"); imshow("box image", box); imshow("scene image", scene); find_known_object(box, scene); //Mat gray; //cvtColor(src, gray, COLOR_BGR2GRAY); auto detector = SIFT::create(); vector&lt;KeyPoint&gt; keypoints_box, keypoints_scene; Mat descriptor_box, descriptor_scene; detector-&gt;detectAndCompute(box, Mat(), keypoints_box, descriptor_box); detector-&gt;detectAndCompute(scene, Mat(), keypoints_scene, descriptor_scene); Ptr&lt;FlannBasedMatcher&gt; matcher = FlannBasedMatcher::create(); vector&lt;DMatch&gt; matches; matcher-&gt;match(descriptor_box, descriptor_scene, matches); Mat dst; drawMatches(box, keypoints_box, scene, keypoints_scene, matches, dst); imshow("match-demo", dst); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526"""SIFT特征提取 – 描述子生成"""import cv2 as cvbox = cv.imread("D:/images/box.png")box_in_sence = cv.imread("D:/images/box_in_scene.png")cv.imshow("box", box)cv.imshow("box_in_sence", box_in_sence)# 创建sift特征检测器sift = cv.xfeatures2d.SIFT_create()kp1, des1 = sift.detectAndCompute(box,None)kp2, des2 = sift.detectAndCompute(box_in_sence,None)# 暴力匹配bf = cv.DescriptorMatcher_create(cv.DescriptorMatcher_BRUTEFORCE)matches = bf.match(des1,des2)# 绘制匹配matches = sorted(matches, key = lambda x:x.distance)result = cv.drawMatches(box, kp1, box_in_sence, kp2, matches[:15], None)cv.imshow("orb-match", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>SIFT特征提取之描述子生成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-098-SIFT特征提取之关键点提取]]></title>
    <url>%2F2019%2F05%2F16%2Fopencv-098%2F</url>
    <content type="text"><![CDATA[知识点SIFT特征提取是图像特征提取中最经典的一个算法，归纳起来SIFT特征提取主要有如下几步： 构建高斯多尺度金字塔 关键点查找/过滤与精准定位 窗口区域角度方向直方图 描述子生成 OpenCV已经实现了SIFT算法，但是在OpenCV3.0之后因为专利授权问题，该算法在扩展模块xfeature2d中，需要自己编译才可以使用，OpenCV Python中从3.4.2之后扩展模块也无法使用，需要自己单独编译python SDK才可以使用。其使用方法与我们前面介绍的ORB完全一致。都是遵循下面的步骤 创建对象 通过detect方法提取对象关键点 同drawKeypoints绘制关键点 构建多尺度高斯金字塔为了在每组图像中检测 S 个尺度的极值点，DoG 金字塔每组需 S+2 层图像，因为每组的第一层和最后一层图像上不能检测极值，DoG 金字塔由高斯金字塔相邻两层相减得到，则高斯金字塔每组最少需 S+3 层图像，实际计算时 S 通常在2到5之间。 代码（python）123456789101112131415161718192021"""SIFT特征提取之关键点提取"""import cv2 as cvsrc = cv.imread("images/test4.jpg")cv.imshow("input", src)# 需要编译才能使用sift = cv.xfeatures2d.SIFT_create()kps = sift.detect(src)# opencv4 python版中好像没有 cv.drawKeypoints()# result = cv.drawKeypoints(src, kps, None, (0, 255, 0), cv.DrawMatchesFlags_DEFAULT)result = src.copy()for marker in kps: result = cv.drawMarker(src, tuple(int(i) for i in marker.pt), color=(0, 255, 0))cv.imshow("result", result)cv.waitKey(0)cv.destroyAllWindows() 结果略 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>SIFT特征提取之关键点提取</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-100-HOG特征与行人检测]]></title>
    <url>%2F2019%2F05%2F16%2Fopencv-100%2F</url>
    <content type="text"><![CDATA[知识点HOG(Histogram of Oriented Gradient)特征在对象识别与模式匹配中是一种常见的特征提取算法，是基于本地像素块进行特征直方图提取的一种算法，对象局部的变形与光照影响有很好的稳定性，最初是用HOG特征来来识别人像，通过HOG特征提取+SVM训练，可以得到很好的效果，OpenCV已经有了。 代码（c++,python）123456789101112131415161718192021222324#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace std;int main(int argc, char** argv) &#123; Mat src = imread("D:/images/pedestrian.png"); if (src.empty()) &#123; printf("could not load image..\n"); return -1; &#125; imshow("input", src); HOGDescriptor *hog = new HOGDescriptor(); hog-&gt;setSVMDetector(hog-&gt;getDefaultPeopleDetector()); vector&lt;Rect&gt; objects; hog-&gt;detectMultiScale(src, objects, 0.0, Size(4, 4), Size(8, 8), 1.25); for (int i = 0; i &lt; objects.size(); i++) &#123; rectangle(src, objects[i], Scalar(0, 0, 255), 2, 8, 0); &#125; imshow("result", src); waitKey(0); return 0;&#125; 123456789101112131415161718192021"""HOG特征与行人检测"""import cv2 as cvsrc = cv.imread("images/pedestrian.png")hog = cv.HOGDescriptor()hog.setSVMDetector(cv.HOGDescriptor_getDefaultPeopleDetector())# detect people in image(rects, weights) = hog.detectMultiScale(src, winStride=(4, 4), padding=(8, 8), scale=1.25, useMeanshiftGrouping=False)for (x, y, w, h) in rects: cv.rectangle(src, (x, y), (x + w, y + h), (0, 255, 0), 2)cv.imshow("hog-detector", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>HOG特征与行人检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-097-基于描述子匹配的已知对象定位]]></title>
    <url>%2F2019%2F05%2F10%2Fopencv-097%2F</url>
    <content type="text"><![CDATA[知识点图像特征点检测、描述子生成以后，就可以通过OpenCV提供的描述子匹配算法，得到描述子直接的距离，距离越小的说明是匹配越好的，设定一个距离阈值，一般是最大匹配距离的1/5～1/4左右作为阈值，得到所有小于阈值的匹配点，作为输入，通过单应性矩阵，获得这两个点所在平面的变换关系H，根据H使用透视变换就可以根据输入的对象图像获得场景图像中对象位置，最终绘制位置即可。 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;#include &lt;math.h&gt;#define RATIO 0.4using namespace cv;using namespace std;int main(int argc, char** argv) &#123; Mat box = imread("D:/images/box.png"); Mat scene = imread("D:/images/box_in_scene.png"); if (scene.empty()) &#123; printf("could not load image...\n"); return -1; &#125; imshow("input image", scene); vector&lt;KeyPoint&gt; keypoints_obj, keypoints_sence; Mat descriptors_box, descriptors_sence; Ptr&lt;ORB&gt; detector = ORB::create(); detector-&gt;detectAndCompute(scene, Mat(), keypoints_sence, descriptors_sence); detector-&gt;detectAndCompute(box, Mat(), keypoints_obj, descriptors_box); vector&lt;DMatch&gt; matches; // 初始化flann匹配 // Ptr&lt;FlannBasedMatcher&gt; matcher = FlannBasedMatcher::create(); // default is bad, using local sensitive hash(LSH) Ptr&lt;DescriptorMatcher&gt; matcher = makePtr&lt;FlannBasedMatcher&gt;(makePtr&lt;flann::LshIndexParams&gt;(12, 20, 2)); matcher-&gt;match(descriptors_box, descriptors_sence, matches); // 发现匹配 vector&lt;DMatch&gt; goodMatches; printf("total match points : %d\n", matches.size()); float maxdist = 0; for (unsigned int i = 0; i &lt; matches.size(); ++i) &#123; printf("dist : %.2f \n", matches[i].distance); maxdist = max(maxdist, matches[i].distance); &#125; for (unsigned int i = 0; i &lt; matches.size(); ++i) &#123; if (matches[i].distance &lt; maxdist*RATIO) goodMatches.push_back(matches[i]); &#125; Mat dst; drawMatches(box, keypoints_obj, scene, keypoints_sence, goodMatches, dst); imshow("output", dst); //-- Localize the object std::vector&lt;Point2f&gt; obj_pts; std::vector&lt;Point2f&gt; scene_pts; for (size_t i = 0; i &lt; goodMatches.size(); i++) &#123; //-- Get the keypoints from the good matches obj_pts.push_back(keypoints_obj[goodMatches[i].queryIdx].pt); scene_pts.push_back(keypoints_sence[goodMatches[i].trainIdx].pt); &#125; Mat H = findHomography(obj_pts, scene_pts, RHO); // 无法配准 // Mat H = findHomography(obj, scene, RANSAC); //-- Get the corners from the image_1 ( the object to be "detected" ) std::vector&lt;Point2f&gt; obj_corners(4); obj_corners[0] = Point(0, 0); obj_corners[1] = Point(box.cols, 0); obj_corners[2] = Point(box.cols, box.rows); obj_corners[3] = Point(0, box.rows); std::vector&lt;Point2f&gt; scene_corners(4); perspectiveTransform(obj_corners, scene_corners, H); //-- Draw lines between the corners (the mapped object in the scene - image_2 ) line(dst, scene_corners[0] + Point2f(box.cols, 0), scene_corners[1] + Point2f(box.cols, 0), Scalar(0, 255, 0), 4); line(dst, scene_corners[1] + Point2f(box.cols, 0), scene_corners[2] + Point2f(box.cols, 0), Scalar(0, 255, 0), 4); line(dst, scene_corners[2] + Point2f(box.cols, 0), scene_corners[3] + Point2f(box.cols, 0), Scalar(0, 255, 0), 4); line(dst, scene_corners[3] + Point2f(box.cols, 0), scene_corners[0] + Point2f(box.cols, 0), Scalar(0, 255, 0), 4); //-- Show detected matches imshow("Good Matches &amp; Object detection", dst); imwrite("D:/result.png", dst); waitKey(0); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758"""基于描述子匹配的已知对象定位"""import cv2 as cvimport numpy as npbox = cv.imread("images/box.png")box_in_scene = cv.imread("images/box_in_scene.png")cv.imshow("box", box)cv.imshow("box_in_scene", box_in_scene)# 创建ORB特征检测器orb = cv.ORB_create()# 得到特征关键点和描述子kp1, des1 = orb.detectAndCompute(box, None)kp2, des2 = orb.detectAndCompute(box_in_scene, None)# 暴力匹配bf = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True)matchers = bf.match(des1, des2)# 发现匹配maxdist = 0goodMatches = []for m in matchers: maxdist = max(maxdist, m.distance)for m in matchers: if m.distance &lt; 0.4 * maxdist: goodMatches.append(m)# 找到本地化对象obj_pts = np.float32([kp1[m.queryIdx].pt for m in goodMatches]).reshape(-1, 1, 2)scene_pts = np.float32([kp2[m.trainIdx].pt for m in goodMatches]).reshape(-1, 1, 2)# findHomography 函数是计算变换矩阵# 参数cv.RANSAC / cv.RHO是使用RANSAC算法寻找一个最佳单应性矩阵H，即返回值M# 返回值：M 为变换矩阵，mask是掩模M, mask = cv.findHomography(obj_pts, scene_pts, cv.RANSAC)# 获取box的图像尺寸h, w, _ = box.shape# obj_corners是图像box的四个顶点obj_corners = np.float32([[0, 0], [w, 0], [w, h], [0, h]]).reshape(-1, 1, 2)# 计算变换后的四个顶点坐标位置,透视变换scene_corners = cv.perspectiveTransform(obj_corners, M)# 根据四个顶点坐标位置在img2图像画出变换后的边框box_in_scene = cv.polylines(box_in_scene, [np.int32(scene_corners)], True, (0, 0, 255), 3, cv.LINE_AA)# 绘制result = cv.drawMatches(box, kp1, box_in_scene, kp2, goodMatches, None)cv.imshow("orb-match", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>基于描述子匹配的已知对象定位</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-096-描述子匹配]]></title>
    <url>%2F2019%2F05%2F10%2Fopencv-096%2F</url>
    <content type="text"><![CDATA[知识点图像特征检测首先会获取关键点，然后根据关键点周围像素ROI区域的大小，生成描述子，完整的描述子向量就表示了一张图像的特征，是图像特征数据，这种方式也被称为图像特征工程，即通过先验模型与合理计算得到图像特征数据的过程，有了特征数据我们就可以利用特征数据实现对象检测与对象识别，这个最简单一个方法就是特征匹配，OPenCV提供了两种图像特征匹配的算法 暴力匹配 FLANN匹配 其中FLANN是一种高效的数值或者字符串匹配算法，SIFT/SURF是基于浮点数的匹配，ORB是二值匹配，速度更快。对于FLANN匹配算法，当使用ORB匹配算法的时候，需要重新构造HASH。这个在C++的代码种做了演示。暴力匹配在Python代码种做了演示。对匹配之后的输出结果，根据距离进行排序，就会得到距离比较的匹配点，这个才是好的特征匹配。 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;#include &lt;math.h&gt;#define RATIO 0.4using namespace cv;using namespace std;int main(int argc, char** argv) &#123; Mat box = imread("D:/images/box.png"); Mat scene = imread("D:/images/box_in_scene.png"); if (scene.empty()) &#123; printf("could not load image...\n"); return -1; &#125; imshow("input image", scene); vector&lt;KeyPoint&gt; keypoints_obj, keypoints_sence; Mat descriptors_box, descriptors_sence; Ptr&lt;ORB&gt; detector = ORB::create(); detector-&gt;detectAndCompute(scene, Mat(), keypoints_sence, descriptors_sence); detector-&gt;detectAndCompute(box, Mat(), keypoints_obj, descriptors_box); vector&lt;DMatch&gt; matches; // ��ʼ��flannƥ�� // Ptr&lt;FlannBasedMatcher&gt; matcher = FlannBasedMatcher::create(); // default is bad, using local sensitive hash(LSH) Ptr&lt;DescriptorMatcher&gt; matcher = makePtr&lt;FlannBasedMatcher&gt;(makePtr&lt;flann::LshIndexParams&gt;(12, 20, 2)); matcher-&gt;match(descriptors_box, descriptors_sence, matches); // ����ƥ�� vector&lt;DMatch&gt; goodMatches; printf("total match points : %d\n", matches.size()); float maxdist = 0; for (unsigned int i = 0; i &lt; matches.size(); ++i) &#123; printf("dist : %.2f \n", matches[i].distance); maxdist = max(maxdist, matches[i].distance); &#125; for (unsigned int i = 0; i &lt; matches.size(); ++i) &#123; if (matches[i].distance &lt; maxdist*RATIO) goodMatches.push_back(matches[i]); &#125; Mat dst; drawMatches(box, keypoints_obj, scene, keypoints_sence, goodMatches, dst); imshow("output", dst); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526272829"""描述子匹配"""import cv2 as cvbox = cv.imread("images/box.png")box_in_scene = cv.imread("images/box_in_scene.png")cv.imshow("box", box)cv.imshow("box_in_scene", box_in_scene)# 创建ORB特征检测器orb = cv.ORB_create()# 得到特征关键点和描述子kp1, des1 = orb.detectAndCompute(box, None)kp2, des2 = orb.detectAndCompute(box_in_scene, None)# 暴力匹配bf = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True)matchers = bf.match(des1, des2)# 绘制匹配matchers = sorted(matchers, key=lambda x: x.distance)result = cv.drawMatches(box, kp1, box_in_scene, kp2, matchers[:15], None)cv.imshow("orb-match", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>描述子匹配</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-095-ORB之BRIEF特征描述子匹配]]></title>
    <url>%2F2019%2F05%2F10%2Fopencv-095%2F</url>
    <content type="text"><![CDATA[知识点得到特征点数据之后，根据BRIEF算法就可以建立描述子。选择候选特征点周围SxS大小的像素块、随机选择n对像素点。其中P(x)是图像模糊处理之后的像素值，原因在于高斯模糊可以抑制噪声影响、提供特征点稳定性，在实际代码实现中通常用均值滤波替代高斯滤波以便利用积分图方式加速计算获得更好的性能表现。常见滤波时候使用3x3~9x9之间的卷积核。滤波之后，根据上述描述子的生成条件，得到描述子。作者论文提到n的取值通常为128、256或者512。得到二进制方式的字符串描述子之后，匹配就可以通过XOR方式矩形，计算汉明距离。ORB特征提取跟纯BRIEF特征提取相比较，BRIEF方式采用随机点方式得最终描述子、而ORB通过FAST得到特征点然后得到描述子。 代码（c++,python）12345678910111213141516171819202122232425262728293031#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace std;int main(int argc, char** argv) &#123; Mat box = imread("D:/images/box.png"); Mat box_in_sence = imread("D:/images/box_in_scene.png"); // 创建ORB auto orb_detector = ORB::create(); vector&lt;KeyPoint&gt; kpts_01, kpts_02; Mat descriptors1, descriptors2; orb_detector-&gt;detectAndCompute(box, Mat(), kpts_01, descriptors1); orb_detector-&gt;detectAndCompute(box_in_sence, Mat(), kpts_02, descriptors2); // 定义描述子匹配 - 暴力匹配 Ptr&lt;DescriptorMatcher&gt; matcher = DescriptorMatcher::create(DescriptorMatcher::BRUTEFORCE); std::vector&lt; DMatch &gt; matches; matcher-&gt;match(descriptors1, descriptors2, matches); // 绘制匹配 Mat img_matches; drawMatches(box, kpts_01, box_in_sence, kpts_02, matches, img_matches); imshow("ORB-Matches", img_matches); imwrite("D:/result.png", img_matches); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425262728"""ORB之BRIEF特征描述子匹配"""import cv2 as cvbox = cv.imread("images/box.png")box_in_scene = cv.imread("images/box_in_scene.png")cv.imshow("box", box)cv.imshow("box_in_scene", box_in_scene)# 创建ORB特征检测器orb = cv.ORB_create()# 得到特征关键点和描述子kp1, des1 = orb.detectAndCompute(box, None)kp2, des2 = orb.detectAndCompute(box_in_scene, None)# 暴力匹配bf = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True)matchers = bf.match(des1, des2)# 绘制匹配result = cv.drawMatches(box, kp1, box_in_scene, kp2, matchers, None)cv.imshow("orb-match", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>BRIEF特征描述子匹配</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-093-对象检测(LBP特征介绍)]]></title>
    <url>%2F2019%2F05%2F09%2Fopencv-093%2F</url>
    <content type="text"><![CDATA[知识点局部二值模式(Local Binary Pattern)主要用来实现2D图像纹理分析。其基本思想是用每个像素跟它周围的像素相比较得到局部图像结构，假设中心像素值大于相邻像素值则则相邻像素点赋值为1，否则赋值为0，最终对每个像素点都会得到一个二进制八位的表示，比如11100111。假设3x3的窗口大小，这样对每个像素点来说组合得到的像素值的空间为[0~2^8]。这种结果称为图像的局部二值模式或者简写为了LBP。 代码（c++,python）12345678910111213141516171819202122232425262728293031#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace std;CascadeClassifier faceDetector;String haar_data_file = "D:/opencv-4.0.0/opencv/build/etc/lbpcascades/lbpcascade_frontalface_improved.xml";int main(int artc, char** argv) &#123; Mat frame, gray; vector&lt;Rect&gt; faces; VideoCapture capture(0); faceDetector.load(haar_data_file); namedWindow("frame", WINDOW_AUTOSIZE); while (true) &#123; capture.read(frame); cvtColor(frame, gray, COLOR_BGR2GRAY); equalizeHist(gray, gray); faceDetector.detectMultiScale(gray, faces, 1.2, 1, 0, Size(30, 30), Size(400, 400)); for (size_t t = 0; t &lt; faces.size(); t++) &#123; rectangle(frame, faces[t], Scalar(0, 0, 255), 2, 8, 0); &#125; char c = waitKey(10); if (c == 27) &#123; break; &#125; imshow("frame", frame); &#125; waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526"""对象检测(LBP特征介绍)"""import cv2 as cvcapture = cv.VideoCapture(0)detector = cv.CascadeClassifier("D:/software/opencv4/build/etc/lbpcascades/lbpcascade_frontalface_improved.xml")while True: ret, image = capture.read() if not ret: break faces = detector.detectMultiScale(image, scaleFactor=1.05, minNeighbors=1, minSize=(30, 30), maxSize=(300, 300)) for x, y, width, height in faces: cv.rectangle(image, (x, y), (x + width, y + height), (0, 0, 255), 2, cv.LINE_8, 0) cv.imshow("faces", image) c = cv.waitKey(50) if c == 27: breakcv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>LBP特征介绍</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-092-对象检测(HAAR特征介绍)]]></title>
    <url>%2F2019%2F05%2F09%2Fopencv-092%2F</url>
    <content type="text"><![CDATA[知识点HAAR小波基函数，因为其满足对称性，对人脸这种生物对称性良好的对象特别适合用来做检测器，常见的Haar特征分为三类： 边缘特征 线性特征 中心特征和对角线特征 不同特征可以进行多种组合，生成更加复杂的级联特征，特征模板内有白色和黑色两种矩形，并定义该模板的特征值为白色矩形像素和减去黑色矩形像素和，Haar特征值反映了图像的对比度与梯度变化。OpenCV中HAAR特征计算是积分图技术，这个我们在前面也分享过啦，所以可以非常快速高效的开窗检测， HAAR级联检测器具备有如下特性： 高类间变异性 低类内变异性 局部强度差 不同尺度 计算效率高 代码（python）123456789101112131415161718192021222324252627282930"""对象检测(HAAR特征介绍)"""import cv2 as cvcapture = cv.VideoCapture(0)face_detector = cv.CascadeClassifier(cv.data.haarcascades + "haarcascade_frontalface_alt.xml")smile_detector = cv.CascadeClassifier(cv.data.haarcascades + "haarcascade_smile.xml")while True: ret, image = capture.read() if ret is True: faces = face_detector.detectMultiScale(image, scaleFactor=1.05, minNeighbors=3, minSize=(30, 30), maxSize=(300, 300)) for x, y, width, height in faces: cv.rectangle(image, (x, y), (x + width, y + height), (0, 0, 255), 2, cv.LINE_8, 0) roi = image[y:y + height, x:x + width] smiles = smile_detector.detectMultiScale(roi, scaleFactor=1.7, minNeighbors=3, minSize=(15, 15), maxSize=(100, 100)) for sx, sy, sw, sh in smiles: cv.rectangle(roi, (sx, sy), (sx + sw, sy + sh), (0, 255, 0), 1) cv.imshow("faces", image) c = cv.waitKey(50) if c == 27: break else: breakcv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>HAAR特征介绍</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-091-对象检测(HAAR级联检测器使用)]]></title>
    <url>%2F2019%2F05%2F09%2Fopencv-091%2F</url>
    <content type="text"><![CDATA[知识点HAAR级联检测器，OpenCV中的HAAR级联检测器支持人脸检测、微笑、眼睛与嘴巴检测等，通过加载这些预先训练的HAAR模型数据可以实现相关的对象检测。 API 1234567891011121314151617void cv::CascadeClassifier::detectMultiScale( InputArray image, std::vector&amp;lt; Rect &amp;gt; &amp; objects, double scaleFactor = 1.1, int minNeighbors = 3, int flags = 0, Size minSize = Size(), Size maxSize = Size() )各个参数解释如下:Image:输入图像Objects 人脸框ScaleFactor 放缩比率minNeighbors 表示最低相邻矩形框flags 标志项OpenCV3.x以后不用啦，minSize 可以检测的最小人脸maxSize 可以检测的最大人脸 代码（c++,python）12345678910111213141516171819202122232425262728293031#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace std;CascadeClassifier faceDetector;String haar_data_file = "D:/opencv-4.0.0/opencv/build/etc/haarcascades/haarcascade_frontalface_alt_tree.xml";int main(int artc, char** argv) &#123; Mat frame, gray; vector&lt;Rect&gt; faces; VideoCapture capture(0); faceDetector.load(haar_data_file); namedWindow("frame", WINDOW_AUTOSIZE); while (true) &#123; capture.read(frame); cvtColor(frame, gray, COLOR_BGR2GRAY); equalizeHist(gray, gray); faceDetector.detectMultiScale(gray, faces, 1.2, 1, 0, Size(30, 30), Size(400, 400)); for (size_t t = 0; t &lt; faces.size(); t++) &#123; rectangle(frame, faces[t], Scalar(0, 0, 255), 2, 8, 0); &#125; char c = waitKey(10); if (c == 27) &#123; break; &#125; imshow("frame", frame); &#125; waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526"""对象检测(HAAR级联检测器使用)"""import cv2 as cvcapture = cv.VideoCapture(0)detector = cv.CascadeClassifier(cv.data.haarcascades + "haarcascade_frontalface_alt.xml")while True: ret, image = capture.read() if not ret: break faces = detector.detectMultiScale(image, scaleFactor=1.05, minNeighbors=1, minSize=(30, 30), maxSize=(200, 200)) for x, y, width, height in faces: cv.rectangle(image, (x, y), (x + width, y + height), (0, 0, 255), 2, cv.LINE_8, 0) cv.imshow("faces", image) c = cv.waitKey(50) if c == 27: breakcv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>HAAR级联检测器使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-094-ORB之FAST特征关键点检测]]></title>
    <url>%2F2019%2F05%2F09%2Fopencv-094%2F</url>
    <content type="text"><![CDATA[知识点ORB - (Oriented Fast and Rotated BRIEF)算法是基于FAST特征检测与BRIEF特征描述子匹配实现，相比BRIEF算法中依靠随机方式获取而值点对，ORB通过FAST方法，FAST方式寻找候选特征点方式是假设灰度图像像素点A周围的像素存在连续大于或者小于A的灰度值，选择任意一个像素点P，假设半径为3，周围16个像素表示。 则像素点P被标记为候选特征点、通常N取值为9、12，上图N=9。为了简化计算，我们可以只计算1、9、5、13四个点，至少其中三个点满足上述不等式条件，即可将P视为候选点。然后通过阈值进行最终的筛选即可得到ORB特征点. API 123456789101112131415161718static Ptr&lt;ORB&gt; cv::ORB::create ( int nfeatures = 500, float scaleFactor = 1.2f, int nlevels = 8, int edgeThreshold = 31, int firstLevel = 0, int WTA_K = 2, ORB::ScoreType scoreType = ORB::HARRIS_SCORE, int patchSize = 31, int fastThreshold = 20 )nfeatures 最终输出最大特征点数目scaleFactor 金字塔上采样比率nlevels 金字塔层数edgeThreshold 边缘阈值firstLevel= 0WTA_K这个是跟BRIEF描述子用的scoreType 对所有的特征点进行排名用的方法 代码（c++,python）123456789101112131415161718#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace std;int main(int argc, char** argv) &#123; Mat src = imread("D:/images/grad.png"); auto orb_detector = ORB::create(1000); vector&lt;KeyPoint&gt; kpts; orb_detector-&gt;detect(src, kpts); Mat result = src.clone(); drawKeypoints(src, kpts, result, Scalar::all(-1), DrawMatchesFlags::DEFAULT); imshow("ORB-detector", result); imwrite("D:/result.png", result); waitKey(0); return 0;&#125; 1234567891011121314151617181920"""ORB之FAST特征关键点检测"""import cv2 as cvsrc = cv.imread("images/test4.jpg")cv.imshow("input", src)orb = cv.ORB().create()kps = orb.detect(src)# opencv4 python版中好像没有 cv.drawKeypoints()# result = cv.drawKeypoints(src, kps, None, (0, 255, 0), cv.DrawMatchesFlags_DEFAULT)result = src.copy()for marker in kps: result = cv.drawMarker(src, tuple(int(i) for i in marker.pt), color=(0, 255, 0))cv.imshow("result", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>FAST特征关键点检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-089-视频分析(基于连续自适应均值迁移（CAM）的对象移动分析)]]></title>
    <url>%2F2019%2F05%2F08%2Fopencv-089%2F</url>
    <content type="text"><![CDATA[知识点CAM是连续自适应的均值迁移跟踪算法，它跟均值迁移相比较有两个改进 会根据跟踪对象大小变化自动调整搜索窗口大小 返回位置信息更加完整，包含了位置与角度信息 API 12345678RotatedRect cv::CamShift( InputArray probImage, Rect &amp; window, TermCriteria criteria )probImage输入图像，是直方图反向投影的结果window 搜索窗口，ROI对象区域criteria 均值迁移停止条件 特别需要注意的是：C++版本中会自动更新搜索窗口，Python语言版本中必须每次从返回信息中手动更新。 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495#include &lt;opencv2/opencv.hpp&gt;"#include &lt;iostream&gt;#include &lt;ctype.h&gt;using namespace cv;using namespace std;Mat image;bool selectObject = false;int trackObject = 0;bool showHist = true;Point origin;Rect selection;int vmin = 10, vmax = 256, smin = 30;int main(int argc, const char** argv)&#123; // VideoCapture cap(0); VideoCapture cap("D:/images/video/balltest.mp4"); Rect trackWindow; int hsize = 16; float hranges[] = &#123; 0,180 &#125;; const float* phranges = hranges; if (!cap.isOpened()) &#123; printf("could not open camera...\n"); return -1; &#125; namedWindow("Histogram", WINDOW_AUTOSIZE); namedWindow("CamShift Demo", WINDOW_AUTOSIZE); Mat frame, hsv, hue, mask, hist, histimg = Mat::zeros(200, 320, CV_8UC3), backproj; bool paused = false; cap.read(frame); Rect selection = selectROI("CamShift Demo", frame, true, false); while (true) &#123; bool ret = cap.read(frame); if (!ret) break; frame.copyTo(image); cvtColor(image, hsv, COLOR_BGR2HSV); int _vmin = vmin, _vmax = vmax; inRange(hsv, Scalar(26, 43, 46), Scalar(34, 255, 255), mask); int ch[] = &#123; 0, 0 &#125;; hue.create(hsv.size(), hsv.depth()); mixChannels(&amp;hsv, 1, &amp;hue, 1, ch, 1); if (trackObject &lt;= 0) &#123; // Object has been selected by user, set up CAMShift search properties once Mat roi(hue, selection), maskroi(mask, selection); calcHist(&amp;roi, 1, 0, maskroi, hist, 1, &amp;hsize, &amp;phranges); normalize(hist, hist, 0, 255, NORM_MINMAX); trackWindow = selection; trackObject = 1; // Don't set up again, unless user selects new ROI histimg = Scalar::all(0); int binW = histimg.cols / hsize; Mat buf(1, hsize, CV_8UC3); for (int i = 0; i &lt; hsize; i++) buf.at&lt;Vec3b&gt;(i) = Vec3b(saturate_cast&lt;uchar&gt;(i*180. / hsize), 255, 255); cvtColor(buf, buf, COLOR_HSV2BGR); for (int i = 0; i &lt; hsize; i++) &#123; int val = saturate_cast&lt;int&gt;(hist.at&lt;float&gt;(i)*histimg.rows / 255); rectangle(histimg, Point(i*binW, histimg.rows), Point((i + 1)*binW, histimg.rows - val), Scalar(buf.at&lt;Vec3b&gt;(i)), -1, 8); &#125; &#125; // Perform CA-MeanShift calcBackProject(&amp;hue, 1, 0, hist, backproj, &amp;phranges); backproj &amp;= mask; RotatedRect trackBox = CamShift(backproj, trackWindow, TermCriteria(TermCriteria::EPS | TermCriteria::COUNT, 10, 1)); ellipse(image, trackBox, Scalar(0, 0, 255), 3, LINE_AA); imshow("CamShift Demo", image); imshow("Histogram", histimg); char c = (char)waitKey(50); if (c == 27) break; &#125; return 0;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950"""视频分析(基于连续自适应均值迁移（CAM）的对象移动分析)"""import cv2 as cvcap = cv.VideoCapture('images/balltest.mp4')if not cap.isOpened(): print("could not read video") exit(0)# 读取第一帧ret, frame = cap.read()# 选择ROI区域x, y, w, h = cv.selectROI("CAM Demo", frame, True, False)track_window = (x, y, w, h)# 获取ROI直方图roi = frame[y:y + h, x:x + w]hsv_roi = cv.cvtColor(roi, cv.COLOR_BGR2HSV)mask = cv.inRange(hsv_roi, (26, 43, 46), (34, 255, 255))roi_hist = cv.calcHist([hsv_roi], [0], mask, [180], [0, 180])cv.normalize(roi_hist, roi_hist, 0, 255, cv.NORM_MINMAX)# 搜索跟踪分析term_crit = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1)while True: ret, frame = cap.read() if not ret: break hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV) dst = cv.calcBackProject([hsv], [0], roi_hist, [0, 180], 1) # 搜索更新roi区域 track_box = cv.CamShift(dst, track_window, term_crit) track_window = track_box[1] # print(track_box) # 绘制窗口 cv.ellipse(frame, track_box[0], (0, 0, 255), 3, 8) cv.imshow("CAM Demo", frame) k = cv.waitKey(50) &amp; 0xff if k == 27: breakcv.destroyAllWindows()cap.release() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>基于连续自适应均值迁移（CAM）的对象移动分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-088-视频分析(基于均值迁移的对象移动分析)]]></title>
    <url>%2F2019%2F05%2F08%2Fopencv-088%2F</url>
    <content type="text"><![CDATA[知识点均值迁移移动对象分析，主要是基于直方图分布与反向投影实现移动对象的轨迹跟踪，其核心的思想是对反向投影之后的图像做均值迁移（meanshift）从而发现密度最高的区域，也是对象分布最大的域。完整的算法流程如下： 读取图像一帧 HSV直方图 反向投影该帧 使用means shift寻找最大分布密度 更新窗口直至最后一帧 API 12345678int cv::meanShift( InputArray probImage, Rect &amp; window, TermCriteria criteria )probImage输入图像，是直方图反向投影的结果window 搜索窗口，ROI对象区域criteria 均值迁移停止条件 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;#include &lt;ctype.h&gt;using namespace cv;using namespace std;Mat image;int trackObject = 0;/* * 视频分析(基于均值迁移的对象移动分析) */int main() &#123; VideoCapture cap("../images/balltest.mp4"); Rect trackWindow; int hsize = 16; float hranges[] = &#123;0, 180&#125;; const float *phranges = hranges; if (!cap.isOpened()) &#123; printf("could not open camera...\n"); return -1; &#125; Mat frame, hsv, hue, mask, hist = Mat::zeros(200, 320, CV_8UC3), backproj; cap.read(frame); Rect selection = selectROI("CamShift Demo", frame, true, false); while (true) &#123; bool ret = cap.read(frame); if (!ret) break; frame.copyTo(image); cvtColor(image, hsv, COLOR_BGR2HSV); inRange(hsv, Scalar(26, 43, 46), Scalar(34, 255, 255), mask); int ch[] = &#123;0, 0&#125;; hue.create(hsv.size(), hsv.depth()); mixChannels(&amp;hsv, 1, &amp;hue, 1, ch, 1); if (trackObject &lt;= 0) &#123; // Object has been selected by user, set up CAMShift search properties once Mat roi(hue, selection), maskroi(mask, selection); calcHist(&amp;roi, 1, 0, maskroi, hist, 1, &amp;hsize, &amp;phranges); normalize(hist, hist, 0, 255, NORM_MINMAX); trackWindow = selection; trackObject = 1; // Don't set up again, unless user selects new ROI &#125; // Perform meanShift calcBackProject(&amp;hue, 1, 0, hist, backproj, &amp;phranges); backproj &amp;= mask; meanShift(backproj, trackWindow, TermCriteria(TermCriteria::EPS | TermCriteria::COUNT, 10, 1)); rectangle(image, trackWindow, Scalar(0, 0, 255), 3, LINE_AA); imshow("CamShift Demo", image); char c = (char) waitKey(50); if (c == 27) break; &#125; return 0;&#125; 12345678910111213141516171819202122232425262728293031323334353637import cv2 as cvcap = cv.VideoCapture('D:/images/video/balltest.mp4')# 读取第一帧ret,frame = cap.read()cv.namedWindow("CAS Demo", cv.WINDOW_AUTOSIZE)x, y, w, h = cv.selectROI("CAS Demo", frame, True, False)track_window = (x, y, w, h)# 获取ROI直方图roi = frame[y:y+h, x:x+w]hsv_roi = cv.cvtColor(roi, cv.COLOR_BGR2HSV)mask = cv.inRange(hsv_roi, (26, 43, 46), (34, 255, 255))roi_hist = cv.calcHist([hsv_roi],[0],mask,[180],[0,180])cv.normalize(roi_hist,roi_hist,0,255,cv.NORM_MINMAX)# 设置搜索跟踪分析term_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )while True: ret, frame = cap.read() if ret is False: break; hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV) dst = cv.calcBackProject([hsv],[0],roi_hist,[0,180],1) # 均值迁移，搜索更新roi区域 ret, track_window = cv.meanShift(dst, track_window, term_crit) # 绘制窗口 x,y,w,h = track_window cv.rectangle(frame, (x,y), (x+w,y+h), 255,2) cv.imshow('CAS Demo',frame) k = cv.waitKey(60) &amp; 0xff if k == 27: breakcv.destroyAllWindows()cap.release() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>基于均值迁移的对象移动分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-087-视频分析(基于帧差法实现移动对象分析)]]></title>
    <url>%2F2019%2F05%2F08%2Fopencv-087%2F</url>
    <content type="text"><![CDATA[知识点光流跟踪与背景消除都是基于建模方式的视频分析方法，其实这类方法最原始的一个例子就是对视频移动对象的帧差法跟踪，这个在视频分析与处理中也是一种很常见的手段，有时候会取得意想不到的好效果，帧差法进一步划分有可以分为： 两帧差 三帧差 假设有当前帧frame， 前一帧prev1，更前一帧prev2两帧差方法直接使用前一帧 减去当前帧 diff = frame – prev1三帧差方法计算如下：diff1 = prev2 – prev1diff2 = frame – prev1diff = diff1 &amp; diff2帧差法在求取帧差之前一般会进行高斯模糊，用以减低干扰，通过得到的diff图像进行形态学操作，用以合并与候选区域，提升效率。帧差法的缺点有如下： 高斯模糊是高耗时计算 容易受到噪声与光线干扰 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 视频分析(基于帧差法实现移动对象分析) */int main() &#123; VideoCapture capture("../images/bike.avi"); if (!capture.isOpened()) &#123; cout &lt;&lt; "could not open video..." &lt;&lt; endl; return -1; &#125; // 读取第一帧 Mat preFrame, preGray; capture.read(preFrame); cvtColor(preFrame, preGray, COLOR_BGR2GRAY); GaussianBlur(preGray, preGray, Size(0, 0), 15); Mat diff; Mat frame, gray; // 定义结构元素 Mat k = getStructuringElement(MORPH_RECT, Size(7, 7)); while (true) &#123; bool ret = capture.read(frame); if (!ret) break; cvtColor(frame, gray, COLOR_BGR2GRAY); GaussianBlur(gray, gray, Size(0, 0), 15); subtract(gray, preGray, diff); threshold(diff, diff, 0, 255, THRESH_BINARY | THRESH_OTSU); morphologyEx(diff, diff, MORPH_OPEN, k); imshow("input", frame); imshow("result", diff); gray.copyTo(preGray); char c = waitKey(5); if (c == 27) &#123; break; &#125; &#125; waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526import numpy as npimport cv2 as cvcap = cv.VideoCapture("D:/images/video/bike.avi")ret, prevFrame = cap.read()prevGray = cv.cvtColor(prevFrame, cv.COLOR_BGR2GRAY)prevGray = cv.GaussianBlur(prevGray, (0, 0), 15)k = cv.getStructuringElement(cv.MORPH_RECT, (7, 7))while True: ret, frame = cap.read() if ret is False: break gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY) gray = cv.GaussianBlur(gray, (0, 0), 15) diff = cv.subtract(gray, prevGray) t, binary = cv.threshold(diff, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU) binary = cv.morphologyEx(binary, cv.MORPH_OPEN, k) cv.imshow('input', frame) cv.imshow('result', binary) cv.imwrite("D:/result.png", binary) c = cv.waitKey(50)&amp;0xff prevGray = np.copy(gray) if c == 27: breakcap.release()cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>基于帧差法实现移动对象分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-090-视频分析(对象移动轨迹绘制)]]></title>
    <url>%2F2019%2F05%2F08%2Fopencv-090%2F</url>
    <content type="text"><![CDATA[知识点移动对象分析，我们可以绘制对象运行轨迹曲线，这个主要是根据移动对象窗口轮廓，获取中心位置，然后使用中心位置进行绘制即可得到。大致的程序步骤如下： 初始化路径点数组 对每帧的预测轮廓提取中心位置添加到路径数组 绘制路径曲线 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798#include &lt;opencv2/opencv.hpp&gt;"#include &lt;iostream&gt;using namespace cv;using namespace std;Mat image;bool selectObject = false;int trackObject = 0;bool showHist = true;Point origin;Rect selection;int vmin = 10, vmax = 256, smin = 30;int main(int argc, const char** argv)&#123; // VideoCapture cap(0); VideoCapture cap("D:/images/video/balltest.mp4"); Rect trackWindow; int hsize = 16; float hranges[] = &#123; 0,180 &#125;; const float* phranges = hranges; if (!cap.isOpened()) &#123; printf("could not open camera...\n"); return -1; &#125; namedWindow("Histogram", WINDOW_AUTOSIZE); namedWindow("CamShift Demo", WINDOW_AUTOSIZE); Mat frame, hsv, hue, mask, hist, histimg = Mat::zeros(200, 320, CV_8UC3), backproj; bool paused = false; cap.read(frame); Rect selection = selectROI("CamShift Demo", frame, true, false); vector&lt;Point&gt; tracking_path; while (true) &#123; bool ret = cap.read(frame); if (!ret) break; frame.copyTo(image); cvtColor(image, hsv, COLOR_BGR2HSV); int _vmin = vmin, _vmax = vmax; inRange(hsv, Scalar(26, 43, 46), Scalar(34, 255, 255), mask); int ch[] = &#123; 0, 0 &#125;; hue.create(hsv.size(), hsv.depth()); mixChannels(&amp;hsv, 1, &amp;hue, 1, ch, 1); if (trackObject &lt;= 0) &#123; // Object has been selected by user, set up CAMShift search properties once Mat roi(hue, selection), maskroi(mask, selection); calcHist(&amp;roi, 1, 0, maskroi, hist, 1, &amp;hsize, &amp;phranges); normalize(hist, hist, 0, 255, NORM_MINMAX); trackWindow = selection; trackObject = 1; // Don't set up again, unless user selects new ROI histimg = Scalar::all(0); int binW = histimg.cols / hsize; Mat buf(1, hsize, CV_8UC3); for (int i = 0; i &lt; hsize; i++) buf.at&lt;Vec3b&gt;(i) = Vec3b(saturate_cast&lt;uchar&gt;(i*180. / hsize), 255, 255); cvtColor(buf, buf, COLOR_HSV2BGR); for (int i = 0; i &lt; hsize; i++) &#123; int val = saturate_cast&lt;int&gt;(hist.at&lt;float&gt;(i)*histimg.rows / 255); rectangle(histimg, Point(i*binW, histimg.rows), Point((i + 1)*binW, histimg.rows - val), Scalar(buf.at&lt;Vec3b&gt;(i)), -1, 8); &#125; &#125; // Perform CA-MeanShift calcBackProject(&amp;hue, 1, 0, hist, backproj, &amp;phranges); backproj &amp;= mask; RotatedRect trackBox = CamShift(backproj, trackWindow, TermCriteria(TermCriteria::EPS | TermCriteria::COUNT, 10, 1)); if (trackBox.center.x&gt;0 &amp;&amp; trackBox.center.y&gt;0) tracking_path.push_back(trackBox.center); ellipse(image, trackBox, Scalar(0, 0, 255), 3, LINE_AA); for (int i = 1; i &lt; tracking_path.size(); i++) &#123; line(image, tracking_path[i - 1], tracking_path[i], Scalar(255, 0, 0), 2, 8, 0); &#125; imshow("CamShift Demo", image); imshow("Histogram", histimg); char c = (char)waitKey(50); if (c == 27) break; &#125; return 0;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364"""视频分析(对象移动轨迹绘制)"""import cv2 as cvimport numpy as npcap = cv.VideoCapture('images/balltest.mp4')if not cap.isOpened(): print("could not read video") exit(0)# 读取第一帧ret, frame = cap.read()# 选择ROI区域x, y, w, h = cv.selectROI("CAM Demo", frame, True, False)track_window = (x, y, w, h)# 获取ROI直方图roi = frame[y:y + h, x:x + w]hsv_roi = cv.cvtColor(roi, cv.COLOR_BGR2HSV)mask = cv.inRange(hsv_roi, (26, 43, 46), (34, 255, 255))roi_hist = cv.calcHist([hsv_roi], [0], mask, [180], [0, 180])cv.normalize(roi_hist, roi_hist, 0, 255, cv.NORM_MINMAX)# 搜索跟踪分析tracking_path = []term_crit = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1)while True: ret, frame = cap.read() if not ret: break hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV) dst = cv.calcBackProject([hsv], [0], roi_hist, [0, 180], 1) # 搜索更新roi区域,保持运行轨迹 track_box = cv.CamShift(dst, track_window, term_crit) track_window = track_box[1] pt = np.int32(track_box[0][0]) if pt[0] &gt; 0 and pt[1] &gt; 0: tracking_path.append(pt) # 绘制窗口 cv.ellipse(frame, track_box[0], (0, 0, 255), 3, 8) # 绘制运动轨迹 if len(tracking_path) &gt; 40: tracking_path = tracking_path[-40:-1] for i in range(1, len(tracking_path)): cv.line(frame, (tracking_path[i-1][0], tracking_path[i-1][1]), (tracking_path[i][0], tracking_path[i][1]), (255, 0, 0), 2, 8, 0) cv.imshow("CAM Demo", frame) k = cv.waitKey(50) &amp; 0xff if k == 27: breakcv.destroyAllWindows()cap.release() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>对象移动轨迹绘制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[keras使用迁移学习实现二分类问题]]></title>
    <url>%2F2019%2F05%2F07%2Fkeras_binary_classification%2F</url>
    <content type="text"><![CDATA[问题描述要解决的是一个医学图像的二分类问题，有AK和SK两种病症，根据一定量数据，进行训练，对图像进行预测。 给定图片数据的格式： 解决思路整体上采用迁移学习来训练神经网络，使用InceptionV3结构，框架采用keras. 具体思路： 读取图片数据，保存成.npy格式，方便后续加载 标签采用one-hot形式，由于标签隐藏在文件夹命名中，所以需要自行添加标签，并保存到.npy文件中，方便后续加载 将数据分为训练集、验证集、测试集 使用keras建立InceptionV3基本模型，不包括顶层，使用预训练权重，在基本模型的基础上自定义几层神经网络，得到最后的模型，对模型进行训练 优化模型，调整超参数，提高准确率 在测试集上对模型进行评估，使用精确率、召回率 对单张图片进行预测，并输出每种类别的概率 代码结构 具体代码1. path.py ==&gt; 定义项目根路径 12345678910import osimport inspectdef mkdir_if_not_exist(dir_list): for directory in dir_list: if not os.path.exists(directory): os.makedirs(directory)curr_filename = inspect.getfile(inspect.currentframe())root_dir = os.path.dirname(os.path.abspath(curr_filename)) 2. load_datasets.py ==&gt; 读取原始数据，生成.npy文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import osimport matplotlib.pyplot as pltimport numpy as npfrom skimage import iofrom skimage import transformfrom tqdm import tqdmfrom paths import root_dir, mkdir_if_not_existfrom sklearn.utils import shuffleorigin_data_dir = os.path.join(root_dir, 'origin_data')data_dir = os.path.join(root_dir, 'data')mkdir_if_not_exist(dir_list=[data_dir])def process_datasets(): images = [] labels = [] for AK_or_SK_dir in tqdm(os.listdir(origin_data_dir)): # AK ==&gt; [1, 0] SK ==&gt; [0, 1] if 'AK' in AK_or_SK_dir: label = [1, 0] elif 'SK' in AK_or_SK_dir: label = [0, 1] else: print("AK_or_SK_dir error") for person_dir in tqdm(os.listdir(os.path.join(origin_data_dir, AK_or_SK_dir))): for fname in os.listdir(os.path.join(origin_data_dir, AK_or_SK_dir, person_dir)): img_fname = os.path.join(origin_data_dir, AK_or_SK_dir, person_dir, fname) image = io.imread(img_fname) image = transform.resize(image, (224, 224), order=1, mode='constant', cval=0, clip=True, preserve_range=True, anti_aliasing=True) image = image.astype(np.uint8) images.append(image) labels.append(label) images = np.stack(images).astype(np.uint8) labels = np.stack(labels, axis=0) return images, labelsdef load_datasets(): images_npy_filename = os.path.join(data_dir, 'images_data.npy') labels_npy_filename = os.path.join(data_dir, 'labels.npy') if os.path.exists(images_npy_filename) and os.path.exists(labels_npy_filename): images = np.load(images_npy_filename) labels = np.load(labels_npy_filename) else: images, labels = process_datasets() # 将数据打乱后保存 images, labels = shuffle(images, labels) np.save(images_npy_filename, images) np.save(labels_npy_filename, labels) return images, labelsif __name__ == '__main__': X, y = load_datasets() print(X.shape,y.shape) # plt.imshow(X[5]) # plt.show() y = np.argmax(y, axis=1) print(y[:20]) count_SK = np.count_nonzero(y) print("SK图片数量：", count_SK) 3. load_train_test_data.py ==&gt; 划分训练集、验证集、测试集 123456789101112131415161718192021222324252627282930from sklearn.model_selection import StratifiedKFoldfrom sklearn.model_selection import train_test_splitimport numpy as npfrom load_datesets import load_datasetsX, y = load_datasets()X_test = X[650:]y_test = y[650:]X = X[0:650]y = y[0:650]def load_test_data(): return X_test, y_testdef load_train_data(test_split=None, use_cross_validation=None, k_fold=None): if use_cross_validation: data = [] sfolder = StratifiedKFold(n_splits=k_fold, random_state=1) y_label = np.argmax(y, axis=1) for train_index, valid_index in sfolder.split(X, y_label): X_train, X_valid, y_train, y_valid = X[train_index], X[valid_index], y[train_index], y[valid_index] data_tmp = (X_train, X_valid, y_train, y_valid) data.append(data_tmp) return data else: X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_split, random_state=1) return X_train, X_valid, y_train, y_valid 4. train.py ==&gt; 建立网络模型，进行训练 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150import osfrom keras import regularizersfrom keras.applications.inception_v3 import InceptionV3from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, TensorBoard, EarlyStoppingfrom keras.layers import Densefrom keras.layers import GlobalAveragePooling2D, Dropoutfrom keras.losses import categorical_crossentropyfrom keras.metrics import categorical_accuracyfrom keras.models import Modelfrom keras.optimizers import Adamfrom keras.preprocessing.image import ImageDataGeneratorimport tensorflow as tffrom keras.backend.tensorflow_backend import set_sessionfrom load_train_test_data import load_train_datafrom paths import root_dir# 超参数test_split = 0.2 # 验证机划分比例num_classes = 2 lr = 1e-4epochs = 30dropout_rate = 0.5kernel_regularizer = regularizers.l1(1e-4) # 正则化batch_size = 64use_data_aug = True # 是否使用数据增强use_cross_validation = False # 是否使用交叉验证k_fold = 5 # k折交叉验证的kdef build_model(): base_model = InceptionV3(weights='imagenet', include_top=False) img_input = base_model.output outputs = GlobalAveragePooling2D(name='avg_pool_my')(img_input) if dropout_rate &gt; 0.: outputs = Dropout(rate=dropout_rate)(outputs) outputs = Dense(256, activation='elu', name='fc1', kernel_regularizer=kernel_regularizer)(outputs) outputs = Dropout(rate=dropout_rate)(outputs) outputs = Dense(128, activation='elu', name='fc2', kernel_regularizer=kernel_regularizer)(outputs) outputs = Dropout(rate=dropout_rate)(outputs) outputs = Dense(num_classes, activation='softmax', name='predictions', kernel_regularizer=kernel_regularizer)(outputs) model = Model(inputs=base_model.input, outputs=outputs) model.summary() model.compile(optimizer=Adam(lr=lr), loss=categorical_crossentropy, metrics=[categorical_accuracy, ]) return modeldef train_model(model, X_train, y_train, X_valid, y_valid): # 模型保存路径 model_path = os.path.join(root_dir, 'model_data', 'model_no_cross.h5') # 定义回调函数 callbacks = [ # 当标准评估停止提升时，降低学习速率 ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=2, verbose=1, mode='auto', min_lr=1e-7), # 在每个训练期之后保存模型，最后保存的是最佳模型 ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True, verbose=True), # tensorboard 可视化 TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=False, write_grads=True, write_images=True, update_freq='epoch') ] if use_data_aug: datagen = ImageDataGenerator(rotation_range=180, horizontal_flip=True, vertical_flip=True, width_shift_range=0.1, height_shift_range=0.1, #featurewise_center=True, # 均值为0 #featurewise_std_normalization=True # 标准化 ) model.fit_generator(generator=datagen.flow(X_train, y_train, batch_size=batch_size), steps_per_epoch=X_train.shape[0] // batch_size * 2, epochs=epochs, initial_epoch=0, verbose=1, validation_data=(X_valid, y_valid), callbacks=callbacks) else: model.fit(x=X_train, y=y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_valid, y_valid), callbacks=callbacks)def set_gpu(): # 指定使用的GPU os.environ["CUDA_VISIBLE_DEVICES"] = "9" ## keras 默认占满gpu所有内存，所以要手动设定内存使用情况 config = tf.ConfigProto() ''' # keras 设置gpu内存使用比例 config.gpu_options.per_process_gpu_memory_fraction = 0.5 ''' # keras 设置gpu内存按需分配 config.gpu_options.allow_growth = True set_session(tf.Session(config=config))if __name__ == '__main__': # 指定GPU set_gpu() # 构建模型 model = build_model() if use_cross_validation: data = load_train_data(use_cross_validation=use_cross_validation, k_fold=k_fold) for i in range(k_fold): # 加载数据 X_train, X_valid, y_train, y_valid = data[i] # 训练模型 train_model(model, X_train, y_train, X_valid, y_valid) else: # 加载数据 X_train, X_valid, y_train, y_valid = load_train_data(test_split=test_split) # 训练模型 train_model(model, X_train, y_train, X_valid, y_valid) 5. eval.py ==&gt; 在测试集上对模型进行评估 123456789101112131415161718192021222324252627282930313233343536import osfrom keras.models import load_modelfrom paths import root_dirfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_scoreimport numpy as npfrom load_train_test_data import load_test_data# 指定使用的GPUos.environ["CUDA_VISIBLE_DEVICES"] = "9"if __name__ == '__main__': # 加载模型 model_path = os.path.join(root_dir, 'model_data', 'model_no_cross.h5') model = load_model(model_path) # 评估数据 X_test, y_test = load_test_data() # y预测 y_pred = model.predict(X_test) y_pred = np.argmax(y_pred, axis=1) y_test = np.argmax(y_test, axis=1) print(y_test) print(y_pred) # 准确率，精确率，召回率，F1 accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) print("accuracy_score = %.2f" % accuracy) print("precision_score = %.2f" % precision) print("recall_score = %.2f" % recall) print("f1_score = %.2f" % f1) 6. predict.py ==&gt; 对单张图片进行预测 12345678910111213141516171819202122232425262728293031323334353637import osimport cv2 as cvimport numpy as npfrom keras.models import load_modelfrom keras.preprocessing import imagefrom paths import root_dir# 指定使用的GPUos.environ["CUDA_VISIBLE_DEVICES"] = "9"clsss_name = &#123;0: "AK", 1: "SK"&#125;if __name__ == '__main__': # 加载模型 model_path = os.path.join(root_dir, 'model_data', 'model_no_cross.h5') model = load_model(model_path) for AK_or_SK_dir in os.listdir(os.path.join(root_dir, "images")): for fname in os.listdir(os.path.join(root_dir, "images", AK_or_SK_dir)): # 读取图片 img_path = os.path.join(root_dir, "images", AK_or_SK_dir, fname) img = image.load_img(img_path, target_size=(224, 224)) img = image.img_to_array(img) # 扩充维度 img = np.expand_dims(img, axis=0) # 预测 pred = model.predict(img) # 打印图片类别 y_pred = np.argmax(pred, axis=1) img_name = clsss_name[y_pred[0]] print(fname, "的预测概率是：") print(pred, " ==&gt; ", img_name) 运行结果1. 训练结果 2. 评估结果 3. 预测结果 知识点总结 如何加载实际数据，如何保存成npy文件，如何打乱数据，如何划分数据，如何进行交叉验证 如何使用keras进行迁移学习 keras中数据增强、回调函数的使用，回调函数涉及：学习速率调整、保存最好模型、tensorboard可视化 如何使用sklearn计算准确率，精确率，召回率，F1_score 如何对单张图片进行预测，并打印分类概率 如何指定特定GPU训练，如何指定使用GPU的内存情况 github地址github]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>keras</tag>
        <tag>二分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-085-视频分析(移动对象的KLT光流跟踪算法_删除静止点与绘制跟踪轨迹)]]></title>
    <url>%2F2019%2F05%2F06%2Fopencv-085%2F</url>
    <content type="text"><![CDATA[知识点在84的知识点分享中我们已经可以跟踪到前后两帧之前的位置移动，但是这个还不足够，我们需要绘制移动对象从初始到最终的完整可以检测的运动轨迹，同时对一些静止的角点进行删除，所以我们需要对状态为1的角点，计算它们之间的距离，只有dx+dy&gt;2（dx=abs(p1.x –p2.x), dy=abs(p1.y-p2.y)）的我们才对它进行保留跟踪。 流程 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;vector&lt;Point2f&gt; featurePoints;vector&lt;Scalar&gt; color_lut;RNG rng(12345);void draw_lines(Mat &amp;image, vector&lt;Point2f&gt; pt1, vector&lt;Point2f&gt; pt2);/* * 视频分析(移动对象的KLT光流跟踪算法_删除静止点与绘制跟踪轨迹) */int main() &#123; VideoCapture capture("../images/vtest.avi"); if (!capture.isOpened()) &#123; cout &lt;&lt; "could not open video..." &lt;&lt; endl; return -1; &#125; // 角点检测参数 double qualityLevel = 0.01; int minDistance = 10; int maxCorners = 100; // KLT光流跟踪参数 vector&lt;Point2f&gt; pts[2]; vector&lt;uchar&gt; status; vector&lt;float&gt; err; vector&lt;Point2f&gt; initPoints; // 读取第一帧及其角点 Mat old_frame, old_gray; capture.read(old_frame); cvtColor(old_frame, old_gray, COLOR_BGR2GRAY); goodFeaturesToTrack(old_gray, featurePoints, maxCorners, qualityLevel, minDistance, Mat()); pts[0].insert(pts[0].end(), featurePoints.begin(), featurePoints.end()); initPoints.insert(initPoints.end(), featurePoints.begin(), featurePoints.end()); int width = capture.get(CAP_PROP_FRAME_WIDTH); int height = capture.get(CAP_PROP_FRAME_HEIGHT); VideoWriter writer("D:/test.mp4", VideoWriter::fourcc('D', 'I', 'V', 'X'), 10, Size(width * 2, height), true); Mat result = Mat::zeros(Size(width * 2, height), CV_8UC3); Rect roi(0, 0, width, height); Mat gray, frame; while (true) &#123; bool ret = capture.read(frame); if (!ret) break; imshow("frame", frame); roi.x = 0; frame.copyTo(result(roi)); cvtColor(frame, gray, COLOR_BGR2GRAY); // 计算光流 calcOpticalFlowPyrLK(old_gray, gray, pts[0], pts[1], status, err, Size(31, 31)); size_t i, k; for (int i = k = 0; i &lt; pts[1].size(); ++i) &#123; // 距离与状态测量,删除静止点 double dist = abs(pts[0][i].x - pts[1][i].x) + abs(pts[0][i].y - pts[1][i].y); if (status[i] &amp;&amp; dist &gt; 2) &#123; pts[0][k] = pts[0][i]; initPoints[k] = initPoints[i]; pts[1][k++] = pts[1][i]; circle(frame, pts[1][i], 4, Scalar(0, 255, 0), -1); &#125; &#125; // resize 有用特征点 pts[0].resize(k); pts[1].resize(k); initPoints.resize(k); // 绘制跟踪轨迹 draw_lines(frame, initPoints, pts[1]); imshow("result", frame); roi.x = width; frame.copyTo(result(roi)); char c = waitKey(50); if (c == 27) break; // 更新old std::swap(pts[1], pts[0]); cv::swap(old_gray, gray); // 重新初始化角点 if (initPoints.size() &lt; 40)&#123; goodFeaturesToTrack(old_gray, featurePoints, maxCorners, qualityLevel, minDistance, Mat()); pts[0].insert(pts[0].end(), featurePoints.begin(), featurePoints.end()); initPoints.insert(initPoints.end(), featurePoints.begin(), featurePoints.end()); &#125; writer.write(result); &#125; return 0;&#125;void draw_lines(Mat &amp;image, vector&lt;Point2f&gt; pt1, vector&lt;Point2f&gt; pt2) &#123; if (color_lut.size() &lt; pt1.size())&#123; for (size_t i = 0; i &lt; pt1.size(); ++i) &#123; int b = rng.uniform(0, 255); int g = rng.uniform(0, 255); int r = rng.uniform(0, 255); Scalar color(b, g, r); color_lut.push_back(color); &#125; &#125; for (size_t j = 0; j &lt; pt1.size(); ++j) &#123; line(image, pt1[j], pt2[j], color_lut[j], 2); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import numpy as npimport cv2 as cvcap = cv.VideoCapture('D:\\code-workspace\\Clion-workspace\\learnOpencv\\images\\vtest.avi')# 角点检测参数feature_params = dict(maxCorners=100, qualityLevel=0.01, minDistance=10, blockSize=3)# KLT光流参数lk_params = dict(winSize=(31, 31), maxLevel=3, criteria=(cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 30, 0.01))# 随机颜色color = np.random.randint(0,255,(100,3))# 读取第一帧ret, old_frame = cap.read()old_gray = cv.cvtColor(old_frame, cv.COLOR_BGR2GRAY)p0 = cv.goodFeaturesToTrack(old_gray, mask=None, **feature_params)initPoints = p0.copy()# 光流跟踪while True: ret, frame = cap.read() if ret is False: break cv.imshow('frame',frame) gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY) # 计算光流 p1, st, err = cv.calcOpticalFlowPyrLK(old_gray, gray, p0, None, **lk_params) # 距离与状态测量，删除静止点 k=0 for i, (new, old) in enumerate(zip(p1,p0)): a,b = new.ravel() c,d = old.ravel() dist = abs(a-c) + abs(b-d) if st[i] == 1 and dist &gt; 2: p0[k] = p0[i] initPoints[k] = initPoints[i] p1[k] = p1[i] k = k+1 frame = cv.circle(frame,(a,b),5,color[i].tolist(),-1) # 取有用特征点 p0 = p0[:k] p1 = p1[:k] initPoints = initPoints[:k] # 绘制跟踪线 for i, (old, new) in enumerate(zip(initPoints,p1)): a,b = old.ravel() c,d = new.ravel() frame = cv.line(frame, (a,b),(c,d), (0,255,0), 2) cv.imshow('result',frame) k = cv.waitKey(30) &amp; 0xff if k == 27: break # 更新old old_gray = gray.copy() p0, _ = p1, p0 # 重新初始化角点 if len(initPoints) &lt; 40: p0 = cv.goodFeaturesToTrack(old_gray, mask=None, **feature_params) initPoints = p0.copy()cv.destroyAllWindows()cap.release() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>KLT光流跟踪算法_删除静止点与绘制跟踪轨迹</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-086-视频分析(稠密光流分析)]]></title>
    <url>%2F2019%2F05%2F06%2Fopencv-086%2F</url>
    <content type="text"><![CDATA[知识点光流跟踪方法分为稠密光流跟踪与稀疏光流跟踪算法，KLT是稀疏光流跟踪算法，前面我们已经介绍过了，OpenCV还支持稠密光流的移动对象跟踪方法，OpenCV中支持的稠密光流算法是由Gunner Farneback在2003年提出来的，它是基于前后两帧所有像素点的移动估算算法，其效果要比稀疏光流算法更好。 API 1234567891011121314151617181920212223void cv::calcOpticalFlowFarneback( InputArray prev, InputArray next, InputOutputArray flow, double pyr_scale, int levels, int winsize, int iterations, int poly_n, double poly_sigma, int flags )prev 前一帧next 后一帧flow 光流，计算得到的移动能量场pyr_scale 金字塔放缩比率levels 金字塔层级数目winsize 表示窗口大小iterations 表示迭代次数poly_n 表示光流生成时候，对邻域像素的多项式展开，n越大越模糊越稳定poly_sigma 表示光流多项式展开时候用的高斯系数，n越大，sigma应该适当增加flags有两个OPTFLOW_USE_INITIAL_FLOW表示使用盒子模糊进行初始化光流OPTFLOW_FARNEBACK_GAUSSIAN表示使用高斯窗口 代码（c++,python）1234567891011121314151617181920212223void cv::calcOpticalFlowFarneback( InputArray prev, InputArray next, InputOutputArray flow, double pyr_scale, int levels, int winsize, int iterations, int poly_n, double poly_sigma, int flags )prev 前一帧next 后一帧flow 光流，计算得到的移动能量场pyr_scale 金字塔放缩比率levels 金字塔层级数目winsize 表示窗口大小iterations 表示迭代次数poly_n 表示光流生成时候，对邻域像素的多项式展开，n越大越模糊越稳定poly_sigma 表示光流多项式展开时候用的高斯系数，n越大，sigma应该适当增加flags有两个OPTFLOW_USE_INITIAL_FLOW表示使用盒子模糊进行初始化光流OPTFLOW_FARNEBACK_GAUSSIAN表示使用高斯窗口 12345678910111213141516171819202122232425import cv2 as cvimport numpy as npcap = cv.VideoCapture("D:/images/video/vtest.avi")ret, frame1 = cap.read()prvs = cv.cvtColor(frame1,cv.COLOR_BGR2GRAY)hsv = np.zeros_like(frame1)hsv[...,1] = 255while(1): ret, frame2 = cap.read() next = cv.cvtColor(frame2,cv.COLOR_BGR2GRAY) flow = cv.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0) mag, ang = cv.cartToPolar(flow[...,0], flow[...,1]) hsv[...,0] = ang*180/np.pi/2 hsv[...,2] = cv.normalize(mag,None,0,255,cv.NORM_MINMAX) bgr = cv.cvtColor(hsv,cv.COLOR_HSV2BGR) cv.imshow('frame2',bgr) k = cv.waitKey(30) &amp; 0xff if k == 27: break elif k == ord('s'): cv.imwrite('opticalfb.png',frame2) cv.imwrite('opticalhsv.png',bgr) prvs = nextcap.release()cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>稠密光流分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查看或监控GPU使用情况]]></title>
    <url>%2F2019%2F05%2F05%2Flinux_monitor_gpu%2F</url>
    <content type="text"><![CDATA[显示当前GPU使用情况1$ nvidia-smi 周期性输出GPU使用情况1$ watch -n 10 nvidia-smi # 每 10s 显示一次显存的情况]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux下监控GPU使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pycharm配置远程服务器并解决cudnn运行时与编译时版本不匹配的问题]]></title>
    <url>%2F2019%2F05%2F05%2Fpycharm_remote_configure%2F</url>
    <content type="text"><![CDATA[配置步骤1 本地新建一工程，并打开本地工程路径：C:\Users\17600\Desktop\binary_classification 2 填写服务器信息并进行本地与远程的映射 3 在pycharm中展示服务器文件 4 本地与服务器文件的互传 5 配置远程解释器 如果使用的是pyenv创建的虚拟环境，则路径是： /home/用户名/.pyenv/versions/python版本(3.6.7)/envs/虚拟环境名称(env3.6)/bin/python&quot; 此时运行文件使用的就是远程的解释器 6 使用远程gpu时可能遇到的问题解决问题： Loaded runtime CuDNN library: 7.3.0 but source was compiled with: 7.4.2. 即cudnn运行时版本和编译时版本不一致 解决： 服务器端，运行vim ~/.bashrc打开.bashrc文件，添加以下代码： 12345export CUDA_DEVICE_ORDER=PCI_BUS_IDexport LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64"export CUDA_HOME=/usr/local/cuda pycharm中配置同样信息：]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>pycharm配置远程服务器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-082-角点检测(shi-tomas角点检测)]]></title>
    <url>%2F2019%2F04%2F26%2Fopencv-082%2F</url>
    <content type="text"><![CDATA[知识点Harris角点检测是一种计算速度很慢的角点检测算法，很难实时计算，所有最常用的是shi-tomas角点检测算法，它的运行速度很快。 API 1234567891011121314151617181920void cv::goodFeaturesToTrack( InputArray image, OutputArray corners, int maxCorners, double qualityLevel, double minDistance, InputArray mask = noArray(), int blockSize = 3, bool useHarrisDetector = false, double k = 0.04 )src单通道输入图像，八位或者浮点数corners是输出的关键点坐标集合maxCorners表示最大返回关键点数目qualityLevel表示拒绝的关键点 R &lt; qualityLevel × max response将会被直接丢弃minDistance 表示两个关键点之间的最短距离mask 表示mask区域，如果有表明只对mask区域做计算blockSize 计算梯度与微分的窗口区域useHarrisDetector 表示是否使用harris角点检测，默认是false 为shi-tomask = 0.04默认值，当useHarrisDetector为ture时候起作用 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;void process_frame(Mat &amp;image);RNG rng(12345);/* * 角点检测(shi-tomas角点检测) */int main() &#123; VideoCapture capture("../images/color_object.mp4"); if (!capture.isOpened()) &#123; cout &lt;&lt; "could not open video..." &lt;&lt; endl; return -1; &#125; Mat frame; while (true) &#123; bool ret = capture.read(frame); imshow("input", frame); if (!ret) break; process_frame(frame); imshow("result", frame); char c = waitKey(5); if (c == 27) &#123; break; &#125; &#125; waitKey(0); return 0;&#125;void process_frame(Mat &amp;image) &#123; // Detector parameters int maxCorners = 100; double quality_level = 0.01; double minDistance = 0.04; // detecting corners Mat gray, dst; cvtColor(image, gray, COLOR_BGR2GRAY); vector&lt;Point2f&gt; corners; goodFeaturesToTrack(gray, corners, maxCorners, quality_level, minDistance, Mat(), 3, false); // drawing corner for (int i = 0; i &lt; corners.size(); ++i) &#123; int b = rng.uniform(0, 255); int g = rng.uniform(0, 255); int r = rng.uniform(0, 255); circle(image, corners[i], 5, Scalar(b, g, r), 3); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041import numpy as npimport cv2 as cvdef process(image, opt=1): # Detecting corners gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY) corners = cv.goodFeaturesToTrack(gray, 100, 0.05, 10) print(len(corners)) for pt in corners: print(pt) b = np.random.random_integers(0, 256) g = np.random.random_integers(0, 256) r = np.random.random_integers(0, 256) x = np.int32(pt[0][0]) y = np.int32(pt[0][1]) cv.circle(image, (x, y), 5, (int(b), int(g), int(r)), 2) # output return imagesrc = cv.imread("D:/images/ele_panel.png")cv.imshow("input", src)result = process(src)cv.imshow('result', result)cv.waitKey(0)cv.destroyAllWindows()"""cap = cv.VideoCapture(0)while True: ret, frame = cap.read() cv.imwrite("D:/input.png", frame) cv.imshow('input', frame) result = process(frame) cv.imshow('result', result) k = cv.waitKey(5)&amp;0xff if k == 27: breakcap.release()cv.destroyAllWindows()""" 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>shi-tomas角点检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-083-角点检测(亚像素级别角点检测)]]></title>
    <url>%2F2019%2F04%2F26%2Fopencv-083%2F</url>
    <content type="text"><![CDATA[知识点OpenCV中角点检测的结果实际不够精准，角点检测最后的结果是整数值，因为真实的计算中有些位置可能是在浮点数的空间内才最大值，这样就需要我们通过给定的响应值，在像素邻域空间进行拟合，实现亚像素级别的角点检测。如：(100,5)实际上应该是(100.126,4.329) . API 123456789101112void cv::cornerSubPix( InputArray image, InputOutputArray corners, Size winSize, Size zeroZone, TermCriteria criteria )image单通道输入图像，八位或者浮点数corners是输入输出的关键点坐标集合winSize表示插值计算时候窗口大小zeroZone表示搜索区域中间的dead region边长的一半，有时用于避免自相关矩阵的奇异性。如果值设为(-1,-1)则表示没有这个区域。criteria角点精准化迭代过程的终止条件 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;void process_frame(Mat &amp;image);RNG rng(12345);/* * 角点检测(亚像素级别角点检测) */int main() &#123; VideoCapture capture("../images/color_object.mp4"); if (!capture.isOpened()) &#123; cout &lt;&lt; "could not open video..." &lt;&lt; endl; return -1; &#125; Mat frame; while (true) &#123; bool ret = capture.read(frame); imshow("input", frame); if (!ret) break; process_frame(frame); imshow("result", frame); char c = waitKey(5); if (c == 27) &#123; break; &#125; &#125; waitKey(0); return 0;&#125;void process_frame(Mat &amp;image) &#123; // Detector parameters int maxCorners = 100; double quality_level = 0.01; double minDistance = 0.04; // detecting corners Mat gray, dst; cvtColor(image, gray, COLOR_BGR2GRAY); vector&lt;Point2f&gt; corners; goodFeaturesToTrack(gray, corners, maxCorners, quality_level, minDistance, Mat(), 3, false); // detect sub-pixel 亚像素检测 Size winSize = Size(5,5); Size zeroZone = Size(-1,-1); TermCriteria criteria = TermCriteria(TermCriteria::EPS + TermCriteria::COUNT, 40, 0.001); cornerSubPix(gray, corners, winSize, zeroZone, criteria); // drawing corner for (int i = 0; i &lt; corners.size(); ++i) &#123; int b = rng.uniform(0, 255); int g = rng.uniform(0, 255); int r = rng.uniform(0, 255); circle(image, corners[i], 5, Scalar(b, g, r), 3); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243import numpy as npimport cv2 as cvdef process(image, opt=1): # Detecting corners gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY) corners = cv.goodFeaturesToTrack(gray, 100, 0.05, 10) print(len(corners)) for pt in corners: print(pt) b = np.random.random_integers(0, 256) g = np.random.random_integers(0, 256) r = np.random.random_integers(0, 256) x = np.int32(pt[0][0]) y = np.int32(pt[0][1]) cv.circle(image, (x, y), 5, (int(b), int(g), int(r)), 2) # detect sub-pixel winSize = (5, 5) zeroZone = (-1, -1) criteria = (cv.TERM_CRITERIA_EPS + cv.TermCriteria_COUNT, 40, 0.001) # Calculate the refined corner locations corners = cv.cornerSubPix(gray, corners, winSize, zeroZone, criteria) # display for i in range(corners.shape[0]): print(" -- Refined Corner [", i, "] (", corners[i, 0, 0], ",", corners[i, 0, 1], ")") return imagecap = cv.VideoCapture("D:/images/video/vtest.avi")while True: ret, frame = cap.read() frame = cv.flip(frame, 1) cv.imwrite("D:/input.png", frame) cv.imshow('input', frame) result = process(frame) cv.imshow('result', result) k = cv.waitKey(5)&amp;0xff if k == 27: breakcap.release()cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>亚像素级别角点检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-084-视频分析(移动对象的KLT光流跟踪算法)]]></title>
    <url>%2F2019%2F04%2F26%2Fopencv-084%2F</url>
    <content type="text"><![CDATA[知识点光流跟踪方法分为稠密光流跟踪与稀疏光流跟踪算法，KLT是稀疏光流跟踪算法，这个算法最早是由Bruce D. Lucas and Takeo Kanade两位作者提出来的，所以又被称为KLT。稀疏光流算法工作有三个假设前提条件： 亮度恒定 短距离移动 空间一致性 API 12345678910111213void cv::calcOpticalFlowPyrLK( InputArray prevImg, // 前一帧图像 InputArray nextImg, // 后一帧图像 InputArray prevPts, // 前一帧的稀疏光流点 InputOutputArray nextPts, // 后一帧光流点 OutputArray status, // 输出状态，1 表示正常该点保留，否则丢弃 OutputArray err, // 表示错误 Size winSize = Size(21, 21), // 光流法对象窗口大小 int maxLevel = 3, // 金字塔层数，0表示只检测当前图像，不构建金字塔图像 TermCriteria criteria = TermCriteria(TermCriteria::COUNT+TermCriteria::EPS, 30, 0.01), // 窗口搜索时候停止条件 int flags = 0, // 操作标志 double minEigThreshold = 1e-4 // 最小特征值响应，低于最小值不做处理) 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;vector&lt;Point2f&gt; featurePoints;RNG rng(12345);/* * 视频分析(移动对象的KLT光流跟踪算法) */int main() &#123; VideoCapture capture("../images/vtest.avi"); if (!capture.isOpened()) &#123; cout &lt;&lt; "could not open video..." &lt;&lt; endl; return -1; &#125; // 角点检测参数 double qualityLevel = 0.01; int minDistance = 10; int maxCorners = 100; // KLT光流跟踪参数 vector&lt;Point2f&gt; pts[2]; vector&lt;uchar&gt; status; vector&lt;float&gt; err; // 读取第一帧及其角点 Mat old_frame, old_gray; capture.read(old_frame); cvtColor(old_frame, old_gray, COLOR_BGR2GRAY); goodFeaturesToTrack(old_gray, featurePoints, maxCorners, qualityLevel, minDistance, Mat()); pts[0].insert(pts[0].end(), featurePoints.begin(), featurePoints.end()); int width = capture.get(CAP_PROP_FRAME_WIDTH); int height = capture.get(CAP_PROP_FRAME_HEIGHT); Rect roi(0, 0, width, height); Mat gray, frame; while (true) &#123; bool ret = capture.read(frame); if (!ret) break; imshow("frame", frame); roi.x = 0; cvtColor(frame, gray, COLOR_BGR2GRAY); // 计算光流 calcOpticalFlowPyrLK(old_gray, gray, pts[0], pts[1], status, err, Size(31, 31)); size_t i, k; for (int i = k = 0; i &lt; pts[1].size(); ++i) &#123; // 根据状态选择 if (status[i])&#123; pts[0][k] = pts[0][i]; pts[1][k++] = pts[1][i]; int b = rng.uniform(0, 256); int g = rng.uniform(0, 256); int r = rng.uniform(0, 256); Scalar color(b, g, r); // 绘制跟踪线 circle(frame, pts[1][i], 4, color, -1); line(frame, pts[0][i], pts[1][i], color, 2); &#125; &#125; // resize 有用特征点 pts[0].resize(k); pts[1].resize(k); imshow("result", frame); roi.x = width; char c = waitKey(50); if (c == 27) break; // 更新old std::swap(pts[1], pts[0]); cv::swap(old_gray, gray); &#125; return 0;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import numpy as npimport cv2 as cvcap = cv.VideoCapture('D:/images/video/vtest.avi')# 角点检测参数feature_params = dict(maxCorners=100, qualityLevel=0.01, minDistance=10, blockSize=3)# KLT光流参数lk_params = dict(winSize=(31, 31), maxLevel=3, criteria=(cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 30, 0.01))# 随机颜色color = np.random.randint(0,255,(100,3))# 读取第一帧ret, old_frame = cap.read()old_gray = cv.cvtColor(old_frame, cv.COLOR_BGR2GRAY)p0 = cv.goodFeaturesToTrack(old_gray, mask=None, **feature_params)# 光流跟踪while True: ret, frame = cap.read() if ret is False: break frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY) # 计算光流 p1, st, err = cv.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params) # 根据状态选择 good_new = p1[st == 1] good_old = p0[st == 1] # 绘制跟踪线 for i, (new, old) in enumerate(zip(good_new,good_old)): a,b = new.ravel() c,d = old.ravel() frame = cv.line(frame, (a,b),(c,d), color[i].tolist(), 2) frame = cv.circle(frame,(a,b),5,color[i].tolist(),-1) cv.imshow('frame',frame) k = cv.waitKey(30) &amp; 0xff if k == 27: break # 更新 old_gray = frame_gray.copy() p0 = good_new.reshape(-1, 1, 2)cv.destroyAllWindows()cap.release() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>KLT光流跟踪算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-081-角点检测(Harris角点检测)]]></title>
    <url>%2F2019%2F04%2F26%2Fopencv-081%2F</url>
    <content type="text"><![CDATA[知识点角点是一幅图像上最明显与重要的特征，对于一阶导数而言，角点在各个方向的变化是最大的，而边缘区域在只是某一方向有明显变化。 API 12345678910111213void cv::cornerHarris( InputArray src, OutputArray dst, int blockSize, int ksize, double k, int borderType = BORDER_DEFAULT)src单通道输入图像dst是输出responseblockSize计算协方差矩阵的时候邻域像素大小ksize表示soble算子的大小k表示系数 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;void process_frame(Mat &amp;image);RNG rng(12345);/* * 角点检测(Harris角点检测) */int main() &#123; Mat src = imread("../images/box.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); process_frame(src); imshow("result", src); waitKey(0); return 0;&#125;void process_frame(Mat &amp;image) &#123; // detector paraments int blockSize = 2; int kSize = 3; double k = 0.04; // detecting corners Mat gray, dst; cvtColor(image, gray, COLOR_BGR2GRAY); cornerHarris(gray, dst, blockSize, kSize, k); // normalizing Mat dst_norm = Mat::zeros(dst.size(), dst.type()); normalize(dst, dst_norm, 0, 255, NORM_MINMAX); convertScaleAbs(dst_norm, dst_norm); // drawing a circle around corners for (int row = 0; row &lt; dst_norm.rows; ++row) &#123; for (int col = 0; col &lt; dst_norm.cols; ++col) &#123; int rsp = dst_norm.at&lt;uchar&gt;(row, col); if (rsp &gt; 150) &#123; int b = rng.uniform(0, 256); int g = rng.uniform(0, 256); int r = rng.uniform(0, 256); circle(image, Point(row, col), 5, Scalar(b, g, r), 2); &#125; &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import numpy as npimport cv2 as cvdef process(image, opt=1): # Detector parameters blockSize = 2 apertureSize = 3 k = 0.04 # Detecting corners gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY) dst = cv.cornerHarris(gray, blockSize, apertureSize, k) # Normalizing dst_norm = np.empty(dst.shape, dtype=np.float32) cv.normalize(dst, dst_norm, alpha=0, beta=255, norm_type=cv.NORM_MINMAX) dst_norm_scaled = cv.convertScaleAbs(dst_norm) # Drawing a circle around corners for i in range(dst_norm.shape[0]): for j in range(dst_norm.shape[1]): if int(dst_norm[i, j]) &gt; 80: b = np.random.random_integers(0, 256) g = np.random.random_integers(0, 256) r = np.random.random_integers(0, 256) cv.circle(image, (j, i), 5, (int(b), int(g), int(r)), 2) # output return imagesrc = cv.imread("D:/images/ele_panel.png")cv.imshow("input", src)result = process(src)cv.imshow('result', result)cv.waitKey(0)cv.destroyAllWindows()"""cap = cv.VideoCapture(0)while True: ret, frame = cap.read() cv.imwrite("D:/input.png", frame) cv.imshow('input', frame) result = process(frame) cv.imshow('result', result) k = cv.waitKey(5)&amp;0xff if k == 27: breakcap.release()cv.destroyAllWindows()""" 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>Harris角点检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-078-识别与跟踪视频中的特定颜色对象]]></title>
    <url>%2F2019%2F04%2F24%2Fopencv-078%2F</url>
    <content type="text"><![CDATA[知识点图像处理与二值分析的视频版本，通过读取视频每一帧的图像，然后对图像二值分析，得到指定的色块区域，主要步骤如下： 色彩转换BGR2HSV inRange提取颜色区域mask 对mask区域进行二值分析得到位置与轮廓信息 绘制外接椭圆与中心位置 显示结果 其中涉及到的知识点主要包括图像处理、色彩空间转换、形态学、轮廓分析等。 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;void process_frame(Mat &amp;image);/* * 识别与跟踪视频中的特定颜色对象 */int main() &#123; VideoCapture capture("../images/color_object.mp4"); if (!capture.isOpened()) &#123; printf("could not open camera...\n"); return -1; &#125; int fps = capture.get(CAP_PROP_FPS); int width = capture.get(CAP_PROP_FRAME_WIDTH); int height = capture.get(CAP_PROP_FRAME_HEIGHT); int num_of_frames = capture.get(CAP_PROP_FRAME_COUNT); printf("frame width: %d, frame height: %d, FPS : %d \n", width, height, fps); Mat frame; while (true) &#123; bool ret = capture.read(frame); if (!ret) break; imshow("input", frame); char c = waitKey(50); process_frame(frame); imshow("result", frame); if (c == 27) &#123; break; &#125; &#125; waitKey(0); return 0; waitKey(0); return 0;&#125;void process_frame(Mat &amp;image) &#123; Mat hsv, mask; // 转换色彩空间 cvtColor(image, hsv, COLOR_BGR2HSV); // 提取颜色区域mask inRange(hsv, Scalar(0, 43, 46), Scalar(10, 255, 255), mask); Mat se = getStructuringElement(MORPH_RECT, Size(15, 15)); morphologyEx(mask, mask, MORPH_OPEN, se); // 寻找最大轮廓 vector&lt;vector&lt;Point&gt;&gt; contours; vector&lt;Vec4i&gt; hierarchy; findContours(mask, contours, hierarchy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE); int index = -1; int max = 0; for (size_t t = 0; t &lt; contours.size(); t++) &#123; double area = contourArea(contours[t]); if (area &gt; max) &#123; max = area; index = t; &#125; &#125; // 绘制外接轮廓 if (index &gt;= 0) &#123; RotatedRect rect = minAreaRect(contours[index]); ellipse(image, rect, Scalar(0, 255, 0), 2); circle(image, rect.center, 2, Scalar(255, 0, 0), 2); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import cv2 as cvimport numpy as npcapture = cv.VideoCapture("D:/images/video/test.mp4")height = capture.get(cv.CAP_PROP_FRAME_HEIGHT)width = capture.get(cv.CAP_PROP_FRAME_WIDTH)count = capture.get(cv.CAP_PROP_FRAME_COUNT)fps = capture.get(cv.CAP_PROP_FPS)print(height, width, count, fps)def process(image, opt=1): hsv = cv.cvtColor(image, cv.COLOR_BGR2HSV) line = cv.getStructuringElement(cv.MORPH_RECT, (15, 15), (-1, -1)) mask = cv.inRange(hsv, (0, 43, 46), (10, 255, 255)) mask = cv.morphologyEx(mask, cv.MORPH_OPEN, line) # 轮廓提取, 发现最大轮廓 out, contours, hierarchy = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE) index = -1 max = 0 for c in range(len(contours)): area = cv.contourArea(contours[c]) if area &gt; max: max = area index = c # 绘制 if index &gt;= 0: rect = cv.minAreaRect(contours[index]) cv.ellipse(image, rect, (0, 255, 0), 2, 8) cv.circle(image, (np.int32(rect[0][0]), np.int32(rect[0][1])), 2, (255, 0, 0), 2, 8, 0) return imagewhile(True): ret, frame = capture.read() if ret is True: cv.imshow("video-input", frame) result = process(frame) cv.imshow("result", result) c = cv.waitKey(50) print(c) if c == 27: #ESC break else: breakcv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>识别与跟踪视频中的特定颜色对象</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-079-视频分析(背景和前景的提取)]]></title>
    <url>%2F2019%2F04%2F24%2Fopencv-079%2F</url>
    <content type="text"><![CDATA[知识点视频场景分析中最常用的技术之一就是通过背景消除来提取前景移动对象，得到前景的对象mask图像，最常用的背景消除技术就是通过帧差相减，用前面一帧作为背景图像，与当前帧进行相减，不过这种方法对光照与噪声影响非常敏感，所有好的办法是通过对前面一系列帧提取背景模型进行相减，OpenCV中实现的背景模型提取算法有两种，一种是基于高斯混合模型GMM实现的背景提取，另外一种是基于最近邻KNN实现的。 API 1234567891011121314Ptr&lt;BackgroundSubtractorMOG2&gt; cv::createBackgroundSubtractorMOG2( int history = 500, double varThreshold = 16, bool detectShadows = true )参数解释如下：history表示过往帧数，500帧，选择history = 1就变成两帧差varThreshold表示像素与模型之间的马氏距离，值越大，只有那些最新的像素会被归到前景，值越小前景对光照越敏感。detectShadows 是否保留阴影检测，请选择False这样速度快点创建Ptr&lt;BackgroundSubtractor&gt; pBackSub = createBackgroundSubtractorMOG2();Ptr&lt;BackgroundSubtractor&gt; pBackSub = createBackgroundSubtractorKNN(); 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 视频分析(背景和前景的提取) */int main() &#123; VideoCapture capture("../images/color_object.mp4"); if (!capture.isOpened()) &#123; printf("could not open camera...\n"); return -1; &#125; namedWindow("input", WINDOW_NORMAL); namedWindow("mask", WINDOW_NORMAL); namedWindow("background image", WINDOW_NORMAL); Mat frame, mask, back_img; Ptr&lt;BackgroundSubtractor&gt; pMOG2 = createBackgroundSubtractorMOG2(500, 1000, false); while (true) &#123; bool ret = capture.read(frame); if (!ret) break; pMOG2-&gt;apply(frame, mask); pMOG2-&gt;getBackgroundImage(back_img); imshow("input", frame); imshow("mask", mask); imshow("background image", back_img); char c = waitKey(50); if (c == 27) &#123; break; &#125; &#125; waitKey(0); return 0; waitKey(0); return 0;&#125; 1234567891011121314151617import numpy as npimport cv2 as cvcap = cv.VideoCapture('D:/images/video/color_object.mp4')fgbg = cv.createBackgroundSubtractorMOG2(history=500, varThreshold=1000, detectShadows=False)while True: ret, frame = cap.read() fgmask = fgbg.apply(frame) background = fgbg.getBackgroundImage() cv.imshow('input', frame) cv.imshow('mask',fgmask) cv.imshow('background', background) k = cv.waitKey(10)&amp;0xff if k == 27: breakcap.release()cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>视频分析(背景和前景的提取)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-080-视频分析(背景消除与前景ROI提取)]]></title>
    <url>%2F2019%2F04%2F24%2Fopencv-080%2F</url>
    <content type="text"><![CDATA[知识点通过视频中的背景进行建模，实现背景消除，生成mask图像，通过对mask二值图像分析实现对前景活动对象ROI区域的提取，是很多视频监控分析软件常用的手段之一，该方法很实时！整个步骤如下： 初始化背景建模对象GMM 读取视频一帧 使用背景建模消除生成mask 对mask进行轮廓分析提取ROI 绘制ROI对象 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;void process_frame(Mat &amp;image);Ptr&lt;BackgroundSubtractor&gt; pMOG2 = createBackgroundSubtractorMOG2(500, 100, false);/* * 视频分析(背景消除与前景ROI提取) */int main() &#123; VideoCapture capture("../images/vtest.avi"); if (!capture.isOpened()) &#123; printf("could not open camera...\n"); return -1; &#125; Mat frame; while (true) &#123; bool ret = capture.read(frame); if (!ret) break; imshow("input", frame); process_frame(frame); imshow("result", frame); char c = waitKey(50); if (c == 27) &#123; break; &#125; &#125; waitKey(0); return 0; waitKey(0); return 0;&#125;void process_frame(Mat &amp;image) &#123; Mat mask; pMOG2-&gt;apply(image, mask); // 开操作 Mat se = getStructuringElement(MORPH_RECT, Size(1, 5)); morphologyEx(mask, mask, MORPH_OPEN, se); // 寻找最大轮廓 vector&lt;vector&lt;Point&gt;&gt; contours; vector&lt;Vec4i&gt; hierarchy; findContours(mask, contours, hierarchy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE); for (size_t t = 0; t &lt; contours.size(); t++) &#123; double area = contourArea(contours[t]); if (area &lt; 100) &#123; continue; &#125; RotatedRect rect = minAreaRect(contours[t]); ellipse(image, rect, Scalar(0, 255, 0), 2); circle(image, rect.center, 2, Scalar(255, 0, 0), 2); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839import numpy as npimport cv2 as cvcap = cv.VideoCapture('D:/images/video/vtest.avi')fgbg = cv.createBackgroundSubtractorMOG2( history=500, varThreshold=100, detectShadows=False)def process(image, opt=1): mask = fgbg.apply(frame) line = cv.getStructuringElement(cv.MORPH_RECT, (1, 5), (-1, -1)) mask = cv.morphologyEx(mask, cv.MORPH_OPEN, line) cv.imshow("mask", mask) # 轮廓提取, 发现最大轮廓 out, contours, hierarchy = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE) for c in range(len(contours)): area = cv.contourArea(contours[c]) if area &lt; 100: continue rect = cv.minAreaRect(contours[c]) cv.ellipse(image, rect, (0, 255, 0), 2, 8) cv.circle(image, (np.int32(rect[0][0]), np.int32(rect[0][1])), 2, (255, 0, 0), 2, 8, 0) return image, maskwhile True: ret, frame = cap.read() cv.imwrite("D:/input.png", frame) cv.imshow('input', frame) result, m_ = process(frame) cv.imshow('result', result) k = cv.waitKey(50)&amp;0xff if k == 27: cv.imwrite("D:/result.png", result) cv.imwrite("D:/mask.png", m_) breakcap.release()cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>视频分析(背景消除与前景ROI提取)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-077-视频读取与处理]]></title>
    <url>%2F2019%2F04%2F24%2Fopencv-077%2F</url>
    <content type="text"><![CDATA[知识点OpenCV中对视频内容的处理本质上对读取视频的关键帧进行解析图像，然后对图像进行各种处理，OpenCV的VideoCapture是一个视频读取与解码的API接口，支持各种视频格式、网络视频流、摄像头读取。正常的视频处理与分析，主要是针对读取到每一帧图像，衡量一个算法处理是否能够满足实时要求的时候通常通过FPS（每秒多少帧的处理能力）。一般情况下每秒大于5帧基本上可以认为是在进行视频处理。 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;void process_frame(Mat &amp;image, int opts);/* * 视频读取与处理 */int main() &#123; VideoCapture capture("../images/roadcars.avi"); if (!capture.isOpened())&#123; cout &lt;&lt; "could not open video.." &lt;&lt; endl; return -1; &#125; namedWindow("input"); int fps = capture.get(CAP_PROP_FPS); int width = capture.get(CAP_PROP_FRAME_WIDTH); int height = capture.get(CAP_PROP_FRAME_HEIGHT); int num_of_frames = capture.get(CAP_PROP_FRAME_COUNT); printf("frame width: %d, frame height: %d, FPS : %d \n", width, height, fps); Mat frame; int index = 0; while(capture.isOpened())&#123; bool ret = capture.read(frame); if (!ret) break; imshow("input", frame); char c = waitKey(50); if (c &gt;= 49)&#123; index = c - 49; &#125; process_frame(frame, index); imshow("result", frame); if (c == 27)&#123; break; &#125; &#125; waitKey(0); return 0;&#125;void process_frame(Mat &amp;image, int opts) &#123; Mat dst = image.clone(); if (opts == 0)&#123; bitwise_not(image, dst); &#125; if (opts == 1)&#123; GaussianBlur(image, dst, Size(0,0), 15); &#125; if (opts == 2)&#123; Canny(image, dst, 100, 200); &#125; dst.copyTo(image); dst.release();&#125; 1234567891011121314151617181920212223242526272829303132333435363738import cv2 as cvcapture = cv.VideoCapture("../images/roadcars.avi")height = capture.get(cv.CAP_PROP_FRAME_HEIGHT)width = capture.get(cv.CAP_PROP_FRAME_WIDTH)count = capture.get(cv.CAP_PROP_FRAME_COUNT)fps = capture.get(cv.CAP_PROP_FPS)print(height, width, count, fps)def process(image, opt=1): dst = None if opt == 0: dst = cv.bitwise_not(image) if opt == 1: dst = cv.GaussianBlur(image, (0, 0), 15) if opt == 2: dst = cv.Canny(image, 100, 200) return dstindex = 0while(True): ret, frame = capture.read() if ret is True: cv.imshow("video-input", frame) c = cv.waitKey(50) if c &gt;= 49: index = c -49 result = process(frame, index) cv.imshow("result", result) print(c) if c == 27: #ESC break else: breakcv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>视频读取与处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-075-图像去水印/修复]]></title>
    <url>%2F2019%2F04%2F23%2Fopencv-075%2F</url>
    <content type="text"><![CDATA[知识点修复API： 123456789void cv::inpaint( InputArray src, InputArray ipaintMask, OutputArray dst double ipaintRadius, // 考虑周围像素范围 int flags //修复方法)基于Navier-Stokes的修复方法基于图像梯度的快速匹配方法又称(Telea法) 代码（c++,python）1234567891011121314151617181920212223242526272829303132#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像去水印/修复 */int main() &#123; Mat src = imread("../images/wm.jpg"); //Mat src = imread("../images/master2.jpg"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); Mat hsv, mask, result; // 得到mask cvtColor(src, hsv, COLOR_BGR2HSV); inRange(hsv, Scalar(100, 43, 46), Scalar(124, 255, 255), mask); Mat se = getStructuringElement(MORPH_RECT, Size(5, 5)); dilate(mask, mask, se); imshow("mask", mask); // 修复 inpaint(src, mask, result, 3, INPAINT_TELEA); imshow("result", result); waitKey(0); return 0;&#125; 12345678910111213141516import cv2 as cvif __name__ == '__main__': src = cv.imread("D:/images/master2.jpg") cv.imshow("watermark image", src) hsv = cv.cvtColor(src, cv.COLOR_BGR2HSV) mask = cv.inRange(hsv, (100, 43, 46), (124, 255, 255)) cv.imshow("mask", mask) cv.imwrite("D:/mask.png", mask) se = cv.getStructuringElement(cv.MORPH_RECT, (5, 5)) cv.dilate(mask, se, mask) result = cv.inpaint(src, mask, 3, cv.INPAINT_TELEA) cv.imshow("result", result) cv.imwrite("D:/result.png", result) cv.waitKey(0) cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像去水印/修复</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-076-图像透视变换应用]]></title>
    <url>%2F2019%2F04%2F23%2Fopencv-076%2F</url>
    <content type="text"><![CDATA[知识点对于很多的文本扫描图像，有时候因为放置的原因导致ROI区域倾斜，这个时候我们会想办法把它纠正为正确的角度视角来，方便下一步的布局分析与文字识别，这个时候通过透视变换就可以取得比较好的裁剪效果，一步就可以实现裁剪与调整。使用透视变换相关几何变换的好处如下： 透视变换不会涉及到几何变换角度旋转 透视变换对畸变图像有一定的展开效果 透视变换可以完成对图像ROI区域提取 API 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像透视变换应用 */int main() &#123; Mat src = imread("../images/case1r.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 二值图像 Mat gray, binary; cvtColor(src, gray, COLOR_BGR2GRAY); threshold(gray, binary, 0, 255, THRESH_BINARY_INV | THRESH_OTSU); // 开操作 Mat se = getStructuringElement(MORPH_RECT, Size(3, 3), Point(-1, -1)); morphologyEx(binary, binary, MORPH_OPEN, se); // 寻找最大轮廓 vector&lt;vector&lt;Point&gt;&gt; contours; vector&lt;Vec4i&gt; hierarchy; findContours(binary, contours, hierarchy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE); int index = -1; int max = 0; for (size_t t = 0; t &lt; contours.size(); t++) &#123; double area = contourArea(contours[t]); if (area &gt; max) &#123; max = area; index = t; &#125; &#125; // 寻找最小外接矩形 RotatedRect rect = minAreaRect(contours[index]); int width = static_cast&lt;int&gt;(rect.size.height); int height = static_cast&lt;int&gt;(rect.size.width); // 透视变换 Point2f vertices[4]; rect.points(vertices); vector&lt;Point&gt; src_pts; vector&lt;Point&gt; dst_pts; dst_pts.push_back(Point(width, height)); dst_pts.push_back(Point(0, height)); dst_pts.push_back(Point(0, 0)); dst_pts.push_back(Point(width, 0)); for (int i = 0; i &lt; 4; i++) &#123; src_pts.push_back(vertices[i]); &#125; Mat M = findHomography(src_pts, dst_pts); Mat result = Mat::zeros(Size(width, height), CV_8UC3); warpPerspective(src, result, M, result.size()); imshow("result", result); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import cv2 as cvimport numpy as npsrc = cv.imread("D:/images/st_02.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# 图像二值化gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)ret, binary = cv.threshold(gray, 0, 255, cv.THRESH_BINARY_INV | cv.THRESH_OTSU)se = cv.getStructuringElement(cv.MORPH_RECT, (3, 3), (-1, -1))binary = cv.morphologyEx(binary, cv.MORPH_OPEN, se)cv.imshow("binary", binary)cv.imwrite("D:/binary.png", binary)# 轮廓提取, 发现最大轮廓out, contours, hierarchy = cv.findContours(binary, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)index = 0max = 0for c in range(len(contours)): area = cv.contourArea(contours[c]) if area &gt; max: max = area index = c# 寻找最小外接矩形rect = cv.minAreaRect(contours[index])print(rect[2])print(rect[0])# trickheight, width = rect[1]print(rect[1])box = cv.boxPoints(rect)src_pts = np.int0(box)print(src_pts)dst_pts = []dst_pts.append([width,height])dst_pts.append([0, height])dst_pts.append([0, 0])dst_pts.append([width, 0])# 透视变换M, status = cv.findHomography(src_pts, np.array(dst_pts))result = cv.warpPerspective(src, M, (np.int32(width), np.int32(height)))if height &lt; width: result = cv.rotate(result, cv.ROTATE_90_CLOCKWISE)cv.imshow("result", result)cv.imwrite("D:/result.png", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像透视变换应用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-074-二值图像分析(提取最大轮廓与编码关键点)]]></title>
    <url>%2F2019%2F04%2F23%2Fopencv-074%2F</url>
    <content type="text"><![CDATA[知识点二值化方法选择： 全局阈值二值化 基于形态学梯度二值化 inRange二值化 基于Canny边缘二值化 自适应二值化 操作步骤 二值化方法，得到二值图像，然后进行轮廓分析，根据面积寻找最大轮廓，然后根据轮廓进行多边形逼近，获得轮廓关键点，最后可以绘制轮廓与关键点。 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 二值图像分析(提取最大轮廓与编码关键点) */int main() &#123; Mat src = imread("../images/case6.jpg"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 图像二值化 Mat gray, binary; GaussianBlur(src, src, Size(5, 5), 0); cvtColor(src, gray, COLOR_BGR2GRAY); threshold(gray, binary, 0, 255, THRESH_BINARY | THRESH_OTSU); // 闭操作 Mat se = getStructuringElement(MORPH_RECT, Size(3, 3)); morphologyEx(binary, binary, MORPH_CLOSE, se); // 发现最大轮廓 vector&lt;vector&lt;Point&gt;&gt; contours; vector&lt;Vec4i&gt; hierarchy; findContours(binary, contours, hierarchy, RETR_LIST, CHAIN_APPROX_SIMPLE); int height = src.rows; int width = src.cols; int index = -1; int max = 0; for (size_t t = 0; t &lt; contours.size(); ++t) &#123; Rect rect = boundingRect(contours[t]); if (rect.height &gt;= height || rect.width &gt;= width) &#123; continue; &#125; double area = contourArea(contours[t]); if (area &gt; max) &#123; max = area; index = t; &#125; &#125; // 绘制关键点 Mat result = Mat::zeros(src.size(), src.type()); Mat pts; drawContours(src, contours, index, Scalar(0, 0, 255)); approxPolyDP(contours[index], pts, 4, true); for (int i = 0; i &lt; pts.rows; ++i) &#123; Vec2i pt = pts.at&lt;Vec2i&gt;(i, 0); circle(src, Point(pt[0], pt[1]), 2, Scalar(0, 255, 0), 2); circle(result, Point(pt[0], pt[1]), 2, Scalar(0, 255, 0), 2); &#125; imshow("result", src); imshow("result_binary", result); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import cv2 as cvimport numpy as npsrc = cv.imread("D:/images/case6.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# 图像二值化# src = cv.GaussianBlur(src, (5, 5), 0)gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)ret, binary = cv.threshold(gray, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)se = cv.getStructuringElement(cv.MORPH_RECT, (3, 3), (-1, -1))binary = cv.morphologyEx(binary, cv.MORPH_CLOSE, se)cv.imshow("binary", binary)# 轮廓提取out, contours, hierarchy = cv.findContours(binary, cv.RETR_LIST, cv.CHAIN_APPROX_SIMPLE)height, width = src.shape[:2]index = 0max = 0for c in range(len(contours)): x, y, w, h = cv.boundingRect(contours[c]) if h &gt;=height or w &gt;= width: continue area = cv.contourArea(contours[c]) if area &gt; max: max = area index = c# 绘制轮廓关键点与轮廓result = np.zeros(src.shape, dtype=np.uint8)keypts = cv.approxPolyDP(contours[index], 4, True)cv.drawContours(src, contours, index, (0, 0, 255), 1, 8)cv.drawContours(result, contours, index, (0, 0, 255), 1, 8)print(keypts)for pt in keypts: cv.circle(src, (pt[0][0], pt[0][1]), 2, (0, 255, 0), 2, 8, 0) cv.circle(result, (pt[0][0], pt[0][1]), 2, (0, 255, 0), 2, 8, 0)cv.imshow("result", result)cv.imshow("output", src)cv.imwrite("D:/result.png", result)cv.imwrite("D:/output.png", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>二值图像分析(提取最大轮廓与编码关键点)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-072-二值图像分析(缺陷检测一)]]></title>
    <url>%2F2019%2F04%2F22%2Fopencv-072%2F</url>
    <content type="text"><![CDATA[知识点缺陷检测，分为两个部分，一个部分是提取指定的轮廓，第二个部分通过对比实现划痕检测与缺角检测。本次主要搞定第一部分，学会观察图像与提取图像ROI对象轮廓外接矩形与轮廓。 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 二值图像分析(缺陷检测一) */int main() &#123; Mat src = imread("../images/ce_02.jpg"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 二值图像 Mat gray, binary; cvtColor(src, gray, COLOR_BGR2GRAY); threshold(gray, binary, 0, 255, THRESH_BINARY_INV | THRESH_OTSU); // 开操作,去掉一些小块 Mat se = getStructuringElement(MORPH_RECT, Size(3, 3)); morphologyEx(binary, binary, MORPH_OPEN, se); // 绘制轮廓 vector&lt;vector&lt;Point&gt;&gt; contours; vector&lt;Vec4i&gt; hierarchy; findContours(binary, contours, hierarchy, RETR_LIST, CHAIN_APPROX_SIMPLE); int height = src.rows; for (size_t t = 0; t &lt; contours.size(); t++) &#123; Rect rect = boundingRect(contours[t]); double area = contourArea(contours[t]); if (rect.height &gt; (height / 2)) &#123; continue; &#125; if (area &lt; 150) &#123; continue; &#125; // 绘制外接矩形 rectangle(src, rect, Scalar(0, 0, 255),2); // 绘制轮廓 drawContours(src, contours, t, Scalar(0, 255, 0), 2); &#125; imshow("result", src); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324252627282930313233import cv2 as cvimport numpy as npsrc = cv.imread("D:/images/ce_02.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# 图像二值化gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)ret, binary = cv.threshold(gray, 0, 255, cv.THRESH_BINARY_INV | cv.THRESH_OTSU)se = cv.getStructuringElement(cv.MORPH_RECT, (3, 3), (-1, -1))binary = cv.morphologyEx(binary, cv.MORPH_OPEN, se)cv.imshow("binary", binary)# 轮廓提取contours, hierarchy = cv.findContours(binary, cv.RETR_LIST, cv.CHAIN_APPROX_SIMPLE)height, width = src.shape[:2]for c in range(len(contours)): x, y, w, h = cv.boundingRect(contours[c]) area = cv.contourArea(contours[c]) if h &gt; (height//2): continue if area &lt; 150: continue cv.rectangle(src, (x, y), (x+w, y+h), (0, 0, 255), 1, 8, 0) cv.drawContours(src, contours, c, (0, 255, 0), 2, 8)cv.imshow("result", src)cv.imwrite("D:/binary2.png", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>二值图像分析(缺陷检测一)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-073-二值图像分析(缺陷检测二)]]></title>
    <url>%2F2019%2F04%2F22%2Fopencv-073%2F</url>
    <content type="text"><![CDATA[知识点对于得到的刀片外接矩形，首先需要通过排序，确定他们的编号，然后根据模板进行相减得到与模板不同的区域，对这些区域进行形态学操作，去掉边缘细微差异，最终就得到了可以检出的缺陷或者划痕刀片。 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;void sort_box(vector&lt;Rect&gt; &amp;boxes);Mat get_template(Mat &amp;binary, vector&lt;Rect&gt; &amp;rects);void detect_defects(Mat &amp;binary, vector&lt;Rect&gt; &amp;rects, Mat &amp;tpl, vector&lt;Rect&gt; &amp;defects);/* * 二值图像分析(缺陷检测二) */int main() &#123; Mat src = imread("../images/ce_02.jpg"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 二值图像 Mat gray, binary; cvtColor(src, gray, COLOR_BGR2GRAY); threshold(gray, binary, 0, 255, THRESH_BINARY_INV | THRESH_OTSU); // 开操作,去掉一些小块 Mat se = getStructuringElement(MORPH_RECT, Size(3, 3)); morphologyEx(binary, binary, MORPH_OPEN, se); // 绘制轮廓 vector&lt;vector&lt;Point&gt;&gt; contours; vector&lt;Vec4i&gt; hierarchy; vector&lt;Rect&gt; rects; findContours(binary.clone(), contours, hierarchy, RETR_LIST, CHAIN_APPROX_SIMPLE); int height = src.rows; for (size_t t = 0; t &lt; contours.size(); t++) &#123; Rect rect = boundingRect(contours[t]); double area = contourArea(contours[t]); if (rect.height &gt; (height / 2)) &#123; continue; &#125; if (area &lt; 150) &#123; continue; &#125; rects.push_back(rect); // 填充边缘，放大缺陷 drawContours(binary, contours, t, Scalar(0), 2, 8); &#125; // 对外接矩形框排序 sort_box(rects); // 获取模板 Mat tpl = get_template(binary, rects); for (int i = 0; i &lt; rects.size(); ++i) &#123; putText(src, format("num:%d", (i + 1)), Point(rects[i].x - 60, rects[i].y + 15), FONT_HERSHEY_PLAIN, 1.0, Scalar(255, 0, 0), 1); &#125; // 检测并标明结果 vector&lt;Rect&gt; defects; detect_defects(binary, rects, tpl, defects); for (int i = 0; i &lt; defects.size(); ++i) &#123; rectangle(src, defects[i], Scalar(0, 0, 255)); putText(src, "bad", Point(defects[i].x, defects[i].y), FONT_HERSHEY_PLAIN, 1.0, Scalar(0, 255, 0), 1); &#125; imshow("result", src); waitKey(0); return 0;&#125;void detect_defects(Mat &amp;binary, vector&lt;Rect&gt; &amp;rects, Mat &amp;tpl, vector&lt;Rect&gt; &amp;defects) &#123; int height = tpl.rows; int width = tpl.cols; int index = 1; int size = rects.size(); // 发现缺失 for (int i = 0; i &lt; size; ++i) &#123; Mat roi = binary(rects[i]); resize(roi, roi, tpl.size()); Mat mask; subtract(tpl, roi, mask); Mat se = getStructuringElement(MORPH_RECT, Size(5, 5)); morphologyEx(mask, mask, MORPH_OPEN, se); threshold(mask, mask, 0, 255, THRESH_BINARY); int count = 0; for (int row = 0; row &lt; height; ++row) &#123; for (int col = 0; col &lt; width; ++col) &#123; int pv = mask.at&lt;uchar&gt;(row, col); if (pv == 255) &#123; ++count; &#125; &#125; &#125; if (count &gt; 0) &#123; defects.push_back(rects[i]); &#125; &#125;&#125;Mat get_template(Mat &amp;binary, vector&lt;Rect&gt; &amp;rects) &#123; return binary(rects[0]);&#125;void sort_box(vector&lt;Rect&gt; &amp;boxes) &#123; int size = boxes.size(); for (int i = 0; i &lt; size - 1; ++i) &#123; for (int j = i; j &lt; size; ++j) &#123; if (boxes[j].y &lt; boxes[i].y) &#123; Rect tmp = boxes[i]; boxes[i] = boxes[j]; boxes[j] = tmp; &#125; &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102import cv2 as cvimport numpy as npdef sort_boxes(rois): for i in range(0, len(rois)-1, 1): for j in range(i, len(rois), 1): x, y, w, h = rois[j] if y &lt; rois[i][1]: bx, by, bw, bh = rois[i] rois[i] = [x, y, w, h] rois[j] = [bx, by, bw, bh] return rois;def get_template(binary, boxes): x, y, w, h = boxes[0] roi = binary[y:y+h, x:x+w] return roidef detect_defect(binary, boxes, tpl): height, width = tpl.shape index = 1 defect_rois = [] # 发现缺失 for x, y, w, h in boxes: roi = binary[y:y + h, x:x + w] roi = cv.resize(roi, (width, height)) mask = cv.subtract(tpl, roi) se = cv.getStructuringElement(cv.MORPH_RECT, (5, 5), (-1, -1)) mask = cv.morphologyEx(mask, cv.MORPH_OPEN, se) ret, mask = cv.threshold(mask, 0, 255, cv.THRESH_BINARY) count = 0 for row in range(height): for col in range(width): pv = mask[row, col] if pv == 255: count += 1 if count &gt; 0: defect_rois.append([x, y, w, h]) index += 1 return defect_roissrc = cv.imread("D:\code-workspace\Clion-workspace\learnOpencv\images\ce_02.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# 图像二值化gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)ret, binary = cv.threshold(gray, 0, 255, cv.THRESH_BINARY_INV | cv.THRESH_OTSU)se = cv.getStructuringElement(cv.MORPH_RECT, (3, 3), (-1, -1))binary = cv.morphologyEx(binary, cv.MORPH_OPEN, se)cv.imshow("binary", binary)# 轮廓提取contours, hierarchy = cv.findContours(binary, cv.RETR_LIST, cv.CHAIN_APPROX_SIMPLE)height, width = src.shape[:2]rects = []for c in range(len(contours)): x, y, w, h = cv.boundingRect(contours[c]) area = cv.contourArea(contours[c]) if h &gt; (height//2): continue if area &lt; 150: continue rects.append([x, y, w, h])# 排序轮廓rects = sort_boxes(rects)print(rects)template = get_template(binary, rects);# 填充边缘for c in range(len(contours)): x, y, w, h = cv.boundingRect(contours[c]) area = cv.contourArea(contours[c]) if h &gt; (height//2): continue if area &lt; 150: continue cv.drawContours(binary, contours, c, (0), 2, 8)cv.imshow("template", template)# 检测缺陷defect_boxes = detect_defect(binary, rects, template)for dx, dy, dw, dh in defect_boxes: cv.rectangle(src, (dx, dy), (dx + dw, dy + dh), (0, 0, 255), 1, 8, 0) cv.putText(src, "bad", (dx, dy), cv.FONT_HERSHEY_PLAIN, 1.0, (0, 255, 0), 2)index = 1for dx, dy, dw, dh in rects: cv.putText(src, "num:%d"%index, (dx-40, dy+15), cv.FONT_HERSHEY_PLAIN, 1.0, (255, 0, 0), 1) index += 1cv.imshow("result", src)cv.imwrite("D:/binary2.png", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>二值图像分析(缺陷检测二)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-071-形态学操作(击中击不中)]]></title>
    <url>%2F2019%2F04%2F22%2Fopencv-071%2F</url>
    <content type="text"><![CDATA[知识点形态学的击中击不中操作，根据结构元素不同，可以提取二值图像中的一些特殊区域，得到我们想要的结果。 API 1234567891011121314151617void cv::morphologyEx( InputArray src, OutputArray dst, int op, InputArray kernel, Point anchor = Point(-1,-1), int iterations = 1, int borderType = BORDER_CONSTANT,)src 输入图像dst 输出图像op 形态学操作kernel 结构元素anchor 中心位置锚定iterations 循环次数borderType 边缘填充类型其中op指定为MORPH_HITMISS即表示使用击中击不中 代码（c++,python）123456789101112131415161718192021222324252627282930#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 形态学操作(击中击不中) */int main() &#123; Mat src = imread("../images/cross.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 二值图像 Mat gray, binary, result; cvtColor(src, gray, COLOR_BGR2GRAY); threshold(gray, binary, 0, 255, THRESH_BINARY_INV | THRESH_OTSU); // 击中击不中 Mat se = getStructuringElement(MORPH_CROSS, Size(11,11)); morphologyEx(binary, result, MORPH_HITMISS, se); imshow("bit_and_miss", result); waitKey(0); return 0;&#125; 123456789101112131415161718192021import cv2 as cvimport numpy as npsrc = cv.imread("D:/images/cross.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# 图像二值化gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)ret, binary = cv.threshold(gray, 0, 255, cv.THRESH_BINARY_INV | cv.THRESH_OTSU)# 击中击不中se = cv.getStructuringElement(cv.MORPH_CROSS, (11, 11), (-1, -1))binary = cv.morphologyEx(binary, cv.MORPH_HITMISS, se)cv.imshow("black hat", binary)cv.imwrite("D:/binary2.png", binary)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>形态学操作(击中击不中)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-068-图像形态学(黑帽操作)]]></title>
    <url>%2F2019%2F04%2F21%2Fopencv-068%2F</url>
    <content type="text"><![CDATA[知识点形态学的黑帽操作是闭操作与输入图像之间的差异，黑帽操作可以表示如下：黑帽操作 = 闭操作 – 输入图像 API 1234567891011121314151617void cv::morphologyEx( InputArray src, OutputArray dst, int op, InputArray kernel, Point anchor = Point(-1,-1), int iterations = 1, int borderType = BORDER_CONSTANT,)src 输入图像dst 输出图像op 形态学操作kernel 结构元素anchor 中心位置锚定iterations 循环次数borderType 边缘填充类型其中op指定为MORPH_BLACKHAT即表示使用顶帽操作 代码（c++,python）12345678910111213141516171819202122232425262728293031#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像形态学(黑帽操作) */int main() &#123; Mat src = imread("../images/morph.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 二值图像 Mat gray, binary, result; cvtColor(src, gray, COLOR_BGR2GRAY); threshold(gray, binary, 0, 255, THRESH_BINARY | THRESH_OTSU); // 黑帽操作 Mat se = getStructuringElement(MORPH_RECT, Size(15, 15), Point(-1, -1)); morphologyEx(binary, result, MORPH_CLOSE, se); imshow("close_op", result); morphologyEx(binary, result, MORPH_BLACKHAT, se); imshow("blackhat_op", result); waitKey(0); return 0;&#125; 123456789101112131415161718192021import cv2 as cvimport numpy as npsrc = cv.imread("D:/images/morph.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# 图像二值化gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)ret, binary = cv.threshold(gray, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)# 黑帽操作se = cv.getStructuringElement(cv.MORPH_RECT, (9, 9), (-1, -1))binary = cv.morphologyEx(binary, cv.MORPH_BLACKHAT, se)cv.imshow("black hat", binary)cv.imwrite("D:/binary2.png", binary)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像形态学(黑帽操作)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-069-图像形态学(图像梯度)]]></title>
    <url>%2F2019%2F04%2F21%2Fopencv-069%2F</url>
    <content type="text"><![CDATA[知识点图像形态学的梯度跟我们前面介绍的图像卷积计算出来的梯度有本质不同，形态学梯度可以帮助我们获得连通组件的边缘与轮廓，实现图像轮廓或者边缘提取。根据使用的形态学操作不同，形态学梯度又分为 基本梯度：图像膨胀与腐蚀操作之间的差值 内梯度：输入图像与腐蚀之间的差值 外梯度：膨胀与输入图像之间的差值 API 1234567891011121314151617void cv::morphologyEx( InputArray src, OutputArray dst, int op, InputArray kernel, Point anchor = Point(-1,-1), int iterations = 1, int borderType = BORDER_CONSTANT,)src 输入图像dst 输出图像op 形态学操作kernel 结构元素anchor 中心位置锚定iterations 循环次数borderType 边缘填充类型其中op指定为MORPH_GRADIEN即表示使用基本梯度操作 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像形态学(图像梯度) */int main() &#123; Mat src = imread("../images/master.jpg", IMREAD_GRAYSCALE); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); //定义结构元素 Mat se = getStructuringElement(MORPH_RECT, Size(3, 3)); Mat basic, exter, inter; // 基本梯度 morphologyEx(src, basic, MORPH_GRADIENT, se); imshow("basic_gradient", basic); // 外梯度 morphologyEx(src, exter, MORPH_DILATE, se); subtract(exter, src, exter); imshow("exter_gradient", exter); // 内梯度 morphologyEx(src, inter, MORPH_ERODE, se); subtract(src, inter, inter); imshow("inter_gradient", inter); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526import cv2 as cvsrc = cv.imread("D:/images/dannis2.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# 形态学梯度 - 基本梯度se = cv.getStructuringElement(cv.MORPH_RECT, (3, 3), (-1, -1))basic = cv.morphologyEx(src, cv.MORPH_GRADIENT, se)cv.imshow("basic gradient", basic)# 外梯度dilate = cv.morphologyEx(src, cv.MORPH_DILATE, se)exteral = cv.subtract(dilate, src)cv.imshow("external gradient", exteral)# 内梯度erode = cv.morphologyEx(src, cv.MORPH_ERODE, se)interal = cv.subtract(src, erode)cv.imshow("interal gradient", interal)cv.imwrite("D:/gradient.png", basic)cv.imwrite("D:/external.png", exteral)cv.imwrite("D:/interal.png", interal)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像形态学(图像梯度)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-70-形态学应用(使用基本梯度实现轮廓分析)]]></title>
    <url>%2F2019%2F04%2F21%2Fopencv-070%2F</url>
    <content type="text"><![CDATA[知识点基于形态学梯度实现图像二值化，进行文本结构分析是OCR识别中常用的处理手段之一，这种好处比简单的二值化对图像有更好的分割效果，主要步骤如下： 图像形态学梯度 灰度 全局阈值二值化 轮廓分析 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 形态学应用(使用基本梯度实现轮廓分析) */int main() &#123; Mat src = imread("../images/address.jpg"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); Mat basic, gray, binary; // 基本梯度 Mat se = getStructuringElement(MORPH_RECT, Size(3, 3)); morphologyEx(src, basic, MORPH_GRADIENT, se); // 二值化 cvtColor(basic, gray, COLOR_BGR2GRAY); threshold(gray, binary, 0, 255, THRESH_BINARY | THRESH_OTSU); // 膨胀 se = getStructuringElement(MORPH_RECT, Size(1, 5)); morphologyEx(binary, binary, MORPH_DILATE, se); // 轮廓绘制 vector&lt;vector&lt;Point&gt;&gt; contours; vector&lt;Vec4i&gt; hierarchy; findContours(binary, contours, hierarchy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE); for (size_t c = 0; c &lt; contours.size(); c++) &#123; Rect rect = boundingRect(contours[c]); double area = contourArea(contours[c]); if (area &lt; 200) &#123; continue; &#125; int h = rect.height; int w = rect.width; if (h &gt; (3 * w) || h &lt; 20) &#123; continue; &#125; rectangle(src, rect, Scalar(0, 0, 255)); &#125; imshow("result", src); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324252627282930import cv2 as cvsrc = cv.imread("D:/images/kd02.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# 形态学梯度 - 基本梯度se = cv.getStructuringElement(cv.MORPH_RECT, (3, 3), (-1, -1))basic = cv.morphologyEx(src, cv.MORPH_GRADIENT, se)cv.imshow("basic gradient", basic)gray = cv.cvtColor(basic, cv.COLOR_BGR2GRAY)ret, binary = cv.threshold(gray, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)cv.imshow("binary", binary)se = cv.getStructuringElement(cv.MORPH_RECT, (1, 5), (-1, -1))binary = cv.morphologyEx(binary, cv.MORPH_DILATE, se)out, contours, hierarchy = cv.findContours(binary, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)for c in range(len(contours)): x, y, w, h = cv.boundingRect(contours[c]) area = cv.contourArea(contours[c]) if area &lt; 200: continue if h &gt; (3*w) or h &lt; 20: continue cv.rectangle(src, (x, y), (x+w, y+h), (0, 0, 255), 1, 8, 0)cv.imshow("result", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>形态学应用(使用基本梯度实现轮廓分析)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-067-图像形态学(顶帽操作)]]></title>
    <url>%2F2019%2F04%2F21%2Fopencv-067%2F</url>
    <content type="text"><![CDATA[知识点形态学的顶帽操作是图像输入与开操作之间的差异，顶帽操作有时候对于我们提取图像中微小部分特别有用。 顶帽操作：顶帽 = 原图 – 开操作 API 1234567891011121314151617void cv::morphologyEx( InputArray src, OutputArray dst, int op, InputArray kernel, Point anchor = Point(-1,-1), int iterations = 1, int borderType = BORDER_CONSTANT,)src 输入图像dst 输出图像op 形态学操作kernel 结构元素anchor 中心位置锚定iterations 循环次数borderType 边缘填充类型其中op指定为MORPH_TOPHAT 即表示使用顶帽操作 代码（c++,python）123456789101112131415161718192021222324252627282930313233#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像形态学(顶帽操作) */int main() &#123; Mat src = imread("../images/cells.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 二值化 Mat gray, binary, result; cvtColor(src, gray, COLOR_BGR2GRAY); threshold(gray, binary, 0, 255, THRESH_BINARY | THRESH_OTSU); // 开操作 Mat se = getStructuringElement(MORPH_RECT, Size(3, 3)); morphologyEx(binary, result, MORPH_OPEN, se); imshow("open_demo", result); // 顶帽操作 morphologyEx(binary, result, MORPH_TOPHAT, se); imshow("tophat_demo", result); waitKey(0); return 0;&#125; 123456789101112131415161718192021import cv2 as cvimport numpy as npsrc = cv.imread("D:/images/cells.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# 二值化gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)ret, binary = cv.threshold(gray, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)# 顶帽操作se = cv.getStructuringElement(cv.MORPH_RECT, (3, 3), (-1, -1))binary = cv.morphologyEx(binary, cv.MORPH_TOPHAT, se)cv.imshow("binary", binary)cv.imwrite("D:/binary2.png", binary)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像形态学(顶帽操作)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-065-图像形态学(闭操作)]]></title>
    <url>%2F2019%2F04%2F20%2Fopencv-065%2F</url>
    <content type="text"><![CDATA[知识点形态学的闭操作跟开操作一样也是基于腐蚀与膨胀两个操作的组合实现的闭操作 = 膨胀 + 腐蚀闭操作的作用：闭操作可以填充二值图像中孔洞区域，形成完整的闭合区域连通组件 123456789101112131415161718void cv::morphologyEx( InputArray src, OutputArray dst, int op, InputArray kernel, Point anchor = Point(-1,-1), int iterations = 1, int borderType = BORDER_CONSTANT,)src 输入图像dst 输出图像op 形态学操作kernel 结构元素anchor 中心位置锚定iterations 循环次数borderType 边缘填充类型其中op指定为MORPH_CLOSE 即表示使用闭操作 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像形态学(闭操作) */int main() &#123; Mat src = imread("../images/cells.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 二值图像 Mat gray, binary, result; cvtColor(src, gray, COLOR_BGR2GRAY); threshold(gray, binary, 0, 255, THRESH_BINARY | THRESH_OTSU); imshow("binary", binary); // 闭操作 Mat se1 = getStructuringElement(MORPH_RECT, Size(25, 5), Point(-1, -1)); Mat se2 = getStructuringElement(MORPH_RECT, Size(5, 25), Point(-1, -1)); morphologyEx(binary, result, MORPH_CLOSE, se1); morphologyEx(result, result, MORPH_CLOSE, se2); imshow("close_demo 25*5 5*25", result); // 去除中间小方黑块 Mat se3 = getStructuringElement(MORPH_CROSS, Size(31, 21)); morphologyEx(result, result, MORPH_CLOSE, se3); // imshow("cross", result); morphologyEx(result, result, MORPH_CLOSE, se1); morphologyEx(result, result, MORPH_CLOSE, se2); imshow("final", result); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324import cv2 as cvimport numpy as npsrc = cv.imread("D:/images/cells.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# 高斯模糊去噪声gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)ret, binary = cv.threshold(gray, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)cv.imwrite("D:/binary1.png", binary)cv.imshow("binary1", binary)# 开操作se1 = cv.getStructuringElement(cv.MORPH_RECT, (25, 5), (-1, -1))se2 = cv.getStructuringElement(cv.MORPH_RECT, (5, 25), (-1, -1))binary = cv.morphologyEx(binary, cv.MORPH_CLOSE, se1)binary = cv.morphologyEx(binary, cv.MORPH_CLOSE, se2)cv.imshow("binary", binary)cv.imwrite("D:/binary2.png", binary)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像形态学(闭操作)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-066-图像形态学(开闭操作时候结构元素应用演示)]]></title>
    <url>%2F2019%2F04%2F20%2Fopencv-066%2F</url>
    <content type="text"><![CDATA[知识点OpenCV中图像形态学开操作与闭操作，根据结构元素的不同可以实现不同的二值图像处理效果，我们可以通过下面的结构元素对图像进行开操作，提取二值图像中水平与垂直线，这个方法比霍夫直线检测要好用得多， 在一些应用场景中会特别有用，图像分析、OCR布局分析中形态学操作十分重要，我们通过两个例子来说明开闭操作的作用。 1 开操作提取水平线，实现填空题横线位置提取结构元素大小为20x1 步骤： 转灰度 转二值，可选降噪 形态学操作，提取水平线 轮廓发现，确定位置 2 闭操作实现不同层次的轮廓填充结构元素分为三种： 矩形结构元素35x35大小 矩形结构元素30x30大小 圆形结构元素30x30大小 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;void open_demo();void close_demo();/* * 图像形态学(开闭操作时候结构元素应用演示) */int main() &#123; //open_demo(); close_demo(); waitKey(0); return 0;&#125;void close_demo() &#123; // 读取图像 Mat src = imread("../images/morph3.png"); imshow("input", src); // 二值图像 Mat gray, binary; cvtColor(src, gray, COLOR_BGR2GRAY); threshold(gray, binary, 0, 255, THRESH_BINARY | THRESH_OTSU); imshow("binary", binary); // 闭操作 //Mat se = getStructuringElement(MORPH_ELLIPSE, Size(30, 30), Point(-1, -1)); //Mat se = getStructuringElement(MORPH_RECT, Size(30, 30), Point(-1, -1)); Mat se = getStructuringElement(MORPH_RECT, Size(35, 35), Point(-1, -1)); morphologyEx(binary, binary, MORPH_CLOSE, se); imshow("close_demo rect=35,35", binary);&#125;void open_demo() &#123; // 读取图像 Mat src = imread("../images/fill.png"); imshow("input", src); // 二值图像 Mat gray, binary; cvtColor(src, gray, COLOR_BGR2GRAY); threshold(gray, binary, 0, 255, THRESH_BINARY_INV | THRESH_OTSU); imshow("binary", binary); // 开操作 Mat se = getStructuringElement(MORPH_RECT, Size(25, 1), Point(-1, -1)); morphologyEx(binary, binary, MORPH_OPEN, se); imshow("open_op", binary); // 绘制填空位置 vector&lt;vector&lt;Point&gt;&gt; contours; vector&lt;Vec4i&gt; hierarhy; findContours(binary, contours, hierarhy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE, Point(-1, -1)); for (size_t t = 0; t &lt; contours.size(); t++) &#123; Rect roi = boundingRect(contours[t]); roi.y = roi.y - 10; roi.height = 12; rectangle(src, roi, Scalar(0, 0, 255)); &#125; // 显示结果 imshow("open_demo", src);&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import cv2 as cvimport numpy as npdef open_demo(): src = cv.imread("D:/images/fill.png") cv.namedWindow("input", cv.WINDOW_AUTOSIZE) cv.imshow("input", src) # 图像二值化 gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY) ret, binary = cv.threshold(gray, 0, 255, cv.THRESH_BINARY_INV | cv.THRESH_OTSU) cv.imwrite("D:/binary1.png", binary) cv.imshow("binary1", binary) # 开操作 se1 = cv.getStructuringElement(cv.MORPH_RECT, (20, 1), (-1, -1)) binary = cv.morphologyEx(binary, cv.MORPH_OPEN, se1) cv.imshow("binary", binary) cv.imwrite("D:/binary2.png", binary) # 提取轮廓 out, contours, hierarchy = cv.findContours(binary, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE) for c in range(len(contours)): x, y, w, h = cv.boundingRect(contours[c]) y = y - 10 h = 12 cv.rectangle(src, (x, y), (x+w, y+h), (0, 0, 255), 1, 8, 0) cv.imshow("result", src)def close_demo(): src = cv.imread("D:/images/morph3.png") cv.namedWindow("input", cv.WINDOW_AUTOSIZE) cv.imshow("input", src) # 图像二值化 gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY) ret, binary = cv.threshold(gray, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU) # 闭操作 se = cv.getStructuringElement(cv.MORPH_ELLIPSE, (15, 15), (-1, -1)) binary = cv.morphologyEx(binary, cv.MORPH_CLOSE, se) cv.imwrite("D:/close.png", binary) cv.imshow("close", binary)close_demo()cv.waitKey(0)cv.destroyAllWindows() 结果开操作应用 闭操作应用 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像形态学(开闭操作时候结构元素应用演示)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-064-图像形态学(开操作)]]></title>
    <url>%2F2019%2F04%2F20%2Fopencv-064%2F</url>
    <content type="text"><![CDATA[知识点形态学的开操作是基于腐蚀与膨胀两个操作的组合实现的开操作 = 腐蚀 + 膨胀开操作的作用：开操作可以删除二值图像中小的干扰块，降低图像二值化之后噪点过多的问题。 API 12345678910111213141516171819void cv::morphologyEx( InputArray src, OutputArray dst, int op, InputArray kernel, Point anchor = Point(-1,-1), int iterations = 1, int borderType = BORDER_CONSTANT,)src 输入图像dst 输出图像op 形态学操作kernel 结构元素anchor 中心位置锚定iterations 循环次数borderType 边缘填充类型其中op指定为MORPH_OPEN 即表示使用开操作 代码（c++,python）1234567891011121314151617181920212223242526272829303132#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像形态学(开操作) */int main() &#123; Mat src = imread("../images/shuini.jpg"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 二值化 Mat gray, binary, result; cvtColor(src, gray, COLOR_BGR2GRAY); GaussianBlur(gray, gray, Size(9, 9), 0, 0); adaptiveThreshold(gray, binary, 255, ADAPTIVE_THRESH_GAUSSIAN_C, THRESH_BINARY_INV, 45, 15); imshow("binary", binary); // 开操作 Mat se = getStructuringElement(MORPH_RECT, Size(5,5)); morphologyEx(binary, result, MORPH_OPEN, se); imshow("open demo", result); waitKey(0); return 0;&#125; 123456789101112131415161718192021import cv2 as cvimport numpy as npsrc = cv.imread("D:/images/shuini.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# 高斯模糊去噪声gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)dst = cv.GaussianBlur(gray, (9, 9), 2, 2)binary = cv.adaptiveThreshold(dst, 255, cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY_INV, 45, 15)# 开操作se = cv.getStructuringElement(cv.MORPH_RECT, (5, 5), (-1, -1))binary = cv.morphologyEx(binary, cv.MORPH_OPEN, se)cv.imshow("binary", binary)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像形态学(开操作)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-062-图像形态学(膨胀与腐蚀)]]></title>
    <url>%2F2019%2F04%2F19%2Fopencv-062%2F</url>
    <content type="text"><![CDATA[知识点膨胀与腐蚀是图像形态学最基础的两个操作，形态学的其它操作都是基于这两个操作基础上得到的，图像形态学是二值图像分析的重要分支学科。在OpenCV中膨胀与腐蚀对应两个相关的API，膨胀可以看成是最大值滤波，即用最大值替换中心像素点；腐蚀可以看出是最小值滤波，即用最小值替换中心像素点。 膨胀的API 12345678910111213141516void cv::dilate( InputArray src, OutputArray dst, InputArray kernel, Point anchor = Point(-1,-1), int iterations = 1, int borderType = BORDER_CONSTANT, const Scalar &amp; borderValue = morphologyDefaultBorderValue() )src 输入图像，任意通道的dst 输出图像，类型与通道数目必须跟输入保持一致kernel 结构元素anchor 中心位置锚定iterations 循环次数borderType 边缘填充类型 腐蚀的API 123456789101112131415void cv::erode( InputArray src, OutputArray dst, InputArray kernel, Point anchor = Point(-1,-1), int iterations = 1, int borderType = BORDER_CONSTANT, const Scalar &amp; borderValue = morphologyDefaultBorderValue() )src 输入图像，任意通道的dst 输出图像，类型与通道数目必须跟输入保持一致kernel 结构元素anchor 中心位置锚定iterations 循环次数borderType 边缘填充类型 代码（c++,python）123456789101112131415161718192021222324252627282930#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像形态学(膨胀与腐蚀) */int main() &#123; Mat src = imread("../images/master.jpg"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); Mat dresult, eresult; // 定义结构元素3*3大小的矩形 Mat se = getStructuringElement(MORPH_RECT, Size(3,3)); // 膨胀 dilate(src, dresult, se); // 腐蚀 erode(src, eresult, se); imshow("dilate", dresult); imshow("erode", eresult); waitKey(0); return 0;&#125; 1234567891011121314151617import cv2 as cvimport numpy as npsrc = cv.imread("D:/images/dannis2.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# 使用3x3结构元素进行膨胀与腐蚀操作se = np.ones((3, 3), dtype=np.uint8)dilate = cv.dilate(src, se, None, (-1, -1), 1)erode = cv.erode(src, se, None, (-1, -1), 1)# 显示cv.imshow("dilate", dilate)cv.imshow("erode", erode)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像形态学(膨胀与腐蚀)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-063-图像形态学(膨胀与腐蚀-获取结构元素)]]></title>
    <url>%2F2019%2F04%2F19%2Fopencv-063%2F</url>
    <content type="text"><![CDATA[知识点膨胀与腐蚀操作不仅可以对二值图像有效操作，对彩色与灰度图像也有作用，对于二值图像的腐蚀与膨胀来说，选择一个好的结构元素至关重要。 API 12345678Mat cv::getStructuringElement( int shape, Size ksize, Point anchor = Point(-1,-1s))shape是指结构元素的类型，常见的有矩形、圆形、十字交叉ksize 是指结构元素大小anchor 中心锚点的位置 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像形态学(膨胀与腐蚀-获取结构元素) */int main() &#123; Mat src = imread("../images/coins.jpg"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 二值化 Mat gray, binary; cvtColor(src, gray, COLOR_BGR2GRAY); threshold(gray, binary, 0, 255, THRESH_BINARY | THRESH_OTSU); imshow("binary", binary); Mat dresult, eresult; // 定义结构元素3*3大小的矩形 Mat se = getStructuringElement(MORPH_RECT, Size(3,3)); // 膨胀 dilate(binary, dresult, se); // 腐蚀 erode(binary, eresult, se); imshow("dilate", dresult); imshow("erode", eresult); waitKey(0); return 0;&#125; 123456789101112131415161718192021import cv2 as cvimport numpy as npsrc = cv.imread("D:/images/coins.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)# 二值化图像gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)ret, binary = cv.threshold(gray, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)cv.imshow("input", binary)# 使用3x3结构元素进行膨胀与腐蚀操作se = cv.getStructuringElement(cv.MORPH_RECT, (3, 3), (-1, -1))dilate = cv.dilate(binary, se, None, (-1, -1), 1)erode = cv.erode(binary, se, None, (-1, -1), 1)# 显示cv.imshow("dilate", dilate)cv.imshow("erode", erode)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像形态学(膨胀与腐蚀-获取结构元素)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-061-二值图像分析(霍夫圆检测)]]></title>
    <url>%2F2019%2F04%2F19%2Fopencv-061%2F</url>
    <content type="text"><![CDATA[知识点根据极坐标,圆上任意一点的坐标可以表示为如下形式, 所以对于任意一个圆, 假设中心像素点p(x0, y0)像素点已知, 圆半径已知,则旋转360由极坐标方程可以得到每个点上得坐标同样,如果只是知道图像上像素点, 圆半径,旋转360°则中心点处的坐标值必定最强.这正是霍夫变换检测圆的数学原理。 API 123456789101112131415161718192021OpenCV中霍夫圆检测的API与参数解释如下：void cv::HoughCircles( InputArray image, OutputArray circles, int method, double dp, double minDist, double param1 = 100, double param2 = 100, int minRadius = 0, int maxRadius = 0 )image表示输入单通道的灰度图像circles 表示检测的圆信息（圆心+半径）method 圆检测的方法dp表示图像分辨率是否有变化，默认1表示保持跟原图大小一致，一般取2，比1 要大minDist表示检测到的圆，两个圆心之间的最小距离param1 表示边缘提取的高阈值param2表示霍夫空间的累加阈值minRadius 表示可以检测圆的最小半径maxRadius 表示可以检测圆的最大 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 二值图像分析(霍夫圆检测) */int main() &#123; Mat src = imread("../images/coins.jpg"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 转换为灰度空间与高斯模糊 Mat gray; cvtColor(src, gray, COLOR_BGR2GRAY); GaussianBlur(gray, gray, Size(9, 9), 2, 2); // 霍夫圆检测 int dp = 2; // 在其它参数保持不变的情况下。dp的取值越高，越容易检测到圆 int min_radius = 20; int max_radius = 100; int minDist = 10; vector&lt;Vec3f&gt; circles; HoughCircles(gray, circles, HOUGH_GRADIENT, dp, minDist, 100, 100, min_radius, max_radius); for (size_t i = 0; i &lt; circles.size(); ++i) &#123; Point center(cvRound(circles[i][0]), cvRound(circles[i][1])); int radius = cvRound(circles[i][2]); // 绘制圆 circle(src, center, radius, Scalar(0, 0, 255), 3); circle(src, center, 3, Scalar(0, 255, 0), -1); //绘制圆心 &#125; imshow("result_dp=2", src); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425import cv2 as cvimport numpy as npsrc = cv.imread("D:/images/test_coins.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)gray = cv.GaussianBlur(gray, (9, 9), 2, 2)dp = 2param1 = 100param2 = 80circles = cv.HoughCircles(gray, cv.HOUGH_GRADIENT, dp, 10, None, param1, param2, 20, 100)for c in circles[0,:]: print(c) cx, cy, r = c cv.circle(src, (cx, cy), 2, (0, 255, 0), 2, 8, 0) cv.circle(src, (cx, cy), r, (0, 0, 255), 2, 8, 0)cv.imshow("hough line demo", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>二值图像分析(霍夫圆检测)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[命令行运行python模块提示找不到包]]></title>
    <url>%2F2019%2F04%2F18%2Fterminal_run_python%2F</url>
    <content type="text"><![CDATA[原因分析 如上图，当在命令行中执行以下命令时： 1python cls_train.py 会提示找不到datasets模块，原因是在命令行运行python文件，系统只会在当前文件夹runs/下寻找，不会到上一级目录查找，在pycharm等IDE中会自动在整个项目中查找，所以命令行运行python文件会提示找不到包。 解决方法在cls_train.py文件中，添加以下代码，将项目根目录加入搜索路径即可，一定要写在导入其他模块之前 12import syssys.path.append('/.../ISIC2018-master/') 见下图： 现在再运行python cls_train.py就没问题了。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>命令行运行python程序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用 pyenv 对python进行多版本管理]]></title>
    <url>%2F2019%2F04%2F18%2Fpyenv%2F</url>
    <content type="text"><![CDATA[阅读链接 pyenv 让 python 版本完美切换]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>pyenv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow图像处理的完整样例]]></title>
    <url>%2F2019%2F04%2F18%2Ftensorflow_image_process%2F</url>
    <content type="text"><![CDATA[代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tf# 给定一张图像，随机调整图像的色彩。因为调整亮度、对比度、饱和度和色相的顺序会影# 响最后得到的结果，所以可以定义多种不同的顺序。具体使用哪一种顺序可以在训练数据# 预处理时随机选择一种。这样可以进一步降低无关因素对模型的影响。def distort_color(image, color_ordering=0): if color_ordering == 0: image = tf.image.random_brightness(image, max_delta=32.0 / 255.0) image = tf.image.random_saturation(image, lower=0.5, upper=1.5) image = tf.image.random_hue(image, max_delta=0.2) image = tf.image.random_contrast(image, lower=0.5, upper=1.5) elif color_ordering == 1: image = tf.image.random_saturation(image, lower=0.5, upper=1.5) image = tf.image.random_brightness(image, max_delta=32.0 / 255.0) image = tf.image.random_contrast(image, lower=0.5, upper=1.5) image = tf.image.random_hue(image, max_delta=0.2) elif color_ordering == 2: pass return tf.clip_by_value(image, 0.0, 1.0)# 给定一张解码后的图像、目标图像的尺寸以及图像上的标注框，此函数可以对给出的图像进行预# 处理。这个函数的输入图像是图像识别问题中原始的训练图像，而输出则是神经网络模型的输入# 层。注意这里只处理模型的训练数据，对于预测的数据，一般不需要使用随机变换的步骤。def preprocess_for_train(image, height, width, bbox): # 如果没有提供标注框，则认为整个图像就是需要关注的部分 if bbox is None: bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4]) # 转换图像张量的类型 if image.dtype != tf.float32: image = tf.image.convert_image_dtype(image, dtype=tf.float32) # 随机截取图像，减小需要关注的物体大小对图像识别算法的影响 bbox_begin, bbox_size, _ = tf.image.sample_distorted_bounding_box(tf.shape(image), bounding_boxes=bbox) distorted_image = tf.slice(image, bbox_begin, bbox_size) # 将随机截取的图像调整为神经网络输入层的大小，大小调整的算法是随机选择的 distorted_image = tf.image.resize_images(distorted_image, [height, width], method=np.random.randint(4)) # 随机左右翻转图像 distorted_image = tf.image.random_flip_left_right(distorted_image) # 使用一种随机的顺序调整图像色彩 distorted_image = distort_color(distorted_image, np.random.randint(2)) return distorted_imageimage_raw_data = tf.gfile.FastGFile("D:\\code-workspace\\Clion-workspace\\learnOpencv\\images\\lena.jpg", "rb").read()with tf.Session() as sess: img_data = tf.image.decode_jpeg(image_raw_data) boxes = tf.constant([[[0.05, 0.05, 0.9, 0.7], [0.35, 0.47, 0.5, 0.56]]]) # 运行六次获得6种不同的图像，并展示 plt.figure() for i in range(6): result = preprocess_for_train(img_data, 299, 299, boxes) plt.subplot(2, 3, i + 1) plt.imshow(result.eval()) plt.show() 结果 延申阅读具体图像增强操作]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>tensorflow图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-058-二值图像分析(寻找最大内接圆)]]></title>
    <url>%2F2019%2F04%2F18%2Fopencv-058%2F</url>
    <content type="text"><![CDATA[知识点对于轮廓来说，有时候我们会需要选择最大内接圆，OpenCV中没有现成的API可以使用，但是我们可以通过点多边形测试巧妙的获取轮廓最大内接圆的半径，从点多边形测试的返回结果我们知道，它返回的是像素距离，而且是当前点距离轮廓最近的距离，当这个点在轮廓内部，其返回的距离是最大值的时候，其实这个距离就是轮廓的最大内接圆的半径，这样我们就巧妙的获得了圆心的位置与半径，然后绘制。 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 二值图像分析(寻找最大内接圆) */int main() &#123; const int r = 100; Mat src = Mat::zeros(Size(4 * r, 4 * r), CV_8U); vector&lt;Point2f&gt; vert(6); vert[0] = Point(3 * r / 2, static_cast&lt;int&gt;(1.34 * r)); vert[1] = Point(1 * r, 2 * r); vert[2] = Point(3 * r / 2, static_cast&lt;int&gt;(2.866 * r)); vert[3] = Point(5 * r / 2, static_cast&lt;int&gt;(2.866 * r)); vert[4] = Point(3 * r, 2 * r); vert[5] = Point(5 * r / 2, static_cast&lt;int&gt;(1.34 * r)); for (int i = 0; i &lt; 6; ++i) &#123; line(src, vert[i], vert[(i + 1) % 6], Scalar(255), 3); &#125; imshow("input", src); // 点多边形测试 vector&lt;vector&lt;Point&gt; &gt; contours; findContours(src, contours, RETR_TREE, CHAIN_APPROX_SIMPLE); Mat raw_dist(src.size(), CV_32F); for (int i = 0; i &lt; src.rows; ++i) &#123; for (int j = 0; j &lt; src.cols; ++j) &#123; raw_dist.at&lt;float&gt;(i, j) = (float) pointPolygonTest(contours[0], Point2f((float) j, (float) i), true); &#125; &#125; // 获取最大内接圆半径 double minval, maxval; Point maxDistPt;// save circle center minMaxLoc(raw_dist, &amp;minval, &amp;maxval, NULL, &amp;maxDistPt); minval = abs(minval); maxval = abs(maxval); Mat drawing = Mat::zeros(src.size(), CV_8UC3); for (int i = 0; i &lt; src.rows; ++i) &#123; for (int j = 0; j &lt; src.cols; ++j) &#123; if (raw_dist.at&lt;float&gt;(i, j) &lt; 0) &#123; drawing.at&lt;Vec3b&gt;(i, j)[0] = (uchar) (255 - abs(raw_dist.at&lt;float&gt;(i, j)) * 255 / minval); &#125; else if (raw_dist.at&lt;float&gt;(i, j) &gt; 0) &#123; drawing.at&lt;Vec3b&gt;(i, j)[2] = (uchar) (255 - raw_dist.at&lt;float&gt;(i, j) * 255 / maxval); &#125; else &#123; drawing.at&lt;Vec3b&gt;(i, j)[0] = 255; drawing.at&lt;Vec3b&gt;(i, j)[1] = 255; drawing.at&lt;Vec3b&gt;(i, j)[2] = 255; &#125; &#125; &#125; // 绘制内接圆 circle(drawing, maxDistPt, (int)maxval, Scalar(255,255,255)); imshow("distance_inscribed_circle", drawing); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152from __future__ import print_functionfrom __future__ import divisionimport cv2 as cvimport numpy as np# Create an imager = 100src = np.zeros((4*r, 4*r), dtype=np.uint8)# Create a sequence of points to make a contourvert = [None]*6vert[0] = (3*r//2, int(1.34*r))vert[1] = (1*r, 2*r)vert[2] = (3*r//2, int(2.866*r))vert[3] = (5*r//2, int(2.866*r))vert[4] = (3*r, 2*r)vert[5] = (5*r//2, int(1.34*r))# Draw it in srcfor i in range(6): cv.line(src, vert[i], vert[(i+1)%6], ( 255 ), 3)# Get the contours_, contours, _ = cv.findContours(src, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)# Calculate the distances to the contourraw_dist = np.empty(src.shape, dtype=np.float32)for i in range(src.shape[0]): for j in range(src.shape[1]): raw_dist[i,j] = cv.pointPolygonTest(contours[0], (j,i), True)# 获取最大值即内接圆半径，中心点坐标minVal, maxVal, _, maxDistPt = cv.minMaxLoc(raw_dist)minVal = abs(minVal)maxVal = abs(maxVal)# Depicting the distances graphicallydrawing = np.zeros((src.shape[0], src.shape[1], 3), dtype=np.uint8)for i in range(src.shape[0]): for j in range(src.shape[1]): if raw_dist[i,j] &lt; 0: drawing[i,j,0] = 255 - abs(raw_dist[i,j]) * 255 / minVal elif raw_dist[i,j] &gt; 0: drawing[i,j,2] = 255 - raw_dist[i,j] * 255 / maxVal else: drawing[i,j,0] = 255 drawing[i,j,1] = 255 drawing[i,j,2] = 255# max inner circlecv.circle(drawing,maxDistPt, np.int(maxVal),(255,255,255), 1, cv.LINE_8, 0)cv.imshow('Source', src)cv.imshow('Distance and inscribed circle', drawing)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>二值图像分析(寻找最大内接圆)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-059-二值图像分析(霍夫直线检测)]]></title>
    <url>%2F2019%2F04%2F18%2Fopencv-059%2F</url>
    <content type="text"><![CDATA[知识点图像霍夫变换是一种特别有用的图像变换，通过把图像的坐标从2D平面坐标系变换到极坐标空间，可以发现原来在平面坐标难以提取的几何特征信息（如：直线、圆等），图像的直线与圆检测就是典型的利用霍夫空间特性实现二值图像几何分析的例子。假设有如下的直线参数方程：r = xcos(theta) + y sin(theta)其中角度theta指r与X轴之间的夹角，r为到直线几何垂直距离。 OpenCV关于霍夫直线检测有两个API，我们首先分享第一个函数，它是提取到直线在霍夫空间得几何特征，然后输出直线得两个极坐标参数。根据这两个参数我们可以组合得到空间坐标直线。该API如下： 12345678910111213141516171819void cv::HoughLines( InputArray image, OutputArray lines, double rho, double theta, int threshold, double srn = 0, double stn = 0, double min_theta = 0, double max_theta = CV_PI)Image 输入图像Lines 输出直线Rho 极坐标r得步长Theta角度步长Threshold累加器阈值Srn、stn多尺度霍夫变换时候需要得参数，经典霍夫变换不需要min_theta 最小角度max_theta最大角度 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 二值图像分析(霍夫直线检测) */int main() &#123; Mat src = imread("../images/sudoku.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 去噪声与二值化 Mat binary; Canny(src, binary, 80, 160); // 标准霍夫直线检测 vector&lt;Vec2f&gt; lines; HoughLines(binary, lines, 1, CV_PI / 180, 150); // 绘制直线 Point pt1, pt2; for (size_t i = 0; i &lt; lines.size(); ++i) &#123; float rho = lines[i][0]; float theta = lines[i][1]; double a = cos(theta), b = sin(theta); double x0 = a * rho, y0 = b * rho; pt1.x = cvRound(x0 + 1000 * (-b)); pt1.y = cvRound(y0 + 1000 * (a)); pt2.x = cvRound(x0 - 1000 * (-b)); pt2.y = cvRound(y0 - 1000 * (a)); line(src, pt1, pt2, Scalar(0, 0, 255), 2, LINE_AA); &#125; imshow("contours", src); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324252627282930313233import cv2 as cvimport numpy as npdef canny_demo(image): t = 80 canny_output = cv.Canny(image, t, t * 2) return canny_outputsrc = cv.imread("D:\code-workspace\Clion-workspace\learnOpencv\images\sudoku.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)binary = canny_demo(src)cv.imshow("binary", binary)lines = cv.HoughLines(binary, 1, np.pi / 180, 150, None, 0, 0)if lines is not None: for i in range(0, len(lines)): rho = lines[i][0][0] theta = lines[i][0][1] a = np.cos(theta) b = np.sin(theta) x0 = a * rho y0 = b * rho pt1 = (int(x0 + 1000 * (-b)), int(y0 + 1000 * (a))) pt2 = (int(x0 - 1000 * (-b)), int(y0 - 1000 * (a))) cv.line(src, pt1, pt2, (0, 0, 255), 3, cv.LINE_AA)cv.imshow("hough line demo", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>二值图像分析(霍夫直线检测)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-060-二值图像分析(霍夫直线检测二)]]></title>
    <url>%2F2019%2F04%2F18%2Fopencv-060%2F</url>
    <content type="text"><![CDATA[知识点OpenCV中还有另外一个霍夫直线检测的API，该API更为常用，它会直接返回直线的空间坐标点，比返回霍夫空间参数更加的直观，容易理解，而且还可以声明线段长度、间隔等参数，非常有用。 API 12345678910111213141516void cv::HoughLinesP( InputArray image, OutputArray lines, double rho, double theta, int threshold, double minLineLength = 0, double maxLineGap = 0 )Image输入二值图像Lines 返回的直线两个点Rho 极坐标r得步长Theta角度步长Threshold累加器阈值minLineLength最小线段长度maxLineGap线段间隔 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 二值图像分析(霍夫直线检测二) */int main() &#123; Mat src = imread("../images/line.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 去噪声与二值化 Mat binary; Canny(src, binary, 80, 160); // 霍夫直线检测 vector&lt;Vec4i&gt; lines; HoughLinesP(binary, lines, 1, CV_PI / 180, 80, 30, 10); // 绘制直线 Mat result = Mat::zeros(src.size(), src.type()); for (size_t i = 0; i &lt; lines.size(); ++i) &#123; line(result, Point(lines[i][0], lines[i][1]), Point(lines[i][2], lines[i][3]), Scalar(0, 0, 255)); &#125; imshow("contours", result); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526import cv2 as cvimport numpy as npdef canny_demo(image): t = 80 canny_output = cv.Canny(image, t, t * 2) cv.imwrite("D:/canny_output.png", canny_output) return canny_outputsrc = cv.imread("D:/images/morph01.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)binary = canny_demo(src)cv.imshow("binary", binary)linesP = cv.HoughLinesP(binary, 1, np.pi / 180, 50, None, 50, 10)if linesP is not None: for i in range(0, len(linesP)): l = linesP[i][0] cv.line(src, (l[0], l[1]), (l[2], l[3]), (255, 0, 0), 1, cv.LINE_AA)cv.imshow("hough line demo", src)cv.imwrite("D:/contours_analysis.png", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>二值图像分析(霍夫直线检测二)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-057-二值图像分析(点多边形测试)]]></title>
    <url>%2F2019%2F04%2F18%2Fopencv-057%2F</url>
    <content type="text"><![CDATA[知识点对于轮廓图像，有时候还需要判断一个点是在轮廓内部还是外部，OpenCV中实现这个功能的API叫做点多边形测试，它可以准确的得到一个点距离多边形的距离，如果点是轮廓点或者属于轮廓多边形上的点，距离是零，如果是多边形内部的点是是正数，如果是负数返回表示点是外部。 API 12345678double cv::pointPolygonTest( InputArray contour, Point2f pt, bool measureDist )Contour轮廓所有点的集合Pt 图像中的任意一点MeasureDist如果是True，则返回每个点到轮廓的距离，如果是False则返回+1，0，-1三个值，其中+1表示点在轮廓内部，0表示点在轮廓上，-1表示点在轮廓外 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 二值图像分析(点多边形测试) */int main() &#123; Mat src = imread("../images/my_mask.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 二值化 Mat dst, gray, binary; cvtColor(src, gray, COLOR_BGR2GRAY); threshold(gray, binary, 0, 255, THRESH_BINARY | THRESH_OTSU); // 轮廓发现与绘制 vector&lt;vector&lt;Point&gt;&gt; contours; vector&lt;Vec4i&gt; hierarchy; findContours(binary, contours, hierarchy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE, Point()); Mat image = Mat::zeros(src.size(), CV_32FC3); // 对轮廓内外的点进行分类 for (int row = 0; row &lt; src.rows; ++row) &#123; for (int col = 0; col &lt; src.cols; ++col) &#123; double dist = pointPolygonTest(contours[0], Point(col, row), true); if (dist == 0) &#123; image.at&lt;Vec3f&gt;(row, col) = Vec3f(255, 255, 255); &#125; else if (dist &gt; 0) &#123; image.at&lt;Vec3f&gt;(row, col) = Vec3f(255 - dist, 0, 0); &#125; else &#123; image.at&lt;Vec3f&gt;(row, col) = Vec3f(0, 0, 255 + dist); &#125; &#125; &#125; convertScaleAbs(image, image); imshow("points", image); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526272829303132import cv2 as cvimport numpy as npsrc = cv.imread("D:/images/my_mask.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)ret, binary = cv.threshold(gray, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)cv.imshow("binary", binary)# 轮廓发现image = np.zeros(src.shape, dtype=np.float32)out, contours, hierarchy = cv.findContours(binary, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)h, w = src.shape[:2]for row in range(h): for col in range(w): dist = cv.pointPolygonTest(contours[0], (col, row), True) if dist == 0: image[row, col] = (255, 255, 255) if dist &gt; 0: image[row, col] = (255-dist, 0, 0) if dist &lt; 0: image[row, col] = (0, 0, 255+dist)dst = cv.convertScaleAbs(image)dst = np.uint8(dst)# 显示cv.imshow("contours_analysis", dst)cv.imwrite("D:/contours_analysis.png", dst)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>二值图像分析(点多边形测试)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下python创建虚拟环境]]></title>
    <url>%2F2019%2F04%2F17%2Flinux_python_env%2F</url>
    <content type="text"><![CDATA[安装 virtualenv1pip install virtualenv 创建工作目录12mkdir myprojectcd myproject 创建一个独立的python运行环境，命名为 venc1234# --no-site-packages 得到了一个不带任何第三方包的“干净”的Python运行环境# 若要继承已有的环境包，不加此参数# --python=python2.7 此处的python版本必须是系统中已经存在的python版本virtualenv --no-site-packages venv --python=python2.7 进入创建的环境1source venv/bin/activate 自动生成和安装 requirements.txt 依赖12345# 生成requirements.txt文件pip freeze &gt; requirements.txt# 安装requirements.txt依赖pip install -r requirements.txt 退出虚拟环境1deactivate 删除虚拟环境1rm -r venv]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>linux</tag>
        <tag>virtualenv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-055-二值图像分析(凸包检测)]]></title>
    <url>%2F2019%2F04%2F17%2Fopencv-055%2F</url>
    <content type="text"><![CDATA[知识点对二值图像进行轮廓分析之后，对获取到的每个轮廓数据，可以构建每个轮廓的凸包，构建完成之后会返回该凸包包含的点集。根据返回的凸包点集可以绘制该轮廓对应的凸包。OpenCV对轮廓提取凸包的API函数如下： 12345678910void cv::convexHull( InputArray points, OutputArray hull, bool clockwise = false, bool returnPoints = true )points参数是输入的轮廓点集hull凸包检测的输出结果，当参数returnPoints为ture的时候返回凸包的顶点坐标是个点集、returnPoints为false的是返回的是一个integer的vector里面是凸包各个顶点在轮廓点集对应的indexclockwise 表示顺时针方向或者逆时针方向returnPoints表示是否返回点集 OpenCV中的凸包寻找算法是基于Graham’s扫描法。OpenCV中还提供了另外一个API函数用来判断一个轮廓是否为凸包，该方法如下： 1234bool cv::isContourConvex( InputArray contour)该方法只有一个输入参数就是轮廓点集。 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 二值图像分析(凸包检测) */int main() &#123; Mat src = imread("../images/hand.jpg"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 二值化 Mat gray, binary; cvtColor(src, gray, COLOR_BGR2GRAY); threshold(gray, binary, 0, 255, THRESH_BINARY | THRESH_OTSU); // 删除干扰块 Mat k = getStructuringElement(MORPH_RECT, Size(3, 3), Point(-1, -1)); morphologyEx(binary, binary, MORPH_OPEN, k); imshow("binary", binary); // 轮廓发现与绘制 vector&lt;vector&lt;Point&gt;&gt; contours; vector&lt;Vec4i&gt; hierarchy; findContours(binary, contours, hierarchy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE, Point()); for (size_t t = 0; t &lt; contours.size(); t++) &#123; vector&lt;Point&gt; hull; convexHull(contours[t], hull); bool isHull = isContourConvex(contours[t]); printf("test convex of the contours %s", isHull?"y":"n"); int len = hull.size(); for (int i = 0; i &lt; len; ++i) &#123; circle(src, hull[i], 4, Scalar(255,0,0), 2); line(src, hull[i%len], hull[(i+1)%len], Scalar(0,0,255),2); &#125; &#125; imshow("contours", src); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526272829303132import cv2 as cvimport numpy as npsrc = cv.imread("D:/images/hand.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)ret, binary = cv.threshold(gray, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)k = cv.getStructuringElement(cv.MORPH_RECT, (3, 3))binary = cv.morphologyEx(binary, cv.MORPH_OPEN, k)cv.imshow("binary", binary)# 轮廓发现out, contours, hierarchy = cv.findContours(binary, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)for c in range(len(contours)): ret = cv.isContourConvex(contours[c]) points = cv.convexHull(contours[c]) total = len(points) for i in range(len(points)): x1, y1 = points[i%total][0] x2, y2 = points[(i+1)%total][0] cv.circle(src, (x1, y1), 4, (255, 0, 0), 2, 8, 0) cv.line(src, (x1, y1), (x2, y2), (0, 0, 255), 2, 8, 0) print(points) print("convex : ", ret)# 显示cv.imshow("contours_analysis", src)cv.imwrite("D:/contours_analysis.png", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>二值图像分析(凸包检测)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-056-二值图像分析(直线拟合与极值点寻找)]]></title>
    <url>%2F2019%2F04%2F17%2Fopencv-056%2F</url>
    <content type="text"><![CDATA[知识点对轮廓进行分析，除了可以对轮廓进行椭圆或者圆的拟合之外，还可以对轮廓点集进行直线拟合，直线拟合的算法有很多，最常见的就是最小二乘法，对于多约束线性方程，最小二乘可以找好直线方程的两个参数、实现直线拟合，OpenCV中直线拟合正是基于最小二乘法实现的。 OpenCV实现直线拟合的API如下: 12345678910111213141516171819void cv::fitLine( InputArray points, OutputArray line, int distType, double param, double reps, double aeps )points表示待拟合的输入点集合line在二维拟合时候输出的是vec4f类型的数据，在三维拟合的时候输出是vec6f的vectordistType表示在拟合时候使用距离计算公式是哪一种，OpenCV支持如下六种方式： DIST_L1 = 1 DIST_L2 = 2 DIST_L12 = 4 DIST_FAIR = 5 DIST_WELSCH = 6 DIST_HUBER = 7param对模型拟合距离计算公式需要参数C，5~7 distType需要参数Creps与aeps是指对拟合结果的精度要求，一般取0.01 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 二值图像分析(直线拟合与极值点寻找) */int main() &#123; Mat src = imread("../images/twolines.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 去噪声与二值化 Mat binary; Canny(src, binary, 80, 160, 3, false); imshow("binary", binary); Mat k = getStructuringElement(MORPH_RECT, Size(3, 3), Point(-1, -1)); dilate(binary, binary, k); // 轮廓发现于绘制 vector&lt;vector&lt;Point&gt;&gt; contours; vector&lt;Vec4i&gt; hierarchy; findContours(binary, contours, hierarchy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE, Point()); for (size_t t = 0; t &lt; contours.size(); ++t) &#123; // 最大外接轮廓 Rect rect = boundingRect(contours[t]); int m = max(rect.width, rect.height); if (m &lt; 30) continue; // 直线拟合 Vec4f oneline; fitLine(contours[t], oneline, DIST_L1, 0, 0.01, 0.01); float dx = oneline[0]; float dy = oneline[1]; float x0 = oneline[2]; float y0 = oneline[3]; // 直线参数斜率k 和 截距b float k = dy / dx; float b = y0 - k * x0; // 寻找轮廓极值点 int minx = 0, miny = 10000; int maxx = 0, maxy = 0; for (int i = 0; i &lt; contours[t].size(); ++i) &#123; Point pt = contours[t][i]; if (miny &gt; pt.y) &#123; miny = pt.y; &#125; if (maxy &lt; pt.y) &#123; maxy = pt.y; &#125; maxx = (maxy - b) / k; minx = (miny - b) / k; line(src, Point(maxx, maxy), Point(minx, miny), Scalar(0,0,255), 2); &#125; &#125; imshow("contours", src); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import cv2 as cvimport numpy as npdef canny_demo(image): t = 80 canny_output = cv.Canny(image, t, t * 2) cv.imwrite("D:/canny_output.png", canny_output) return canny_outputsrc = cv.imread("D:/images/twolines.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)binary = canny_demo(src)k = np.ones((3, 3), dtype=np.uint8)binary = cv.morphologyEx(binary, cv.MORPH_DILATE, k)cv.imshow("binary", binary)# 轮廓发现out, contours, hierarchy = cv.findContours(binary, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)# 直线拟合与极值点寻找for c in range(len(contours)): x, y, w, h = cv.boundingRect(contours[c]) m = max(w, h) if m &lt; 30: continue vx, vy, x0, y0 = cv.fitLine(contours[c], cv.DIST_L1, 0, 0.01, 0.01) k = vy/vx b = y0 - k*x0 maxx = 0 maxy = 0 miny = 100000 minx = 0 for pt in contours[c]: px, py = pt[0] if maxy &lt; py: maxy = py if miny &gt; py: miny = py maxx = (maxy - b) / k minx = (miny - b) / k cv.line(src, (np.int32(maxx), np.int32(maxy)), (np.int32(minx), np.int32(miny)), (0, 0, 255), 2, 8, 0)# 显示cv.imshow("contours_analysis", src)cv.imwrite("D:/contours_analysis.png", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>二值图像分析(直线拟合与极值点寻找)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-054-二值图像分析(对轮廓圆与椭圆拟合)]]></title>
    <url>%2F2019%2F04%2F17%2Fopencv-054%2F</url>
    <content type="text"><![CDATA[知识点有时候我们需要对找到的轮廓点进行拟合，生成一个拟合的圆形或者椭圆，以便我们对轮廓进行更进一步的处理，满足我们对最终轮廓形状的判断，OpenCV对轮廓进行圆形或者椭圆拟合的API函数如下： 123456789RotatedRect cv::fitEllipse( InputArray points)参数points是轮廓点，输出RotatedRect包含下面三个信息- 拟合之后圆或者椭圆的中心位置、- 长轴与短轴的直径- 角度然后我们就可以根据得到拟合信息绘制椭圆、当长轴与短轴相等的时候就是圆。 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 二值图像分析(对轮廓圆与椭圆拟合) */int main() &#123; Mat src = imread("../images/stuff.jpg"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 去噪声与二值化 Mat dst, gray, binary; Canny(src, binary, 80, 160); Mat k = getStructuringElement(MORPH_RECT, Size(3, 3), Point(-1, -1)); dilate(binary, binary, k); // 轮廓发现与绘制 vector&lt;vector&lt;Point&gt;&gt; contours; vector&lt;Vec4i&gt; hierarchy; findContours(binary, contours, hierarchy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE, Point()); for (size_t t = 0; t &lt; contours.size(); t++) &#123; // 寻找适配圆 RotatedRect rrt = fitEllipse(contours[t]); ellipse(src, rrt, Scalar(0,0,255), 2); &#125; imshow("contours", src); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425262728293031323334import cv2 as cvimport numpy as npdef canny_demo(image): t = 80 canny_output = cv.Canny(image, t, t * 2) cv.imshow("canny_output", canny_output) cv.imwrite("D:/canny_output.png", canny_output) return canny_outputsrc = cv.imread("D:/images/stuff.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)binary = canny_demo(src)k = np.ones((3, 3), dtype=np.uint8)binary = cv.morphologyEx(binary, cv.MORPH_DILATE, k)# 轮廓发现out, contours, hierarchy = cv.findContours(binary, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)for c in range(len(contours)): # 椭圆拟合 (cx, cy), (a, b), angle = cv.fitEllipse(contours[c]) # 绘制椭圆 cv.ellipse(src, (np.int32(cx), np.int32(cy)), (np.int32(a/2), np.int32(b/2)), angle, 0, 360, (0, 0, 255), 2, 8, 0)# 显示cv.imshow("contours_analysis", src)cv.imwrite("D:/contours_analysis.png", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>二值图像分析(对轮廓圆与椭圆拟合)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-052-二值图像分析(使用几何矩计算轮廓中心与横纵比过滤)]]></title>
    <url>%2F2019%2F04%2F16%2Fopencv-052%2F</url>
    <content type="text"><![CDATA[知识点对图像二值图像的每个轮廓，可以计算轮廓几何矩，根据几何矩可以计算图像的中心位置，估计得到中心位置可以计算中心矩、然后再根据中心矩可以计算胡矩。 几何矩 API 123456Moments cv::moments( InputArray array, bool binaryImage = false)array是输入的图像轮廓点集合输出图像的几何矩。 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;int main() &#123; Mat src = imread("../images/stuff.jpg"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 去噪声与二值化 Mat dst, gray, binary; Canny(src, binary, 80, 160); Mat k = getStructuringElement(MORPH_RECT, Size(3, 3), Point(-1, -1)); dilate(binary, binary, k); // 轮廓发现与绘制 vector&lt;vector&lt;Point&gt;&gt; contours; vector&lt;Vec4i&gt; hierarchy; findContours(binary, contours, hierarchy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE, Point()); for (size_t t = 0; t &lt; contours.size(); t++) &#123; // 最小外接轮廓 RotatedRect rrt = minAreaRect(contours[t]); float w = rrt.size.width; float h = rrt.size.height; Point2f pts[4]; rrt.points(pts); // 计算横纵比 float ratio = min(w,h) / max(w,h); Point2f cpt = rrt.center; circle(src, cpt, 2, Scalar(255,0,0), 2); if(ratio &gt; 0.9)&#123; circle(src, cpt, 2, Scalar(255,0,0), 2); // 绘制旋转矩形 for (int i = 0; i &lt; 4; i++) &#123; line(src, pts[i % 4], pts[(i + 1) % 4], Scalar(0, 0, 255), 2); &#125; &#125; if(ratio &lt; 0.5)&#123; circle(src, cpt, 2, Scalar(255,0,0), 2); // 绘制旋转矩形 for (int i = 0; i &lt; 4; i++) &#123; line(src, pts[i % 4], pts[(i + 1) % 4], Scalar(0, 255, 0), 2); &#125; &#125; &#125; imshow("contours", src); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import cv2 as cvimport numpy as npdef canny_demo(image): t = 80 canny_output = cv.Canny(image, t, t * 2) cv.imshow("canny_output", canny_output) cv.imwrite("D:/canny_output.png", canny_output) return canny_outputsrc = cv.imread("D:/images/stuff.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)binary = canny_demo(src)k = np.ones((3, 3), dtype=np.uint8)binary = cv.morphologyEx(binary, cv.MORPH_DILATE, k)# 轮廓发现out, contours, hierarchy = cv.findContours(binary, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)for c in range(len(contours)): rect = cv.minAreaRect(contours[c]) cx, cy = rect[0] ww, hh = rect[1] ratio = np.minimum(ww, hh) / np.maximum(ww, hh) print(ratio) mm = cv.moments(contours[c]) m00 = mm['m00'] m10 = mm['m10'] m01 = mm['m01'] cx = np.int(m10 / m00) cy = np.int(m01 / m00) box = cv.boxPoints(rect) box = np.int0(box) if ratio &gt; 0.9: cv.drawContours(src, [box], 0, (0, 0, 255), 2) cv.circle(src, (np.int32(cx), np.int32(cy)), 2, (255, 0, 0), 2, 8, 0) if ratio &lt; 0.5: cv.drawContours(src, [box], 0, (255, 0, 255), 2) cv.circle(src, (np.int32(cx), np.int32(cy)), 2, (0, 0, 255), 2, 8, 0)# 显示cv.imshow("contours_analysis", src)cv.imwrite("D:/contours_analysis.png", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>使用几何矩计算轮廓中心</tag>
        <tag>横纵比过滤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-053-二值图像分析(使用Hu矩实现轮廓匹配)]]></title>
    <url>%2F2019%2F04%2F16%2Fopencv-053%2F</url>
    <content type="text"><![CDATA[知识点对图像二值图像的每个轮廓，可以计算轮廓几何矩，根据几何矩可以计算图像的中心位置，估计得到中心位置可以计算中心矩、然后再根据中心矩可以计算胡矩。 OpenCV中可以通过如下的API一次计算出上述三种矩，API如下： 123456Moments cv::moments( InputArray array, bool binaryImage = false)array是输入的图像轮廓点集合输出的图像几何矩 根据几何矩输出结果可以计算胡矩，胡矩计算的API如下： 123456void cv::HuMoments( const Moments &amp; moments, double hu[7] )moments参数表示输入的图像矩hu[7]表示输出的胡矩七个值 然后我们可以使用hu矩作为输入，对轮廓进行匹配，进行轮廓外形匹配的API如下： 123456789101112double cv::matchShapes( InputArray contour1, InputArray contour2, int method, double parameter)contour1第一个轮廓点集合，或者灰度图像contour2第二个轮廓点集合，或者灰度图像method表示比较方法，最常见有CONTOURS_MATCH_I1CONTOURS_MATCH_I2CONTOURS_MATCH_I3 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;void contours_info(Mat &amp;image, vector&lt;vector&lt;Point&gt;&gt; &amp;pts);int main() &#123; Mat src = imread("../images/abc.png"); Mat src2 = imread("../images/a5.png"); if (src.empty() || src2.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); imshow("input1", src2); // 轮廓发现 vector&lt;vector&lt;Point&gt;&gt; contours1; vector&lt;vector&lt;Point&gt;&gt; contours2; contours_info(src, contours1); contours_info(src2, contours2); // 几何矩计算与hu矩计算 Moments mm2 = moments(contours2[0]); Mat hu2; HuMoments(mm2, hu2); // 轮廓匹配 for (size_t t = 0; t &lt; contours1.size(); ++t) &#123; Moments mm = moments(contours1[t]); Mat hu; HuMoments(mm, hu); double dist = matchShapes(hu, hu2, CONTOURS_MATCH_I1, 0); if (dist &lt; 1) &#123; drawContours(src, contours1, t, Scalar(0,0,255), 2); &#125; &#125; imshow("match_result", src); waitKey(0); return 0;&#125;void contours_info(Mat &amp;image, vector&lt;vector&lt;Point&gt;&gt; &amp;pts) &#123; Mat gray, binary; vector&lt;Vec4i&gt; hierarchy; cvtColor(image, gray, COLOR_BGR2GRAY); threshold(gray, binary, 0, 255, THRESH_BINARY | THRESH_OTSU); findContours(binary, pts, hierarchy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE);&#125; 123456789101112131415161718192021222324252627282930313233343536373839import cv2 as cvimport numpy as npdef contours_info(image): gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY) ret, binary = cv.threshold(gray, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU) out, contours, hierarchy = cv.findContours(binary, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE) return contourssrc = cv.imread("D:/images/abc.png")cv.namedWindow("input1", cv.WINDOW_AUTOSIZE)cv.imshow("input1", src)src2 = cv.imread("D:/images/a5.png")cv.imshow("input2", src2)# 轮廓发现contours1 = contours_info(src)contours2 = contours_info(src2)# 几何矩计算与hu矩计算mm2 = cv.moments(contours2[0])hum2 = cv.HuMoments(mm2)# 轮廓匹配for c in range(len(contours1)): mm = cv.moments(contours1[c]) hum = cv.HuMoments(mm) dist = cv.matchShapes(hum, hum2, cv.CONTOURS_MATCH_I1, 0) if dist &lt; 1: cv.drawContours(src, contours1, c, (0, 0, 255), 2, 8) print("dist %f"%(dist))# 显示cv.imshow("contours_analysis", src)cv.imwrite("D:/contours_analysis.png", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>使用Hu矩实现轮廓匹配</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-051-二值图像分析(使用轮廓逼近)]]></title>
    <url>%2F2019%2F04%2F16%2Fopencv-051%2F</url>
    <content type="text"><![CDATA[知识点对图像二值图像的每个轮廓，可以使用轮廓逼近，逼近每个轮廓的真实几何形状，从而通过轮廓逼近的输出结果判断一个对象是什么形状。 API 1234567891011void cv::approxPolyDP( InputArray curve, OutputArray approxCurve, double epsilon, bool closed )其中Curve表示轮廓曲线approxCurve 表示轮廓逼近输出的顶点数目epsilon 轮廓逼近的顶点距离真实轮廓曲线的最大距离，该值越小表示越逼近真实轮廓close 表示是否为闭合区域 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;int main() &#123; Mat src = imread("../images/contours.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 去噪声与二值化 Mat gray, binary; cvtColor(src, gray, COLOR_BGR2GRAY); threshold(gray, binary, 0, 255, THRESH_BINARY | THRESH_OTSU); // 轮廓发现与绘制 vector&lt;vector&lt;Point&gt;&gt; contours; vector&lt;Vec4i&gt; hierarchy; findContours(binary, contours, hierarchy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE, Point()); Scalar color = Scalar(255, 0, 0); for (size_t t = 0; t &lt; contours.size(); ++t) &#123; RotatedRect rrt = minAreaRect(contours[t]); Point2f cpt = rrt.center; circle(src, cpt, 2, Scalar(0, 255, 0), 2); Mat result; approxPolyDP(contours[t], result, 4, true); if (result.rows == 6) &#123; putText(src, "poly", cpt, FONT_HERSHEY_SIMPLEX, .7, color); &#125; if (result.rows == 3) &#123; putText(src, "triangle", cpt, FONT_HERSHEY_SIMPLEX, .7, color); &#125; if (result.rows == 4) &#123; putText(src, "rectangle", cpt, FONT_HERSHEY_SIMPLEX, .7, color); &#125; if(result.rows &gt; 10) &#123; putText(src, "circle", cpt, FONT_HERSHEY_SIMPLEX, .7, color); &#125; &#125; imshow("contours", src); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324252627282930313233343536import cv2 as cvimport numpy as npsrc = cv.imread("D:/images/contours.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)ret, binary = cv.threshold(gray, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)# 轮廓发现out, contours, hierarchy = cv.findContours(binary, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)for c in range(len(contours)): rect = cv.minAreaRect(contours[c]) cx, cy = rect[0] result = cv.approxPolyDP(contours[c], 4, True) vertexes = result.shape[0] if vertexes == 3: cv.putText(src, "triangle", (np.int32(cx), np.int32(cy)), cv.FONT_HERSHEY_SIMPLEX, .7, (0, 0, 255), 2, 8); if vertexes == 4: cv.putText(src, "rectangle", (np.int32(cx), np.int32(cy)), cv.FONT_HERSHEY_SIMPLEX, .7, (0, 0, 255), 2, 8); if vertexes == 6: cv.putText(src, "poly", (np.int32(cx), np.int32(cy)), cv.FONT_HERSHEY_SIMPLEX, .7, (0, 0, 255), 2, 8); if vertexes &gt; 10: cv.putText(src, "circle", (np.int32(cx), np.int32(cy)), cv.FONT_HERSHEY_SIMPLEX, .7, (0, 0, 255), 2, 8); print(vertexes)# 显示cv.imshow("contours_analysis", src)cv.imwrite("D:/contours_analysis.png", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>二值图像分析(使用轮廓逼近)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用keras实现一个多输入多输出的网络]]></title>
    <url>%2F2019%2F04%2F16%2Fkeras_multiple_input_output%2F</url>
    <content type="text"><![CDATA[结构图 代码12345678910111213141516171819import kerasfrom keras.layers import Input, Densefrom keras.models import Modelinput1 = Input(shape=(784,), name="input1")input2 = Input(shape=(10,), name="input2")hidden = Dense(1, activation='relu')(input1)output1 = Dense(10, activation='relu', name="output1")(hidden)hidden_input2 = keras.layers.concatenate([hidden, input2])output2 = Dense(10, activation='relu', name="output2")(hidden_input2)model = Model(inputs=[input1, input2], outputs=[output1, output2])model.compile(loss=&#123;'output1': ... , 'output2': ...&#125;, optimizer=..., loss_weights= [1, 0.4], metrics=['accuracy'])model.fit([train_X1, train_X2], [train_y1, train_y2], batch_size=None, epochs=1, validation_data=([test_X1, test_X2], [test_y1, test_y2]))]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>keras</tag>
        <tag>多输入多输出的网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用keras实现Inception结构]]></title>
    <url>%2F2019%2F04%2F16%2Fkeras_inception%2F</url>
    <content type="text"><![CDATA[Inception结构 代码12345678910111213141516171819202122232425262728import kerasfrom keras.layers import Input, Conv2D, MaxPooling2Dfrom keras.models import Model# 定义输入图像尺寸inputs = Input(shape=(256, 256, 3))# 定义第一个分支tower_1 = Conv2D(64, (1, 1), padding='same', activation='relu')(inputs)# 定义第二个分支tower_2 = Conv2D(64, (1, 1), padding='same', activation='relu')(inputs)tower_2 = Conv2D(64, (3, 3), padding='same', activation='relu')(tower_2)# 定义第三个分支tower_3 = Conv2D(64, (1, 1), padding='same', activation='relu')(inputs)tower_3 = Conv2D(64, (5, 5), padding='same', activation='relu')(tower_3)# 定义第四个分支tower_4 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(inputs)tower_4 = Conv2D(64, (1, 1), padding='same', activation='relu')(tower_4)output = keras.layers.concatenate([tower_1, tower_2, tower_3, tower_4], axis=1)model = Model(inputs=inputs, outputs=output)model.compile(...)model.fit(...)]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>keras</tag>
        <tag>Inception</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-048-二值图像分析之轮廓发现]]></title>
    <url>%2F2019%2F04%2F15%2Fopencv-048%2F</url>
    <content type="text"><![CDATA[知识点图像连通组件分析，可以得到二值图像的每个连通组件，但是我们还无法得知各个组件之间的层次关系与几何拓扑关系，如果我们需要进一步分析图像轮廓拓扑信息就可以通过OpenCV的轮廓发现API获取二值图像的轮廓拓扑信息. 轮廓发现API 1234567891011121314151617void cv::findContours( InputOutputArray image, OutputArrayOfArrays contours, OutputArray hierarchy, int mode, int method, Point offset = Point() )各个参数详解如下：Image表示输入图像，必须是二值图像，二值图像可以threshold输出、Canny输出、inRange输出、自适应阈值输出等。Contours获取的轮廓，每个轮廓是一系列的点集合Hierarchy轮廓的层次信息，每个轮廓有四个相关信息，分别是同层下一个、前一个、第一个子节点、父节点mode 表示轮廓寻找时候的拓扑结构返回-RETR_EXTERNAL表示只返回最外层轮廓-RETR_TREE表示返回轮廓树结构Method表示轮廓点集合取得是基于什么算法，常见的是基于CHAIN_APPROX_SIMPLE链式编码方法 绘制轮廓API 12345678910111213void cv::drawContours( InputOutputArray image, InputArrayOfArrays contours, int contourIdx, const Scalar &amp; color, int thickness = 1, int lineType = LINE_8, InputArray hierarchy = noArray(), int maxLevel = INT_MAX, Point offset = Point() )当thickness为正数的时候表示绘制该轮廓当thickness为-1表示填充该轮廓 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 二值图像分析之轮廓发现 */int main() &#123; Mat src1 = imread("../images/master.jpg"); Mat src2 = imread("../images/coins.jpg"); if (src1.empty() || src2.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; //imshow("input_1", src1); imshow("input_2", src2); // 去噪声与二值化 Mat dst, gray, binary; GaussianBlur(src2, dst, Size(3, 3), 0, 0); cvtColor(dst, gray, COLOR_BGR2GRAY); threshold(gray, binary, 0, 255, THRESH_BINARY | THRESH_OTSU); imshow("binary", binary); // 轮廓发现与绘制 vector&lt;vector&lt;Point&gt;&gt; contours; vector&lt;Vec4i&gt; hierarchy; findContours(binary, contours, hierarchy, RETR_TREE, CHAIN_APPROX_SIMPLE, Point()); for (auto t = 0; t &lt; contours.size(); ++t) &#123; drawContours(src2, contours, t, Scalar(0,0,255), 2, 8); &#125; imshow("contours", src2); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526272829303132333435import cv2 as cvimport numpy as npdef threshold_demo(image): # 去噪声+二值化 dst = cv.GaussianBlur(image,(3, 3), 0) gray = cv.cvtColor(dst, cv.COLOR_BGR2GRAY) ret, binary = cv.threshold(gray, 0, 255, cv.THRESH_OTSU | cv.THRESH_BINARY) cv.imshow("binary", binary) return binarydef canny_demo(image): t = 100 canny_output = cv.Canny(image, t, t * 2) cv.imshow("canny_output", canny_output) return canny_outputsrc = cv.imread("D:/images/yuan_test.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)binary = threshold_demo(src)# 轮廓发现out, contours, hierarchy = cv.findContours(binary, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)for c in range(len(contours)): cv.drawContours(src, contours, c, (0, 0, 255), 2, 8)# 显示cv.imshow("contours-demo", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>二值图像分析之轮廓发现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-050-二值图像分析(矩形面积与弧长)]]></title>
    <url>%2F2019%2F04%2F15%2Fopencv-050%2F</url>
    <content type="text"><![CDATA[知识点对图像二值图像的每个轮廓，我们可以计算轮廓的弧长与面积，根据轮廓的面积与弧长可以实现对不同大小对象的过滤，寻找到我们感兴趣的roi区域，这个也是图像二值分析的任务之一。 OpenCV对轮廓点集计算面积的API函数如下: 12345678double cv::contourArea( InputArray contour, bool oriented = false)计算轮廓的面积，其原理是基于格林公式。参数contour表示输入的轮廓点集参数oriented默认是false返回的面积是正数，如果方向参数为true表示会根据是顺时针或者逆时针方向返回正值或者负值面积。 OpenCV对轮廓点集计算弧长的API函数如下: 1234567double cv::arcLength( InputArray curve, bool closed )参数curve表示输入的轮廓点集参数closed默认表示是否闭合区域 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 二值图像分析(矩形面积与弧长) */int main() &#123; Mat src = imread("../images/zhifang_ball.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 提取边缘 Mat binary; Canny(src, binary, 80, 160); imshow("binary", binary); // 膨胀 Mat k = getStructuringElement(MORPH_RECT, Size(3, 3), Point(-1, -1)); dilate(binary, binary, k); // 轮廓发现于绘制 vector&lt;vector&lt;Point&gt;&gt; contours; vector&lt;Vec4i&gt; hierarchy; findContours(binary, contours, hierarchy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE, Point()); for (size_t t = 0; t &lt; contours.size(); ++t) &#123; // 最大外接矩形 // Rect rect = boundingRect(contours[t]); // rectangle(src, rect, Scalar(0, 0, 255)); // 面积与弧长过滤 double area = contourArea(contours[t]); double curvelen = arcLength(contours[t], true); if (area &lt; 100 || curvelen &lt; 100)&#123; continue; &#125; // 最小外接矩形 RotatedRect rrt = minAreaRect(contours[t]); Point2f pts[4]; rrt.points(pts); // 绘制旋转矩形与中心位置 for (int i = 0; i &lt; 4; ++i) &#123; line(src, pts[i % 4], pts[(i + 1) % 4], Scalar(0, 255, 0), 2); &#125; Point2f cpt = rrt.center; circle(src, cpt, 2, Scalar(255, 0, 0), 2); &#125; imshow("contours", src); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142import cv2 as cvimport numpy as npdef canny_demo(image): t = 80 canny_output = cv.Canny(image, t, t * 2) cv.imshow("canny_output", canny_output) cv.imwrite("D:/canny_output.png", canny_output) return canny_outputsrc = cv.imread("D:/images/zhifang_ball.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)binary = canny_demo(src)k = np.ones((3, 3), dtype=np.uint8)binary = cv.morphologyEx(binary, cv.MORPH_DILATE, k)# 轮廓发现out, contours, hierarchy = cv.findContours(binary, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)for c in range(len(contours)): # x, y, w, h = cv.boundingRect(contours[c]); # cv.drawContours(src, contours, c, (0, 0, 255), 2, 8) # cv.rectangle(src, (x, y), (x+w, y+h), (0, 0, 255), 1, 8, 0); area = cv.contourArea(contours[c]) arclen = cv.arcLength(contours[c], True) if area &lt; 100 or arclen &lt; 100: continue rect = cv.minAreaRect(contours[c]) cx, cy = rect[0] box = cv.boxPoints(rect) box = np.int0(box) cv.drawContours(src,[box],0,(0,0,255),2) cv.circle(src, (np.int32(cx), np.int32(cy)), 2, (255, 0, 0), 2, 8, 0)# 显示cv.imshow("contours_analysis", src)cv.imwrite("D:/contours_analysis.png", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>二值图像分析(矩形面积与弧长)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-049-二值图像分析(轮廓外接矩形)]]></title>
    <url>%2F2019%2F04%2F15%2Fopencv-049%2F</url>
    <content type="text"><![CDATA[知识点对图像二值图像的每个轮廓，OpenCV都提供了API可以求取轮廓的外接矩形. 求取轮廓外接矩形有两种方式，一种是可以基于像素的最大外接轮廓矩形，API解释如下： 123456Rect cv::boundingRect( InputArray points)输入参数points可以一系列点的集合，对轮廓来说就是该轮廓的点集返回结果是一个矩形，x, y, w, h 另一种是可以基于像素的最大外接轮廓矩形，API解释如下： 123456789RotatedRect cv::minAreaRect( InputArray points)输入参数points可以一系列点的集合，对轮廓来说就是该轮廓的点集返回结果是一个旋转矩形，包含下面的信息：-矩形中心位置-矩形的宽高-旋转角度 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;int main() &#123; Mat src = imread("../images/stuff.jpg"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 提取边缘 Mat binary; Canny(src, binary, 80, 160); imshow("binary", binary); // 膨胀 Mat k = getStructuringElement(MORPH_RECT, Size(3, 3), Point(-1, -1)); dilate(binary, binary, k); // 轮廓发现于绘制 vector&lt;vector&lt;Point&gt;&gt; contours; vector&lt;Vec4i&gt; hierarchy; findContours(binary, contours, hierarchy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE, Point()); for (size_t t = 0; t &lt; contours.size(); ++t) &#123; // 最大外接矩形 Rect rect = boundingRect(contours[t]); rectangle(src, rect, Scalar(0, 0, 255)); // 最小外接矩形 RotatedRect rrt = minAreaRect(contours[t]); Point2f pts[4]; rrt.points(pts); // 绘制旋转矩形与中心位置 for (int i = 0; i &lt; 4; ++i) &#123; line(src, pts[i % 4], pts[(i + 1) % 4], Scalar(0, 255, 0), 2); &#125; Point2f cpt = rrt.center; circle(src, cpt, 2, Scalar(255, 0, 0), 2); &#125; imshow("contours", src); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526272829303132333435363738import cv2 as cvimport numpy as npdef canny_demo(image): t = 80 canny_output = cv.Canny(image, t, t * 2) cv.imshow("canny_output", canny_output) cv.imwrite("D:/canny_output.png", canny_output) return canny_outputsrc = cv.imread("D:/images/stuff.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)binary = canny_demo(src)k = np.ones((3, 3), dtype=np.uint8)binary = cv.morphologyEx(binary, cv.MORPH_DILATE, k)# 轮廓发现out, contours, hierarchy = cv.findContours(binary, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)for c in range(len(contours)): x, y, w, h = cv.boundingRect(contours[c]); cv.drawContours(src, contours, c, (0, 0, 255), 2, 8) cv.rectangle(src, (x, y), (x+w, y+h), (0, 0, 255), 1, 8, 0); rect = cv.minAreaRect(contours[c]) cx, cy = rect[0] box = cv.boxPoints(rect) box = np.int0(box) cv.drawContours(src,[box],0,(0,0,255),2) cv.circle(src, (np.int32(cx), np.int32(cy)), 2, (255, 0, 0), 2, 8, 0)# 显示cv.imshow("contours_analysis", src)cv.imwrite("D:/contours_analysis.png", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>二值图像分析(轮廓外接矩形)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-047-二值图像连通组件状态统计]]></title>
    <url>%2F2019%2F04%2F15%2Fopencv-047%2F</url>
    <content type="text"><![CDATA[知识点OpenCV中的连通组件标记算法有两个相关的API： 123456789101112131415161718192021222324252627282930313233343536373839一个是不带统计信息的API int cv::connectedComponents( InputArray image, // 输入二值图像，黑色背景 OutputArray labels, // 输出的标记图像，背景index=0 int connectivity = 8, // 连通域，默认是8连通 int ltype = CV_32S // 输出的labels类型，默认是CV_32S)另外一个是会输出连通组件统计信息的相关API，int cv::connectedComponentsWithStats( InputArray image, OutputArray labels, OutputArray stats, OutputArray centroids, int connectivity, int ltype, int ccltype )相关的统计信息包括在输出stats的对象中，每个连通组件有一个这样的输出结构体。CC_STAT_LEFT Python: cv.CC_STAT_LEFT连通组件外接矩形左上角坐标的X位置信息CC_STAT_TOP Python: cv.CC_STAT_TOP连通组件外接左上角坐标的Y位置信息CC_STAT_WIDTH Python: cv.CC_STAT_WIDTH连通组件外接矩形宽度CC_STAT_HEIGHT Python: cv.CC_STAT_HEIGHT连通组件外接矩形高度CC_STAT_AREA Python: cv.CC_STAT_AREA连通组件的面积大小，基于像素多少统计。Centroids输出的是每个连通组件的中心位置坐标(x, y) 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;RNG rng(12345);void componentwithstats_demo(Mat &amp;image);/* * 二值图像连通组件状态统计 */int main() &#123; Mat src = imread("../images/rice.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); componentwithstats_demo(src); waitKey(0); return 0;&#125;void componentwithstats_demo(Mat &amp;image) &#123; // extract labels Mat gray, binary; GaussianBlur(image, image, Size(3, 3), 0); cvtColor(image, gray, COLOR_BGR2GRAY); threshold(gray, binary, 0, 255, THRESH_BINARY | THRESH_OTSU); Mat labels = Mat::zeros(image.size(), CV_32S); Mat stats, centroids; int num_labels = connectedComponentsWithStats(binary, labels, stats, centroids, 8, 4); cout &lt;&lt; "total labels : " &lt;&lt; num_labels - 1 &lt;&lt; endl; vector&lt;Vec3b&gt; colors(num_labels); // 背景颜色 colors[0] = Vec3b(0, 0, 0); // 目标颜色 for (int i = 1; i &lt; num_labels; ++i) &#123; colors[i] = Vec3b(rng.uniform(0, 256), rng.uniform(0, 256), rng.uniform(0, 256)); &#125; // 抽取统计信息 Mat dst = image.clone(); for (int i = 1; i &lt; num_labels; ++i) &#123; // 中心位置 int cx = centroids.at&lt;double&gt;(i, 0); int cy = centroids.at&lt;double&gt;(i, 1); // 统计信息 int x = stats.at&lt;int&gt;(i, CC_STAT_LEFT); int y = stats.at&lt;int&gt;(i, CC_STAT_TOP); int w = stats.at&lt;int&gt;(i, CC_STAT_WIDTH); int h = stats.at&lt;int&gt;(i, CC_STAT_HEIGHT); int area = stats.at&lt;int&gt;(i, CC_STAT_AREA); // 中心位置绘制 circle(dst, Point(cx, cy), 2, Scalar(0, 255, 0), 2); // 外接矩形 Rect rect(x, y, w, h); rectangle(dst, rect, colors[i]); putText(dst, format("num:%d", i), Point(x, y), FONT_HERSHEY_SIMPLEX, .5, Scalar(0, 0, 255), 1); printf("num : %d, rice area : %d\n", i, area); &#125; imshow("result", dst);&#125; 12345678910111213141516171819202122232425262728293031323334353637import cv2 as cvimport numpy as npdef connected_components_stats_demo(src): src = cv.GaussianBlur(src, (3, 3), 0) gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY) ret, binary = cv.threshold(gray, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU) cv.imshow("binary", binary) num_labels, labels, stats, centers = cv.connectedComponentsWithStats(binary, connectivity=8, ltype=cv.CV_32S) colors = [] for i in range(num_labels): b = np.random.randint(0, 256) g = np.random.randint(0, 256) r = np.random.randint(0, 256) colors.append((b, g, r)) colors[0] = (0, 0, 0) image = np.copy(src) for t in range(1, num_labels, 1): x, y, w, h, area = stats[t] cx, cy = centers[t] cv.circle(image, (np.int32(cx), np.int32(cy)), 2, (0, 255, 0), 2, 8, 0) cv.rectangle(image, (x, y), (x+w, y+h), colors[t], 1, 8, 0) cv.putText(image, "num:" + str(t), (x, y), cv.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 1); print("label index %d, area of the label : %d"%(t, area)) cv.imshow("colored labels", image) cv.imwrite("D:/labels.png", image) print("total rice : ", num_labels - 1)input = cv.imread("D:/images/rice.png")connected_components_stats_demo(input)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>二值图像连通组件状态统计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-046-二值图像的联通组件寻找]]></title>
    <url>%2F2019%2F04%2F14%2Fopencv-046%2F</url>
    <content type="text"><![CDATA[知识点 连通组件标记算法介绍连接组件标记算法(connected component labeling algorithm)是图像分析中最常用的算法之一，算法的实质是扫描二值图像的每个像素点，对于像素值相同的而且相互连通分为相同的组(group),最终得到图像中所有的像素连通组件。扫描的方式可以是从上到下，从左到右，对于一幅有N个像素的图像来说，最大连通组件个数为N/2。扫描是基于每个像素单位，OpenCV中进行连通组件扫码调用的时候必须保证背景像素是黑色、前景像素是白色。最常见的连通组件扫码有如下两类算法： 一步法，基于图的搜索算法 两步法、基于扫描与等价类合并算法 API 123456int cv::connectedComponents( InputArray image, // 输入二值图像，黑色背景 OutputArray labels, // 输出的标记图像，背景index=0 int connectivity = 8, // 连通域，默认是8连通 int ltype = CV_32S // 输出的labels类型，默认是CV_32S) 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;RNG rng(12345);void connected_component_demo(Mat &amp;image);/* * 二值图像的连通组件标记 */int main() &#123; Mat src = imread("../images/rice.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); connected_component_demo(src); waitKey(0); return 0;&#125;void connected_component_demo(Mat &amp;image) &#123; // extract labels Mat gray, binary; GaussianBlur(image, image, Size(3, 3), 0); cvtColor(image, gray, COLOR_BGR2GRAY); threshold(gray, binary, 0, 255, THRESH_BINARY | THRESH_OTSU); Mat labels = Mat::zeros(image.size(), CV_32S); int num_labels = connectedComponents(binary, labels, 8, CV_32S); cout &lt;&lt; "total labels : " &lt;&lt; num_labels - 1 &lt;&lt; endl; vector&lt;Vec3b&gt; colors(num_labels); // 背景颜色 colors[0] = Vec3b(0, 0, 0); // 目标颜色 for (int i = 1; i &lt; num_labels; ++i) &#123; colors[i] = Vec3b(rng.uniform(0, 256), rng.uniform(0, 256), rng.uniform(0, 256)); &#125; // 给结果着色 Mat dst = Mat::zeros(image.size(), image.type()); for (int row = 0; row &lt; image.rows; ++row) &#123; for (int col = 0; col &lt; image.cols; ++col) &#123; int label = labels.at&lt;int&gt;(row, col); if (label == 0) continue; dst.at&lt;Vec3b&gt;(row, col) = colors[label]; &#125; &#125; imshow("result", dst);&#125; 12345678910111213141516171819202122232425262728293031323334353637import cv2 as cvimport numpy as npdef connected_components_demo(src): src = cv.GaussianBlur(src, (3, 3), 0) gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY) ret, binary = cv.threshold(gray, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU) cv.imshow("binary", binary) output = cv.connectedComponents(binary, connectivity=8, ltype=cv.CV_32S) num_labels = output[0] labels = output[1] colors = [] for i in range(num_labels): b = np.random.randint(0, 256) g = np.random.randint(0, 256) r = np.random.randint(0, 256) colors.append((b, g, r)) colors[0] = (0, 0, 0) h, w = gray.shape image = np.zeros((h, w, 3), dtype=np.uint8) for row in range(h): for col in range(w): image[row, col] = colors[labels[row, col]] cv.imshow("colored labels", image) cv.imwrite("D:/labels.png", image) print("total rice : ", num_labels - 1)src = cv.imread("D:/images/rice.png")h, w = src.shape[:2]connected_components_demo(src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>连通组件标记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-045-图像二值化与去噪]]></title>
    <url>%2F2019%2F04%2F14%2Fopencv-045%2F</url>
    <content type="text"><![CDATA[知识点对于一张需要二值化的图像，我们有两种选择：选择一直接对输入图像转换为灰度图像，然后二值化选择二首先对输入图像进行降噪，去除噪声干扰，然后再二值化 在进行去噪声的时候，可以选择的有： 均值模糊去噪声 高斯模糊去噪声 双边/均值迁移模糊去噪声 非局部均值去噪声 下面以三种方式进行实验， 第一张图是输入图像直接转换为二值图像 第二张图是输入图像先高斯模糊去噪声，然后二值化图像 第三张图是输入图像先均值迁移去噪声，然后二值化的图像 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像二值化与去噪 */int main() &#123; Mat src = imread("../images/coins.jpg"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); Mat gray, blurred, binary; // 直接二值化 cvtColor(src, gray, COLOR_BGR2GRAY); threshold(gray, binary, 0, 255, THRESH_BINARY | THRESH_OTSU); imshow("binary_direct", binary); // 先高斯模糊，再二值化 GaussianBlur(src, blurred, Size(3,3), 0, 0); cvtColor(blurred, gray, COLOR_BGR2GRAY); threshold(gray, binary, 0, 255, THRESH_BINARY | THRESH_OTSU); imshow("binary_gaussian", binary); // 先均值迁移模糊，再二值化 pyrMeanShiftFiltering(src, blurred, 10, 100); cvtColor(blurred, gray, COLOR_BGR2GRAY); threshold(gray, binary, 0, 255, THRESH_BINARY | THRESH_OTSU); imshow("binary_pyrmean", binary); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526272829303132333435363738import cv2 as cvimport numpy as npdef method_1(image): gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY) t, binary = cv.threshold(gray, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU) return binarydef method_2(image): blurred = cv.GaussianBlur(image, (3, 3), 0) gray = cv.cvtColor(blurred, cv.COLOR_BGR2GRAY) t, binary = cv.threshold(gray, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU) return binarydef method_3(image): blurred = cv.pyrMeanShiftFiltering(image, 10, 100) gray = cv.cvtColor(blurred, cv.COLOR_BGR2GRAY) t, binary = cv.threshold(gray, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU) return binarysrc = cv.imread("D:/images/coins.jpg")h, w = src.shape[:2]ret = method_3(src)result = np.zeros([h, w*2, 3], dtype=src.dtype)result[0:h,0:w,:] = srcresult[0:h,w:2*w,:] = cv.cvtColor(ret, cv.COLOR_GRAY2BGR)cv.putText(result, "input", (10, 30), cv.FONT_ITALIC, 1.0, (0, 0, 255), 2)cv.putText(result, "binary", (w+10, 30), cv.FONT_ITALIC, 1.0, (0, 0, 255), 2)cv.imshow("result", result)cv.imwrite("D:/binary_result.png", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像二值化与去噪</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-044-图像二化自适应阈值算法]]></title>
    <url>%2F2019%2F04%2F14%2Fopencv-044%2F</url>
    <content type="text"><![CDATA[知识点OpenCV中的自适应阈值算法主要是基于均值实现，根据计算均值的方法不同分为盒子模糊均值与高斯模糊均值，然后使用原图减去均值图像，得到的差值图像进行自适应分割. 1234567891011121314151617181920void cv::adaptiveThreshold( InputArray src, OutputArray dst, double maxValue, int adaptiveMethod, int thresholdType, int blockSize, double C)其中blockSize取值必须是奇数，C取值在10左右自适应方法类型：ADAPTIVE_THRESH_GAUSSIAN_C = 1ADAPTIVE_THRESH_MEAN_C = 0当阈值操作类型thresholdType为：THRESH_BINARY二值图像 = 原图 – 均值图像 &gt; -C ? 255 : 0当阈值操作类型thresholdType为：THRESH_BINARY_INV二值图像 = 原图 – 均s值图像 &gt; -C ? 0 : 255 代码（c++,python）1234567891011121314151617181920212223242526272829#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 自动阈值寻找与二值化 */int main() &#123; Mat src = imread("../images/text1.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 自动阈值寻找与二值化 Mat gray, binary1, binary2; cvtColor(src, gray, COLOR_BGR2GRAY); // int T = mean(src)[0]; // threshold(gray, binary1, T, 255, THRESH_BINARY); adaptiveThreshold(gray, binary2, 255, ADAPTIVE_THRESH_GAUSSIAN_C, THRESH_BINARY, 25, 10); // imshow("binary_T=mean", binary1); imshow("binary_adaptive", binary2); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425262728293031import cv2 as cvimport numpy as np## THRESH_BINARY = 0# THRESH_BINARY_INV = 1# THRESH_TRUNC = 2# THRESH_TOZERO = 3# THRESH_TOZERO_INV = 4#src = cv.imread("D:/images/text1.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)h, w = src.shape[:2]# 自动阈值分割 TRIANGLEgray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)binary = cv.adaptiveThreshold(gray, 255, cv.ADAPTIVE_THRESH_MEAN_C, cv.THRESH_BINARY, 25, 10)cv.imshow("binary", binary)result = np.zeros([h, w*2, 3], dtype=src.dtype)result[0:h,0:w,:] = srcresult[0:h,w:2*w,:] = cv.cvtColor(binary, cv.COLOR_GRAY2BGR)cv.putText(result, "input", (10, 30), cv.FONT_ITALIC, 1.0, (0, 0, 255), 2)cv.putText(result, "adaptive threshold", (w+10, 30), cv.FONT_ITALIC, 1.0, (0, 0, 255), 2)cv.imshow("result", result)cv.imwrite("D:/binary_result.png", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像二化自适应阈值算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-043-图像二值寻找算法(TRIANGLE)]]></title>
    <url>%2F2019%2F04%2F14%2Fopencv-043%2F</url>
    <content type="text"><![CDATA[知识点OpenCV中支持的有OTSU与Triangle两种直方图阈值寻找算法。OTSU基于类内最小方差实现阈值寻找, 它对有两个波峰之间有一个波谷的直方图特别好,但是有时候图像的直方图只有一个波峰,这个时候使用TRIANGLE方法寻找阈值是比较好的一个选择。 注意：两个波峰 –&gt; OTSU ， 一个波峰 –&gt; TRANGLE OpenCV中TRIANGLE算法使用只需要在threshold函数的type类型声明THRESH_TRIANGLE即可 代码（c++,python）1234567891011121314151617181920212223242526#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像二值寻找算法 – TRIANGLE */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 自动阈值寻找与二值化 Mat gray, binary; cvtColor(src, gray, COLOR_BGR2GRAY); double T = threshold(gray, binary, 0, 255, THRESH_BINARY | THRESH_TRIANGLE); cout &lt;&lt; "threshold : " &lt;&lt; T &lt;&lt; endl; imshow("binary", binary); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425262728293031323334import cv2 as cvimport numpy as npimport tensorflow as tftf.enable_eager_execution()## THRESH_BINARY = 0# THRESH_BINARY_INV = 1# THRESH_TRUNC = 2# THRESH_TOZERO = 3# THRESH_TOZERO_INV = 4#src = cv.imread("D:/images/lena.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)h, w = src.shape[:2]# 自动阈值分割 TRIANGLEgray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)ret, binary = cv.threshold(gray, 0, 255, cv.THRESH_BINARY | cv.THRESH_TRIANGLE)print("ret :", ret)cv.imshow("binary", binary)result = np.zeros([h, w*2, 3], dtype=src.dtype)result[0:h,0:w,:] = srcresult[0:h,w:2*w,:] = cv.cvtColor(binary, cv.COLOR_GRAY2BGR)cv.putText(result, "input", (10, 30), cv.FONT_ITALIC, 1.0, (0, 0, 255), 2)cv.putText(result, "binary, threshold = " + str(ret), (w+10, 30), cv.FONT_ITALIC, 1.0, (0, 0, 255), 2)cv.imshow("result", result)cv.imwrite("D:/binary_result.png", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像二值寻找算法(TRIANGLE)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-042-图像二值寻找算法(OTSU)]]></title>
    <url>%2F2019%2F04%2F13%2Fopencv-042%2F</url>
    <content type="text"><![CDATA[知识点图像二值化，除了我们上次分享的手动阈值设置与根据灰度图像均值的方法之外，还有几个根据图像直方图实现自动全局阈值寻找的方法，OpenCV中支持的有OTSU与Triangle两种直方图阈值寻找算法。其中OTSU的是通过计算类间最大方差来确定分割阈值的阈值选择算法，OTSU算法对直方图有两个峰，中间有明显波谷的直方图对应图像二值化效果比较好，而对于只有一个单峰的直方图对应的图像分割效果没有双峰的好。 OpenCV中OTSU算法使用只需要在threshold函数的type类型声明THRESH_OTSU即可. 代码（c++,python）1234567891011121314151617181920212223242526#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像二值寻找算法 – OTSU */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 自动阈值寻找与二值化 Mat gray, binary; cvtColor(src, gray, COLOR_BGR2GRAY); double T = threshold(gray, binary, 0, 255, THRESH_BINARY | THRESH_OTSU); cout &lt;&lt; "threshold : " &lt;&lt; T &lt;&lt; endl; imshow("binary", binary); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324252627282930import cv2 as cvimport numpy as np## THRESH_BINARY = 0# THRESH_BINARY_INV = 1# THRESH_TRUNC = 2# THRESH_TOZERO = 3# THRESH_TOZERO_INV = 4#src = cv.imread("D:/images/lena.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)h, w = src.shape[:2]# 自动阈值分割 OTSUgray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)ret, binary = cv.threshold(gray, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)print("ret :", ret)cv.imshow("binary", binary)result = np.zeros([h, w*2, 3], dtype=src.dtype)result[0:h,0:w,:] = srcresult[0:h,w:2*w,:] = cv.cvtColor(binary, cv.COLOR_GRAY2BGR)cv.putText(result, "input", (10, 30), cv.FONT_ITALIC, 1.0, (0, 0, 255), 2)cv.putText(result, "binary, threshold = " + str(ret), (w+10, 30), cv.FONT_ITALIC, 1.0, (0, 0, 255), 2)cv.imshow("result", result)cv.imwrite("D:/binary_result.png", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像二值寻找算法(OTSU)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-041-基本阈值操作]]></title>
    <url>%2F2019%2F04%2F13%2Fopencv-041%2F</url>
    <content type="text"><![CDATA[知识点 API 12345678910111213double cv::threshold( InputArray src, OutputArray dst, double thresh, double maxval, int type)其中type表示阈值分割的方法，支持如下五种：THRESH_BINARY = 0 二值分割THRESH_BINARY_INV = 1 反向二值分割THRESH_TRUNC = 2 截断THRESH_TOZERO = 3 取零THRESH_TOZERO_INV = 4 反向取零 代码（c++,python）1234567891011121314151617181920212223242526272829303132#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 基本阈值操作 */int main() &#123; Mat src = imread("../images/master.jpg"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); int T = mean(src)[0]; Mat gray, binary; cvtColor(src, gray, COLOR_BGR2GRAY); for (int i = 0; i &lt; 5; ++i) &#123;// THRESH_BINARY = 0// THRESH_BINARY_INV = 1// THRESH_TRUNC = 2// THRESH_TOZERO = 3// THRESH_TOZERO_INV = 4 threshold(gray, binary, T, 255, i); imshow(format("binary_%d", i), binary); &#125; waitKey(0); return 0;&#125; 1234567891011121314151617181920212223import cv2 as cvimport numpy as np## THRESH_BINARY = 0# THRESH_BINARY_INV = 1# THRESH_TRUNC = 2# THRESH_TOZERO = 3# THRESH_TOZERO_INV = 4#src = cv.imread("D:/images/master.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)T = 127# 转换为灰度图像gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)for i in range(5): ret, binary = cv.threshold(gray, T, 255, i) cv.imshow("binary_" + str(i), binary)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>基本阈值操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-039-图像模板匹配]]></title>
    <url>%2F2019%2F04%2F12%2Fopencv-039%2F</url>
    <content type="text"><![CDATA[知识点模板匹配被称为最简单的模式识别方法、同时也被很多人认为是最没有用的模式识别方法。这里里面有很大的误区，就是模板匹配是工作条件限制比较严格，只有满足理论设置的条件以后，模板匹配才会比较好的开始工作，而且它不是基于特征的匹配，所以有很多弊端，但是不妨碍它成为入门级别模式识别的方法，通过它可以学习到很多相关的原理性内容，为后续学习打下良好的基础。 API 123456789101112131415161718192021222324void cv::matchTemplate ( InputArray image, InputArray templ, OutputArray result, int method, InputArray mask = noArray() ) Python:result = cv.matchTemplate( image, templ, method[, result[, mask]] )其中method表示模板匹配时候采用的计算像素相似程度的方法，常见有如下TM_SQDIFF = 0TM_SQDIFF_NORMED = 1平方不同与平方不同的归一化版本TM_CCORR = 2TM_CCORR_NORMED = 3相关性，值越大相关性越强，表示匹配程度越高。归一化版本值在0～1之间，1表示高度匹配，0表示完全不匹配TM_CCOEFF = 4TM_CCOEFF_NORMED = 5相关因子，值越大相关性越强，表示匹配程度越高。归一化版本值在0～1之间，1表示高度匹配，0表示完全不匹配 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;const float t = 0.95;/* * 图像模板匹配 */int main() &#123; Mat src = imread("../images/llk.jpg"); Mat tpl = imread("../images/llk_tpl.png"); if (src.empty() || tpl.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); imshow("match_template", tpl); int res_h = src.rows - tpl.rows + 1; int res_w = src.cols - tpl.cols + 1; Mat result = Mat::zeros(Size(res_w, res_h), CV_32FC1); matchTemplate(src, tpl, result, TM_CCOEFF_NORMED); imshow("result", result); for (int row = 0; row &lt; result.rows; ++row) &#123; for (int col = 0; col &lt; result.cols; ++col) &#123; float v = result.at&lt;float&gt;(row, col); if (v &gt; t)&#123; rectangle(src, Point(col, row), Point(col + tpl.cols, row+tpl.rows), Scalar(255,0,0)); &#125; &#125; &#125; imshow("template_result", src); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425import cv2 as cvimport numpy as npdef template_demo(): src = cv.imread("D:/images/llk.jpg") tpl = cv.imread("D:/images/llk_tpl.png") cv.imshow("input", src) cv.imshow("tpl", tpl) th, tw = tpl.shape[:2] result = cv.matchTemplate(src, tpl, cv.TM_CCORR_NORMED) cv.imshow("result", result) cv.imwrite("D:/039_003.png", np.uint8(result*255)) t = 0.98 loc = np.where(result &gt; t) for pt in zip(*loc[::-1]): cv.rectangle(src, pt, (pt[0] + tw, pt[1] + th), (255, 0, 0), 1, 8, 0) cv.imshow("llk-demo", src) cv.imwrite("D:/039_004.png", src)template_demo()cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像模板匹配</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-040-二值图像介绍]]></title>
    <url>%2F2019%2F04%2F12%2Fopencv-040%2F</url>
    <content type="text"><![CDATA[知识点 二值图像就是只有黑白两种颜色表示的图像，其中0 – 表示黑色， 1 – 表示白色(255) 。二值图像处理与分析在机器视觉与机器人视觉中非常重要，涉及到非常多的图像处理相关的知识，常见的二值图像分析包括轮廓分析、对象测量、轮廓匹配与识别、形态学处理与分割、各种形状检测与拟合、投影与逻辑操作、轮廓特征提取与编码等。此外图像二值化的方法也有很多，OpenCV主要是支持几种经典的二值化算法。 从编程与代码角度，OpenCV中二值图像单通道的、字节类型的Mat对象、对于任意的输入图像首先需要把图像转换为灰度、然后通过二值化方法转换为二值图像。本质上，从灰度到二值图像，是对数据的二分类分割，所以很多数据处理的方法都可以使用，但是图像是特殊类型的数据，它有很多限制条件，决定了只有一些合适的方法才会取得比较好的效果。这些算法的最主要的一个任务就是寻找合理的分割阈值T、对于给定任意一个像素点灰度值P(x, y) &gt; T ? 255 : 0 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 二值图像介绍 */int main() &#123; Mat src = imread("../images/master.jpg"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 转为灰度图像 Mat gray, binary; cvtColor(src, gray, COLOR_BGR2GRAY); int t = 127; Scalar m = mean(src); int t_mean = m[0]; // 转二值图像 binary = Mat::zeros(src.size(), CV_8UC1); for (int row = 0; row &lt; src.rows; ++row) &#123; for (int col = 0; col &lt; src.cols; ++col) &#123; int pv = gray.at&lt;uchar&gt;(row, col); pv &gt; t ? binary.at&lt;uchar&gt;(row, col) = 255 : binary.at&lt;uchar&gt;(row, col) = 0; &#125; &#125; imshow("binary_t=127", binary); //imshow("binary_t=mean", binary); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425262728import cv2 as cvimport numpy as npsrc = cv.imread("D:/images/master.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)T = 127# 转换为灰度图像gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)h, w = gray.shapeT = cv.mean(gray)[0]print("current threshold value : ", T)# 二值图像binary = np.zeros((h, w), dtype=np.uint8)for row in range(h): for col in range(w): pv = gray[row, col] if pv &gt; T: binary[row, col] = 255 else: binary[row, col] = 0cv.imshow("binary", binary)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>二值图像介绍</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-038-拉普拉斯金字塔]]></title>
    <url>%2F2019%2F04%2F11%2Fopencv-038%2F</url>
    <content type="text"><![CDATA[知识点对输入图像实现金字塔的reduce操作就会生成不同分辨率的图像、对这些图像进行金字塔expand操作，然后使用reduce减去expand之后的结果就会得到图像拉普拉斯金字塔图像。举例如下：输入图像G(0)金字塔reduce操作生成 G(1), G(2), G(3)拉普拉斯金字塔：L0 = G(0)-expand(G(1))L1 = G(1)-expand(G(2))L2 = G(2)–expand(G(3))G(0)减去expand(G(1))得到的结果就是两次高斯模糊输出的不同，所以L0称为DOG（高斯不同）、它约等于LOG所以又称为拉普拉斯金字塔。所以要求的图像的拉普拉斯金字塔，首先要进行金字塔的reduce操作，然后在通过expand操作，最后相减得到拉普拉斯金字塔图像。 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;void pyramid_up(Mat &amp;image, vector&lt;Mat&gt; &amp;pyramid_images, int level);void laplaian_demo(vector&lt;Mat&gt; &amp;pyramid_images, Mat &amp;image);/* * 拉普拉斯金字塔 */int main() &#123; Mat src = imread("../images/test1.jpg"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); vector&lt;Mat&gt; p_images; pyramid_up(src, p_images, 2); laplaian_demo(p_images, src); waitKey(0); return 0;&#125;void laplaian_demo(vector&lt;Mat&gt; &amp;pyramid_images, Mat &amp;image) &#123; for (int i = pyramid_images.size() - 1; i &gt; -1; --i) &#123; Mat dst; if (i - 1 &lt; 0) &#123; pyrUp(pyramid_images[i], dst, image.size()); subtract(image, dst, dst); dst = dst + Scalar(127, 127, 127);# 调亮度， 实际中不能这么用 imshow(format("laplaian_layer_%d", i), dst); &#125; else &#123; pyrUp(pyramid_images[i], dst, pyramid_images[i-1].size()); subtract(pyramid_images[i - 1], dst, dst); dst = dst + Scalar(127, 127, 127); imshow(format("laplaian_layer_%d", i), dst); &#125; &#125;&#125;void pyramid_up(Mat &amp;image, vector&lt;Mat&gt; &amp;pyramid_images, int level) &#123; Mat temp = image.clone(); Mat dst; for (int i = 0; i &lt; level; ++i) &#123; pyrDown(temp, dst); //imshow(format("pyramid_up_%d", i), dst); temp = dst.clone(); pyramid_images.push_back(temp); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839import cv2 as cvimport numpy as npdef laplaian_demo(pyramid_images): level = len(pyramid_images) for i in range(level-1, -1, -1): if (i-1) &lt; 0: h, w = src.shape[:2] expand = cv.pyrUp(pyramid_images[i], dstsize=(w, h)) lpls = cv.subtract(src, expand) + 127 cv.imshow("lpls_" + str(i), lpls) else: h, w = pyramid_images[i-1].shape[:2] expand = cv.pyrUp(pyramid_images[i], dstsize=(w, h)) lpls = cv.subtract(pyramid_images[i-1], expand) + 127 cv.imshow("lpls_"+str(i), lpls)def pyramid_up(image, level=3): temp = image.copy() # cv.imshow("input", image) pyramid_images = [] for i in range(level): dst = cv.pyrDown(temp) pyramid_images.append(dst) # cv.imshow("pyramid_up_" + str(i), dst) temp = dst.copy() return pyramid_imagessrc = cv.imread("D:/images/master.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# pyramid_up(src)laplaian_demo(pyramid_up(src))cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>拉普拉斯金字塔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-037-图像金字塔]]></title>
    <url>%2F2019%2F04%2F11%2Fopencv-037%2F</url>
    <content type="text"><![CDATA[知识点图像金字塔是对一张输入图像先模糊再下采样为原来大小的1/4（宽高缩小一半）、不断重复模糊与下采样的过程就得到了不同分辨率的输出图像，叠加在一起就形成了图像金字塔、所以图像金字塔是图像的空间多分辨率存在形式。这里的模糊是指高斯模糊，所以这个方式生成的金字塔图像又称为高斯金字塔图像。高斯金字塔图像有两个基本操作： reduce 是从原图生成高斯金字塔图像、生成一系列低分辨图像 expand 是从高斯金字塔图像反向生成高分辨率图像 规则： 图像金字塔在redude过程或者expand过程中必须是逐层 reduce过程中每一层都是前一层的1/4 API reduce 操作 pyrDownexpand操作 pyrUp 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;void pyramid_up(Mat &amp;image, vector&lt;Mat&gt; &amp;pyramid_images, int level);void pyramid_down(vector&lt;Mat&gt; &amp;pyramid_images);int main() &#123; Mat src = imread("../images/test1.jpg"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); vector&lt;Mat&gt; p_images; pyramid_up(src, p_images, 2); pyramid_down(p_images); waitKey(0); return 0;&#125;void pyramid_down(vector&lt;Mat&gt; &amp;pyramid_images) &#123; for (int i = pyramid_images.size() - 1; i &gt; -1; --i) &#123; Mat dst; pyrUp(pyramid_images[i], dst); imshow(format("pyramid_down_%d", i), dst); &#125;&#125;void pyramid_up(Mat &amp;image, vector&lt;Mat&gt; &amp;pyramid_images, int level) &#123; Mat temp = image.clone(); Mat dst; for (int i = 0; i &lt; level; ++i) &#123; pyrDown(temp, dst); imshow(format("pyramid_up_%d", i), dst); temp = dst.clone(); pyramid_images.push_back(temp); &#125;&#125; 12345678910111213141516171819202122232425262728293031import cv2 as cvdef pyramid_down(pyramid_images): level = len(pyramid_images) print("level = ",level) for i in range(level-1, -1, -1): expand = cv.pyrUp(pyramid_images[i]) cv.imshow("pyramid_down_"+str(i), expand)def pyramid_up(image, level=3): temp = image.copy() # cv.imshow("input", image) pyramid_images = [] for i in range(level): dst = cv.pyrDown(temp) pyramid_images.append(dst) # cv.imshow("pyramid_up_" + str(i), dst) temp = dst.copy() return pyramid_imagessrc = cv.imread("D:/images/master.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# pyramid_up(src)pyramid_down(pyramid_up(src))cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像金字塔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-036-Canny边缘检测器]]></title>
    <url>%2F2019%2F04%2F10%2Fopencv-036%2F</url>
    <content type="text"><![CDATA[知识点1986年，JOHN CANNY 提出一个很好的边缘检测算法，被称为Canny编边缘检测器。Canny边缘检测器是一种经典的图像边缘检测与提取算法，应用广泛，主要是因为Canny边缘检测具备以下特点： 有效的噪声抑制 更强的完整边缘提取能力 Canny算法是如何做到精准的边缘提取的，主要是靠下面五个步骤： 高斯模糊 – 抑制噪声 梯度提取得到边缘候选 角度计算与非最大信号抑制 高低阈值链接、获取完整边缘 输出边缘 API 123456789101112void cv::Canny( InputArray image, OutputArray edges, double threshold1, double threshold2, int apertureSize = 3, bool L2gradient = false)threshold1 是Canny边缘检测算法第四步中高低阈值链接中低阈值threshold2 是Canny边缘检测算法第四步中高低阈值链接中高阈值、高低阈值之比在2:1～3:1之间最后一个参数是计算gradient的方法L1或者L2 代码（c++,python）1234567891011121314151617181920212223242526#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * Canny边缘检测器 */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); Mat edges, edges_src; Canny(src, edges, 100, 300); // 提取彩色边缘 bitwise_and(src, src, edges_src, edges); imshow("edges", edges); imshow("edges_src", edges_src); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324import cv2 as cvimport numpy as npsrc = cv.imread("D:/images/master.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# t1 = 100, t2 = 3*t1 = 300edge = cv.Canny(src, 100, 300)cv.imshow("mask image", edge)cv.imwrite("D:/edge.png", edge)edge_src = cv.bitwise_and(src, src, mask=edge)h, w = src.shape[:2]result = np.zeros([h, w*2, 3], dtype=src.dtype)result[0:h,0:w,:] = srcresult[0:h,w:2*w,:] = edge_srccv.putText(result, "original image", (10, 30), cv.FONT_ITALIC, 1.0, (0, 0, 255), 2)cv.putText(result, "edge image", (w+10, 30), cv.FONT_ITALIC, 1.0, (0, 0, 255), 2)cv.imshow("edge detector", result)cv.imwrite("D:/result.png", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>Canny边缘检测器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-035-USM-锐化增强算法]]></title>
    <url>%2F2019%2F04%2F10%2Fopencv-035%2F</url>
    <content type="text"><![CDATA[知识点图像卷积处理实现锐化有一种常用的算法叫做Unsharpen Mask方法，这种锐化的方法就是对原图像先做一个高斯模糊，然后用原来的图像减去一个系数乘以高斯模糊之后的图像，然后再把值Scale到0～255的RGB像素值范围之内。基于USM锐化的方法可以去除一些细小的干扰细节和噪声，比一般直接使用卷积锐化算子得到的图像锐化结果更加真实可信。 USM锐化公式表示如下：（源图像– w*高斯模糊）/（1-w）；其中w表示权重（0.1～0.9），默认为0.6 OpenCV中的代码实现步骤 高斯模糊 权重叠加 输出结果 代码（c++,python）123456789101112131415161718192021#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); Mat blur_img, usm; GaussianBlur(src, blur_img, Size(0,0), 25); addWeighted(src, 1.5, blur_img, -0.5, 0, usm); imshow("usm", usm); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223import cv2 as cvimport numpy as npsrc = cv.imread("D:/images/master.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# sigma = 5、15、25blur_img = cv.GaussianBlur(src, (0, 0), 5)usm = cv.addWeighted(src, 1.5, blur_img, -0.5, 0)cv.imshow("mask image", usm)h, w = src.shape[:2]result = np.zeros([h, w*2, 3], dtype=src.dtype)result[0:h,0:w,:] = srcresult[0:h,w:2*w,:] = usmcv.putText(result, "original image", (10, 30), cv.FONT_ITALIC, 1.0, (0, 0, 255), 2)cv.putText(result, "sharpen image", (w+10, 30), cv.FONT_ITALIC, 1.0, (0, 0, 255), 2)cv.imshow("sharpen_image", result)cv.imwrite("D:/result.png", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>USM-锐化增强算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-034-图像锐化]]></title>
    <url>%2F2019%2F04%2F09%2Fopencv-034%2F</url>
    <content type="text"><![CDATA[知识点图像卷积的主要有三功能分别是图像的模糊/去噪、图像梯度/边缘发现、图像锐化/增强，前面的两个功能我们以前通过相关知识点的分享加以了解，学习了相关API的使用。图像锐化的本质是图像拉普拉斯滤波加原图权重像素叠加的输出 ： 123-1 -1 -1-1 C -1-1 -1 -1 当C值大于8时候表示图像锐化、越接近8表示锐化效果越好 当C值等于8时候图像的高通滤波 当C值越大，图像锐化效果在减弱、中心像素的作用在提升 代码（c++,python）123456789101112131415161718192021222324252627282930313233#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像锐化 */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); Mat sharpen_op = (Mat_&lt;char&gt;(3,3) &lt;&lt; -1, -1, -1, -1, 9, -1, -1, -1, -1); // Mat sharpen_op1 = (Mat_&lt;char&gt;(3,3) &lt;&lt; 0, -1, 0, // -1, 9, -1, // 0, -1, 0); Mat dst; filter2D(src, dst, CV_32F, sharpen_op); convertScaleAbs(dst, dst); imshow("sharpen", dst); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324import cv2 as cvimport numpy as npsrc = cv.imread("D:/images/test.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# sharpen_op = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]], dtype=np.float32)sharpen_op = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]], dtype=np.float32)sharpen_image = cv.filter2D(src, cv.CV_32F, sharpen_op)sharpen_image = cv.convertScaleAbs(sharpen_image)cv.imshow("sharpen_image", sharpen_image)h, w = src.shape[:2]result = np.zeros([h, w*2, 3], dtype=src.dtype)result[0:h,0:w,:] = srcresult[0:h,w:2*w,:] = sharpen_imagecv.putText(result, "original image", (10, 30), cv.FONT_ITALIC, 1.0, (0, 0, 255), 2)cv.putText(result, "sharpen image", (w+10, 30), cv.FONT_ITALIC, 1.0, (0, 0, 255), 2)cv.imshow("sharpen_image", result)cv.imwrite("D:/result.png", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像锐化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-033-图像梯度之拉普拉斯算子(二阶导数算子)]]></title>
    <url>%2F2019%2F04%2F09%2Fopencv-033%2F</url>
    <content type="text"><![CDATA[知识点图像的一阶导数算子可以得到图像梯度局部梯度相应值，二阶导数可以通过快速的图像像素值强度的变化来检测图像边缘，其检测图像边缘的原理跟图像的一阶导数有点类似，只是在二阶导数是求X、Y方向的二阶偏导数，对图像来说： X方向的二阶偏导数就是 dx = f(x+1, y) + f(x-1, y) – 2*f(x, y) Y方向的二阶偏导数就是 dy = f(x, y+1) + f(x, y-1) – 2*f(x, y) 对X方向与Y方向进行叠加最终就得到delta对应的二阶导数算子。 API 12345678910111213// OpenCV中Laplacian滤波函数就是二阶导数发现边缘的函数void cv::Laplacian( InputArray src, OutputArray dst, int ddepth, // 深度默认是-1表示输入与输出图像相同 int ksize = 1,// 必须是奇数， 等于1是四邻域算子，大于1改用八邻域算子 double scale = 1, double delta = 0, // 对输出图像加上常量值 int borderType = BORDER_DEFAULT ) Python:dst = cv.Laplacian(src, ddepth[, dst[, ksize[, scale[, delta[, borderType]]]] ) 代码（c++,python）12345678910111213141516171819202122232425#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 拉普拉斯算子(二阶导数算子) */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); Mat blured, dst; GaussianBlur(src, blured, Size(3,3), 0); Laplacian(blured, dst, CV_32F, 1, 1.0, 127.0); convertScaleAbs(dst, dst); imshow("Laplacian", dst); waitKey(0); return 0;&#125; 1234567891011121314151617181920import cv2 as cvimport numpy as npimage = cv.imread("D:/images/yuan_test.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", image)h, w = image.shape[:2]src = cv.GaussianBlur(image, (0, 0), 1)dst = cv.Laplacian(src, cv.CV_32F, ksize=3, delta=127)dst = cv.convertScaleAbs(dst)result = np.zeros([h, w*2, 3], dtype=image.dtype)result[0:h,0:w,:] = imageresult[0:h,w:2*w,:] = dstcv.imshow("result", result)cv.imwrite("D:/laplacian_08.png", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>拉普拉斯算子</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-032-图像梯度之robert算子与prewitt算子]]></title>
    <url>%2F2019%2F04%2F09%2Fopencv-032%2F</url>
    <content type="text"><![CDATA[知识点图像的一阶导数算子除了sobel算子之外，常见的还有robert算子与prewitt算子，它们也都是非常好的可以检测图像的梯度边缘信息，通过OpenCV中自定义滤波器，使用自定义创建的robert与prewitt算子就可以实现图像的rober与prewitt梯度边缘检测。 API 123456789101112filter2D( InputArray src, OutputArray dst, int ddepth, InputArray kernel, Point anchor = Point(-1,-1), double delta = 0, int borderType = BORDER_DEFAULT )Python:dst =cv.filter2D(src, ddepth, kernel[, dst[, anchor[, delta[, borderType]]]]) 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // Robert算子 Mat robert_x = (Mat_&lt;int&gt;(2, 2) &lt;&lt; 1, 0, 0, -1); Mat robert_y = (Mat_&lt;int&gt;(2, 2) &lt;&lt; 0, -1, 1, 0); Mat robert_grad_x, robert_grad_y, robert_grad; filter2D(src, robert_grad_x, CV_16S, robert_x); filter2D(src, robert_grad_y, CV_16S, robert_y); convertScaleAbs(robert_grad_x, robert_grad_x); convertScaleAbs(robert_grad_y, robert_grad_y); add(robert_grad_x, robert_grad_y, robert_grad); convertScaleAbs(robert_grad, robert_grad); imshow("robert_grad_x", robert_grad_x); imshow("robert_grad_y", robert_grad_y); imshow("robert_grad", robert_grad); // 定义Prewitt算子 Mat prewitt_x = (Mat_&lt;char&gt;(3, 3) &lt;&lt; -1, 0, 1, -1, 0, 1, -1, 0, 1); Mat prewitt_y = (Mat_&lt;char&gt;(3, 3) &lt;&lt; -1, -1, -1, 0, 0, 0, 1, 1, 1); Mat prewitt_grad_x, prewitt_grad_y, prewitt_grad; filter2D(src, prewitt_grad_x, CV_32F, prewitt_x); filter2D(src, prewitt_grad_y, CV_32F, prewitt_y); convertScaleAbs(prewitt_grad_x, prewitt_grad_x); convertScaleAbs(prewitt_grad_y, prewitt_grad_y); add(prewitt_grad_x, prewitt_grad_y, prewitt_grad); convertScaleAbs(prewitt_grad, prewitt_grad); imshow("prewitt_grad_x", prewitt_grad_x); imshow("prewitt_grad_y", prewitt_grad_y); imshow("prewitt_grad", prewitt_grad); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344import cv2 as cvimport numpy as npsrc = cv.imread("D:/images/test.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)robert_x = np.array([[1, 0],[0, -1]], dtype=np.float32)robert_y = np.array([[0, -1],[1, 0]], dtype=np.float32)prewitt_x = np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]], dtype=np.float32)prewitt_y = np.array([[-1, -1, -1], [0, 0, 0], [1, 1, 1]], dtype=np.float32)robert_grad_x = cv.filter2D(src, cv.CV_16S, robert_x)robert_grad_y = cv.filter2D(src, cv.CV_16S, robert_y)robert_grad_x = cv.convertScaleAbs(robert_grad_x)robert_grad_y = cv.convertScaleAbs(robert_grad_y)prewitt_grad_x = cv.filter2D(src, cv.CV_32F, prewitt_x)prewitt_grad_y = cv.filter2D(src, cv.CV_32F, prewitt_y)prewitt_grad_x = cv.convertScaleAbs(prewitt_grad_x)prewitt_grad_y = cv.convertScaleAbs(prewitt_grad_y)# cv.imshow("robert x", robert_grad_x);# cv.imshow("robert y", robert_grad_y);# cv.imshow("prewitt x", prewitt_grad_x);# cv.imshow("prewitt y", prewitt_grad_y);h, w = src.shape[:2]robert_result = np.zeros([h, w*2, 3], dtype=src.dtype)robert_result[0:h,0:w,:] = robert_grad_xrobert_result[0:h,w:2*w,:] = robert_grad_ycv.imshow("robert_result", robert_result)prewitt_result = np.zeros([h, w*2, 3], dtype=src.dtype)prewitt_result[0:h,0:w,:] = prewitt_grad_xprewitt_result[0:h,w:2*w,:] = prewitt_grad_ycv.imshow("prewitt_result", prewitt_result)cv.imwrite("D:/prewitt.png", prewitt_result)cv.imwrite("D:/robert.png", robert_result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>robert算子</tag>
        <tag>prewitt算子</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-031-图像梯度之Sobel算子]]></title>
    <url>%2F2019%2F04%2F09%2Fopencv-031%2F</url>
    <content type="text"><![CDATA[知识点卷积的作用除了实现图像模糊或者去噪，还可以寻找一张图像上所有梯度信息，这些梯度信息是图像的最原始特征数据，进一步处理之后就可以生成一些比较高级的特征用来表示一张图像实现基于图像特征的匹配，图像分类等应用。Sobel算子是一种很经典的图像梯度提取算子，其本质是基于图像空间域卷积，背后的思想是图像一阶导数算子的理论支持。 API 12345678910111213void cv::Sobel( InputArray src, // 输入图像 OutputArray dst, // 输出结果 int ddepth, // 图像深度CV_32F int dx,// 1，X方向 一阶导数 int dy, // 1，Y方向 一阶导数 int ksize = 3, // 窗口大小 double scale = 1, // 放缩比率，1 表示不变 double delta = 0, // 对输出结果图像加上常量值 int borderType = BORDER_DEFAULT ) Python:dst = cv.Sobel(src, ddepth, dx, dy[, dst[, ksize[, scale[, delta[, borderType]]]]]) 代码（c++,python）123456789101112131415161718192021222324252627282930#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;int main() &#123; Mat src = imread("../images/test3.jpg"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); Mat grad_x, grad_y, grad; // 求取x方向和y方向梯度 Sobel(src, grad_x, CV_32F, 1, 0); Sobel(src, grad_y, CV_32F, 0, 1); convertScaleAbs(grad_x, grad_x); convertScaleAbs(grad_y, grad_y); // 求取总梯度 add(grad_x, grad_y, grad, Mat(), CV_16S); convertScaleAbs(grad, grad); imshow("grad_x", grad_x); imshow("grad_y", grad_y); imshow("grad", grad); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425262728import cv2 as cvimport numpy as npsrc = cv.imread("D:/images/grad.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)h, w = src.shape[:2]x_grad = cv.Sobel(src, cv.CV_32F, 1, 0)y_grad = cv.Sobel(src, cv.CV_32F, 0, 1)x_grad = cv.convertScaleAbs(x_grad)y_grad = cv.convertScaleAbs(y_grad)# cv.imshow("x_grad", x_grad)# cv.imshow("y_grad", y_grad)dst = cv.add(x_grad, y_grad, dtype=cv.CV_16S)dst = cv.convertScaleAbs(dst)cv.imshow("gradient", dst)result = np.zeros([h, w*2, 3], dtype=src.dtype)result[0:h,0:w,:] = srcresult[0:h,w:2*w,:] = dstcv.imshow("result", result)cv.imwrite("D:/result.png", dst)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>Sobel算子</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-030-自定义滤波器]]></title>
    <url>%2F2019%2F04%2F09%2Fopencv-030%2F</url>
    <content type="text"><![CDATA[知识点图像卷积最主要功能有图像模糊、锐化、梯度边缘等，前面已经分享图像卷积模糊的相关知识点，OpenCV除了支持上述的卷积模糊（均值与边缘保留）还支持自定义卷积核，实现自定义的滤波操作。自定义卷积核常见的主要是均值、锐化、梯度等算子。 下面的三个自定义卷积核分别可以实现卷积的均值模糊、锐化、梯度功能： 1231，1， 1 0， -1， 0 1， 01，1， 1 -1， 5， -1 0， -11，1， 1 0， -1， 0 API int ddepth, // 默认-1，表示输入与输出图像类型一致，但是当涉及浮点数计算时候，需要设置为CV_32F。滤波完成之后需要使用convertScaleAbs函数将结果转换为字节类型。 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 自定义滤波 */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); Mat dst1, dst2, dst3; // 均值模糊 Mat kernel1 = Mat::ones(5, 5, CV_32F) / (float) (25); filter2D(src, dst1, -1, kernel1); // 锐化 Mat kernel2 = (Mat_&lt;char&gt;(3, 3) &lt;&lt; 0, -1, 0, -1, 5, -1, 0, -1, 0); filter2D(src, dst2, -1, kernel2); // 梯度 Mat kernel3 = (Mat_&lt;int&gt;(2, 2) &lt;&lt; 1, 0, 0, -1); filter2D(src, dst3, CV_32F, kernel3); convertScaleAbs(dst3, dst3); // 转换为字节类型，非常重要 imshow("blur=5x5", dst1); imshow("shape=3x3", dst2); imshow("gradient=2x2", dst3); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324import cv2 as cvimport numpy as npsrc = cv.imread("D:/images/test.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)blur_op = np.ones([5, 5], dtype=np.float32)/25.shape_op = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]], np.float32)grad_op = np.array([[1, 0],[0, -1]], dtype=np.float32)dst1 = cv.filter2D(src, -1, blur_op)dst2 = cv.filter2D(src, -1, shape_op)dst3 = cv.filter2D(src, cv.CV_32F, grad_op)dst3 = cv.convertScaleAbs(dst3)cv.imshow("blur=5x5", dst1);cv.imshow("shape=3x3", dst2);cv.imshow("gradient=2x2", dst3);cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>自定义滤波器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决 Clion-opencv-undefined reference to "XXX" 问题]]></title>
    <url>%2F2019%2F04%2F09%2FClion_opencv_undefined_reference_to%2F</url>
    <content type="text"><![CDATA[问题描述在CLion使用opencv4 的一些函数时，出现 “ undefined reference to “ 的问题，如下图： 原因查看CMakeLists.txt opencv官网查看edgePreservingFilter函数 edgePreservingFilter函数需要导入photo库，而CMakeLists.txt中没有导入 解决方法CMakeLists.txt修改如下： 成功解决。]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>Clion-opencv-undefined_reference_to</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-029-快速的图像边缘滤波算法]]></title>
    <url>%2F2019%2F04%2F08%2Fopencv-029%2F</url>
    <content type="text"><![CDATA[知识点高斯双边模糊与mean shift均值模糊两种边缘保留滤波算法，都因为计算量比较大，无法实时实现图像边缘保留滤波，限制了它们的使用场景，OpenCV中还实现了一种快速的边缘保留滤波算法。高斯双边与mean shift均值在计算时候使用五维向量是其计算量大速度慢的根本原因，该算法通过等价变换到低纬维度空间，实现了数据降维与快速计算。 API 其中 sigma_s的取值范围为0～200， sigma_r的取值范围为0～1 当sigma_s取值不变时候，sigma_r越大图像滤波效果越明显 当sigma_r取值不变时候，窗口sigma_s越大图像模糊效果越明显 当sgma_r取值很小的时候，窗口sigma_s取值无论如何变化，图像双边滤波效果都不好！ 代码（c++,python）1234567891011121314151617181920212223#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 快速的图像边缘滤波算法 */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); Mat dst; edgePreservingFilter(src, dst, 1, 60, 0.44); imshow("result", dst); waitKey(0); return 0;&#125; 123456789101112131415161718import cv2 as cvimport numpy as npsrc = cv.imread("D:/images/example.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)h, w = src.shape[:2]dst = cv.edgePreservingFilter(src, sigma_s=100, sigma_r=0.4, flags=cv.RECURS_FILTER)result = np.zeros([h, w*2, 3], dtype=src.dtype)result[0:h,0:w,:] = srcresult[0:h,w:2*w,:] = dstcv.imshow("result", result)cv.imwrite("D:/result.png", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>快速的图像边缘滤波算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-028-图像积分图算法]]></title>
    <url>%2F2019%2F04%2F07%2Fopencv-028%2F</url>
    <content type="text"><![CDATA[知识点积分图像是Crow在1984年首次提出，是为了在多尺度透视投影中提高渲染速度，是一种快速计算图像区域和与平方和的算法。其核心思想是对每个图像建立自己的积分图查找表，在图像积分处理计算阶段根据预先建立的积分图查找表，直接查找从而实现对均值卷积线性时间计算，做到了卷积执行的时间与半径窗口大小的无关联。图像积分图在图像特征提取HAAR/SURF、二值图像分析、图像相似相关性NCC计算、图像卷积快速计算等方面均有应用，是图像处理中的经典算法之一。 图像积分图建立与查找在积分图像(Integral Image - ii)上任意位置(x, y)处的ii(x, y)表示该点左上角所有像素之和， 其中(x,y)是图像像素点坐标。 API integral( InputArray src, // 输入图像 OutputArray sum, // 和表 OutputArray sqsum, // 平方和表 OutputArray tilted, // 瓦块和表 int sdepth = -1, // 和表数据深度常见CV_32S int sqdepth = -1 // 平方和表数据深度 常见 CV_32F) 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;void blur_demo(Mat &amp;image, Mat &amp;sum);void edge_demo(Mat &amp;image, Mat &amp;sum);int getblockSum(Mat &amp;sum, int x1, int y1, int x2, int y2, int i);/* * 图像积分图算法 */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 计算积分图 Mat sum, sqrsum; integral(src, sum, sqrsum); /* * 积分图应用 */ int type = 0; // 模糊应用 blur_demo(src, sum); // 边缘检测 edge_demo(src, sum); waitKey(0); return 0;&#125;void blur_demo(Mat &amp;image, Mat &amp;sum) &#123; int w = image.cols; int h = image.rows; Mat result = Mat::zeros(image.size(), image.type()); int x2 = 0, y2 = 0; int x1 = 0, y1 = 0; int ksize = 5; int radius = ksize / 2; int ch = image.channels(); int cx = 0, cy = 0; for (int row = 0; row &lt; h + radius; row++) &#123; y2 = (row + 1)&gt;h ? h : (row + 1); y1 = (row - ksize) &lt; 0 ? 0 : (row - ksize); for (int col = 0; col &lt; w + radius; col++) &#123; x2 = (col + 1)&gt;w ? w : (col + 1); x1 = (col - ksize) &lt; 0 ? 0 : (col - ksize); cx = (col - radius) &lt; 0 ? 0 : col - radius; cy = (row - radius) &lt; 0 ? 0 : row - radius; int num = (x2 - x1)*(y2 - y1); for (int i = 0; i &lt; ch; i++) &#123; // 积分图查找和表，计算卷积 int s = getblockSum(sum, x1, y1, x2, y2, i); result.at&lt;Vec3b&gt;(cy, cx)[i] = saturate_cast&lt;uchar&gt;(s / num); &#125; &#125; &#125; imshow("blur_demo", result);&#125;/*** 3x3 sobel 垂直边缘检测演示*/void edge_demo(Mat &amp;image, Mat &amp;sum) &#123; int w = image.cols; int h = image.rows; Mat result = Mat::zeros(image.size(), CV_32SC3); int x2 = 0, y2 = 0; int x1 = 0, y1 = 0; int ksize = 3; // 算子大小，可以修改，越大边缘效应越明显 int radius = ksize / 2; int ch = image.channels(); int cx = 0, cy = 0; for (int row = 0; row &lt; h + radius; row++) &#123; y2 = (row + 1)&gt;h ? h : (row + 1); y1 = (row - ksize) &lt; 0 ? 0 : (row - ksize); for (int col = 0; col &lt; w + radius; col++) &#123; x2 = (col + 1)&gt;w ? w : (col + 1); x1 = (col - ksize) &lt; 0 ? 0 : (col - ksize); cx = (col - radius) &lt; 0 ? 0 : col - radius; cy = (row - radius) &lt; 0 ? 0 : row - radius; int num = (x2 - x1)*(y2 - y1); for (int i = 0; i &lt; ch; i++) &#123; // 积分图查找和表，计算卷积 int s1 = getblockSum(sum, x1, y1, cx, y2, i); int s2 = getblockSum(sum, cx, y1, x2, y2, i); result.at&lt;Vec3i&gt;(cy, cx)[i] = saturate_cast&lt;int&gt;(s2 - s1); &#125; &#125; &#125; Mat dst, gray; convertScaleAbs(result, dst); normalize(dst, dst, 0, 255, NORM_MINMAX); cvtColor(dst, gray, COLOR_BGR2GRAY); imshow("edge_demo", gray);&#125;int getblockSum(Mat &amp;sum, int x1, int y1, int x2, int y2, int i) &#123; int tl = sum.at&lt;Vec3i&gt;(y1, x1)[i]; int tr = sum.at&lt;Vec3i&gt;(y2, x1)[i]; int bl = sum.at&lt;Vec3i&gt;(y1, x2)[i]; int br = sum.at&lt;Vec3i&gt;(y2, x2)[i]; int s = (br - bl - tr + tl); return s;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243import cv2 as cvimport numpy as npdef get_block_sum(ii, x1, y1, x2, y2, index): tl = ii[y1, x1][index] tr = ii[y2, x1][index] bl = ii[y1, x2][index] br = ii[y2, x2][index] s = (br - bl - tr + tl) return sdef blur_demo(image, ii): h, w, dims = image.shape result = np.zeros(image.shape, image.dtype) ksize = 15 radius = ksize // 2 for row in range(0, h + radius, 1): y2 = h if (row + 1)&gt; h else (row + 1) y1 = 0 if (row - ksize) &lt; 0 else (row - ksize) for col in range(0, w + radius, 1): x2 = w if (col + 1)&gt;w else (col + 1) x1 = 0 if (col - ksize) &lt; 0 else (col - ksize) cx = 0 if (col - radius) &lt; 0 else (col - radius) cy = 0 if (row - radius) &lt; 0 else (row - radius) num = (x2 - x1)*(y2 - y1) for i in range(0, 3, 1): s = get_block_sum(ii, x1, y1, x2, y2, i) result[cy, cx][i] = s // num cv.imshow("integral fast blur", result) cv.imwrite("D:/result.png", result)src = cv.imread("D:/images/test1.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)sum_table = cv.integral(src, sdepth=cv.CV_32S)blur_demo(src, sum_table)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像积分图算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-027-边缘保留滤波算法 – 均值迁移模糊(mean-shift blur)]]></title>
    <url>%2F2019%2F04%2F07%2Fopencv-027%2F</url>
    <content type="text"><![CDATA[知识点均值迁移模糊是图像边缘保留滤波算法中一种，经常用来在对图像进行分水岭分割之前去噪声，可以大幅度提升分水岭分割的效果。 均值迁移模糊的主要思想如下：就是在图像进行开窗的时候，同时考虑像素值空间范围分布，只有符合分布的像素点才参与计算，计算得到像素均值与空间位置均值，使用新的均值位置作为窗口中心位置继续基于给定像素值空间分布计算均值与均值位置，如此不断迁移中心位置直到不再变化位置（dx=dy=0），但是在实际情况中我们会人为设置一个停止条件比如迁移几次，这样就可以把最后的RGB均值赋值给中心位置。 API 代码（c++,python）1234567891011121314151617181920212223#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 均值迁移模糊 */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); Mat dst; pyrMeanShiftFiltering(src, dst, 15, 30); imshow("mean_shift_blur", dst); waitKey(0); return 0;&#125; 1234567891011121314151617import cv2 as cvimport numpy as npsrc = cv.imread("D:/images/example.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)h, w = src.shape[:2]dst = cv.pyrMeanShiftFiltering(src, 15, 30, termcrit=(cv.TERM_CRITERIA_MAX_ITER+cv.TERM_CRITERIA_EPS, 5, 1))result = np.zeros([h, w*2, 3], dtype=src.dtype)result[0:h,0:w,:] = srcresult[0:h,w:2*w,:] = dstcv.imshow("result", result)cv.imwrite("D:/result.png", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>均值迁移模糊</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-026-边缘保留滤波算法(EPF)–高斯双边模糊]]></title>
    <url>%2F2019%2F04%2F06%2Fopencv-026%2F</url>
    <content type="text"><![CDATA[知识点前面我们介绍的图像卷积处理无论是均值还是高斯都是属于模糊卷积，它们都有一个共同的特点就是模糊之后图像的边缘信息不复存在，受到了破坏。我们今天介绍的滤波方法有能力通过卷积处理实现图像模糊的同时对图像边缘不会造成破坏，滤波之后的输出完整的保存了图像整体边缘（轮廓）信息，我们称这类滤波算法为边缘保留滤波算法（EPF）。最常见的边缘保留滤波算法有以下几种 高斯双边模糊 Meanshift均值迁移模糊 局部均方差模糊 OpenCV中对边缘保留滤波还有一个专门的API 高斯模糊是考虑图像空间位置对权重的影响，但是它没有考虑图像像素分布对图像卷积输出的影响，双边模糊考虑了像素值分布的影响，对像素值空间分布差异较大的进行保留从而完整的保留了图像的边缘信息。 原理 API 代码（c++,python）1234567891011121314151617181920212223#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 高斯双边模糊 */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); Mat dst; bilateralFilter(src, dst, 0, 100, 10, 4); imshow("gaussian_bilater_blur", dst); waitKey(0); return 0;&#125; 1234567891011121314151617import cv2 as cvimport numpy as npsrc = cv.imread("../images/test.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)h, w = src.shape[:2]dst = cv.bilateralFilter(src, 0, 100, 10)result = np.zeros([h, w*2, 3], dtype=src.dtype)result[0:h,0:w,:] = srcresult[0:h,w:2*w,:] = dstcv.imshow("result", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>高斯双边模糊</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贪吃蛇小项目(c++实现)]]></title>
    <url>%2F2019%2F04%2F05%2Ftanchishe%2F</url>
    <content type="text"><![CDATA[模块结构 墙模块 食物模块 蛇模块 主程序 代码墙模块(wall.h) 123456789101112131415161718192021#ifndef TANCHISHE_WALL_H#define TANCHISHE_WALL_H#include &lt;iostream&gt;using namespace std;class Wall&#123;public: enum &#123;ROW = 20, COL = 30&#125;; // 初始化墙壁 void initWall(); // 画出墙壁 void drawWall(); // 根据索引设置二维数据里的内容 void setWall(int x, int y, char c); // 根据索引获取当前位置的符号 char getWall(int x, int y);private: char gameArray[ROW][COL];&#125;;#endif //TANCHISHE_WALL_H 食物模块(food.h) 123456789101112131415161718#ifndef TANCHISHE_FOOD_H#define TANCHISHE_FOOD_H#include &lt;iostream&gt;#include "wall.h"using namespace std;class Food&#123;public: Food(Wall &amp;tempWall); // 设置食物 void setFood();private: int foodX; int foodY; Wall &amp;wall;&#125;;#endif //TANCHISHE_FOOD_H 蛇模块(snake.h) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#ifndef TANCHISHE_SNAKE_H#define TANCHISHE_SNAKE_H#include &lt;iostream&gt;#include "wall.h"#include "food.h"using namespace std;class Snake &#123;public: Snake(Wall &amp;tempWall, Food &amp;tempFood); enum &#123; UP = 'w', DOWN = 's', LEFT = 'a', RIGHT = 'd' &#125;; struct Point &#123; int x; int y; Point *next; &#125;; // 初始化 void initSnake(); // 销毁节点 void destroyPoint(); // 添加节点 void addPoint(int x, int y); // 删除节点 void delPoint(); // 移动蛇 bool move(char key); // 设定难度 // 获取刷屏时间 int getSleepTime(); // 获取蛇身段 int countList(); // 获取分数 int getScore();private: Point *pHead; Wall &amp;wall; Food &amp;food; bool isRool; // 判断循环表示&#125;;#endif //TANCHISHE_SNAKE_H 主程序(game.cpp) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#include &lt;iostream&gt;#include "wall.h"#include "snake.h"#include "food.h"#include &lt;ctime&gt;#include &lt;conio.h&gt;#include &lt;windows.h&gt;// 定位光标void gotoxy(HANDLE hOut1, int x, int y) &#123; COORD pos; pos.X = x; pos.Y = y; SetConsoleCursorPosition(hOut1, pos);&#125;HANDLE hOut = GetStdHandle(STD_OUTPUT_HANDLE); // 定义显示器句柄变量int main() &#123; // 添加随机数种子 srand((unsigned int) time(NULL)); // 是否死亡的标识 bool isDead = false; // 上一次按键标识 bool preKey = true; Wall wall; wall.initWall(); wall.drawWall(); Food food(wall); food.setFood(); Snake snake(wall, food); snake.initSnake(); gotoxy(hOut, 0, Wall::ROW); cout &lt;&lt; "score：" &lt;&lt; snake.getScore() &lt;&lt; " points" &lt;&lt; endl; while (!isDead) &#123; // 接受用户输入 char key = _getch(); // 第一次按左键，不激活游戏 if (preKey &amp;&amp; key == snake.LEFT) &#123; continue; &#125; preKey = false; do &#123; if (key == snake.UP || key == snake.DOWN || key == snake.LEFT || key == snake.RIGHT) &#123; if (snake.move(key)) &#123; //system("cls"); //wall.drawWall(); gotoxy(hOut, 0, Wall::ROW); cout &lt;&lt; "score：" &lt;&lt; snake.getScore() &lt;&lt; " points" &lt;&lt; endl; Sleep(snake.getSleepTime()); &#125; else &#123; isDead = true; break; &#125; &#125; else &#123; cout &lt;&lt; "input error, please input again" &lt;&lt; endl; break; &#125; &#125; while (!_kbhit()); // 没有键盘输入的时候，返回0 &#125; system("pause"); return 0;&#125; 结果 代码地址github]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>贪吃蛇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-025-图像去噪声]]></title>
    <url>%2F2019%2F04%2F04%2Fopencv-025%2F</url>
    <content type="text"><![CDATA[知识点图像去噪声在OCR、机器人视觉与机器视觉领域应用开发中是重要的图像预处理手段之一，对图像二值化与二值分析很有帮助，OpenCV中常见的图像去噪声的方法有 均值去噪声 高斯模糊去噪声 非局部均值去噪声 双边滤波去噪声 形态学去噪声 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;void add_gaussian_noise(Mat &amp;image);/* * 图像去噪 */int main() &#123; Mat src = imread("../images/test.png"); Mat src_clone = src.clone(); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 产生高斯噪声 add_gaussian_noise(src); Mat res1, res2, res3, res4; // 均值去噪 blur(src, res1, Size(3, 3)); imshow("mean_blur", res1); // 高斯去噪 GaussianBlur(src, res2, Size(5, 5), 0); imshow("gaussian_blur", res2); // 中值去噪 medianBlur(src, res3, 3); imshow("median_blur", res3); // 非局部均值去噪 fastNlMeansDenoisingColored(src, res4， 15， 15， 10， 30); imshow("NLmeans_blur", res4); waitKey(0); return 0;&#125;void add_gaussian_noise(Mat &amp;image) &#123; Mat noise = Mat::zeros(image.size(), image.type()); // 产生高斯噪声 randn(noise, (15, 15, 15), (30, 30, 30)); Mat dst; add(image, noise, dst); imshow("gaussian_noise", dst); dst.copyTo(image);&#125; 1234567891011121314151617181920212223242526272829303132import cv2 as cvimport numpy as npdef gaussian_noise(image): noise = np.zeros(image.shape, image.dtype) m = (15, 15, 15) s = (30, 30, 30) cv.randn(noise, m, s) dst = cv.add(image, noise) cv.imshow("gaussian noise", dst) return dstsrc = cv.imread("D:\\code-workspace\\Clion-workspace\\learnOpencv\\images\\test.png")cv.imshow("input", src)h, w = src.shape[:2]src = gaussian_noise(src)result1 = cv.blur(src, (5, 5))cv.imshow("mean_blur", result1)result2 = cv.GaussianBlur(src, (5, 5), 0)cv.imshow("gaussian_blur", result2)result3 = cv.medianBlur(src, 5)cv.imshow("median_blur", result3)result4 = cv.fastNlMeansDenoisingColored(src, None, 15, 15, 10, 30)cv.imshow("NLmeans_blur", result4)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像去噪声</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动编码器(含两个隐藏层)]]></title>
    <url>%2F2019%2F04%2F03%2FAutoEncoder%2F</url>
    <content type="text"><![CDATA[知识点 自动编码器分类 降噪自动编码器代码 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_data# 控制训练过程的参数learning_rate = 0.01training_epochs = 20batch_size = 256display_step = 5examples_to_show = 10# w网络模型参数n_input_units = 784 # 输入神经元数量 MNIST data input (img shape : 28*28)n_hidden1_units = 256 # 编码起第一隐藏层神经元数量（让编码器和解码器都有同样规模的隐藏层n_hidden2_units = 128 # 编码起第二隐藏层神经元数量（让编码器和解码器都有同样规模的隐藏层n_output_units = n_input_units # 解码器输出层神经元数量必须等于输入数据的units数量# 对一个张量进行全面汇总(均值，标准差，最大最小值，直方图)def varible_summaries(var): with tf.name_scope('summaries'): mean = tf.reduce_mean(var) stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean))) tf.summary.scalar('mean', mean) tf.summary.scalar('stddev', stddev) tf.summary.scalar('max', tf.reduce_max(var)) tf.summary.scalar('min', tf.reduce_min(var)) tf.summary.histogram('histogram', var)# 根据输入输出节点数量返回权重def WeightsVarible(n_in, n_out, name_str='weights'): return tf.Variable(tf.random_normal([n_in, n_out]), dtype=tf.float32, name=name_str)# 根据输出节点数量返回偏置def BiasesVarible(n_out, name_str='biases'): return tf.Variable(tf.random_normal([n_out]), dtype=tf.float32, name=name_str)# 构建编码器def Encoder(x_origin, activate_func=tf.nn.sigmoid): # 编码器第一隐藏层 with tf.name_scope('Layer1'): weights = WeightsVarible(n_input_units, n_hidden1_units) biases = BiasesVarible(n_hidden1_units) x_code1 = activate_func(tf.nn.xw_plus_b(x_origin, weights, biases)) varible_summaries(weights) # 编码器第二隐藏层 with tf.name_scope('Layer2'): weights = WeightsVarible(n_hidden1_units, n_hidden2_units) biases = BiasesVarible(n_hidden2_units) x_code = activate_func(tf.nn.xw_plus_b(x_code1, weights, biases)) varible_summaries(weights) return x_code# 构建解吗器def Decoder(x_code, activate_func=tf.nn.sigmoid): # 解码器第一隐藏层 with tf.name_scope('Layer'): weights = WeightsVarible(n_hidden2_units, n_hidden1_units) biases = BiasesVarible(n_hidden1_units) x_decode1 = activate_func(tf.nn.xw_plus_b(x_code, weights, biases)) varible_summaries(weights) # 解码器第二隐藏层 with tf.name_scope('Layer'): weights = WeightsVarible(n_hidden1_units, n_output_units) biases = BiasesVarible(n_output_units) x_decode = activate_func(tf.nn.xw_plus_b(x_decode1, weights, biases)) varible_summaries(weights) return x_decode# 调用上面写的函数构造计算图with tf.Graph().as_default(): # 计算图输入 with tf.name_scope('X_origin'): X_origin = tf.placeholder(tf.float32, [None, n_input_units]) # 构建编码器 with tf.name_scope('Encoder'): X_code = Encoder(X_origin) # 构建解吗器 with tf.name_scope('Decoder'): X_decode = Decoder(X_code) # 定义损失节点 with tf.name_scope('Loss'): Loss = tf.reduce_mean(tf.pow(X_origin - X_decode, 2)) # 定义优化器 with tf.name_scope('Train'): Optimizer = tf.train.RMSPropOptimizer(learning_rate) Train = Optimizer.minimize(Loss) # 为计算图添加损失节点的标量汇总(scalar summary) with tf.name_scope('LossSummary'): tf.summary.scalar('loss', Loss) tf.summary.scalar('learning_rate', learning_rate) # 为计算图添加图像汇总 with tf.name_scope('ImageSummary'): image_origin = tf.reshape(X_origin, [-1, 28, 28, 1]) image_reconstructed = tf.reshape(X_decode, [-1, 28, 28, 1]) tf.summary.image('image_origin', image_origin, 10) tf.summary.image('image_reconstructed', image_reconstructed, 10) # 聚合所有汇总节点 merged_summary = tf.summary.merge_all() init = tf.global_variables_initializer() print("把计算图写入事件文件，在TensorBoard里面查看") writer = tf.summary.FileWriter(logdir='logs', graph=tf.get_default_graph()) writer.flush() # 读取数据集 mnist = input_data.read_data_sets('mnist_data/', one_hot=True) with tf.Session() as sess: sess.run(init) total_batch = int(mnist.train.num_examples / batch_size) for epoch in range(training_epochs): for i in range(total_batch): batch_xs, batch_ys = mnist.train.next_batch(batch_size) _, loss = sess.run([Train, Loss], feed_dict=&#123;X_origin: batch_xs&#125;) if epoch % display_step == 0: print("epoch : %03d, loss = %.3f" % (epoch + 1, loss)) # 运行汇总节点，更新事件文件 summary_str = sess.run(merged_summary, feed_dict=&#123;X_origin: batch_xs&#125;) writer.add_summary(summary_str, epoch) writer.flush() writer.close() print("训练完毕！") # 把训练好的编码器-解码器模型用在测试集上，输出重建后的样本数据 reconstructions = sess.run(X_decode, feed_dict=&#123;X_origin: mnist.test.images[:examples_to_show]&#125;) # 比较原始图像与重建后的图像 f, a = plt.subplots(2, 10, figsize=(10, 2)) for i in range(examples_to_show): a[0][i].imshow(np.reshape(mnist.test.images[i], (28, 28))) a[1][i].imshow(np.reshape(reconstructions[i], (28, 28))) f.show() plt.draw() 结果计算图 测试结果 代码地址github]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>自动编码器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[降噪自动编码器]]></title>
    <url>%2F2019%2F04%2F03%2FDenoiseAutoEncoder%2F</url>
    <content type="text"><![CDATA[知识点 参数初始化问题 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126import numpy as npimport sklearn.preprocessing as prepimport tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_data# Xavier均匀初始化def xavier_init(fan_in, fan_out, constant = 1): low = -constant * np.sqrt(6.0 / (fan_in + fan_out)) high = constant * np.sqrt(6.0 / (fan_in + fan_out)) return tf.random_uniform((fan_in, fan_out), minval=low, maxval=high, dtype=tf.float32)# 加性高斯噪声的自动编码器class AdditiveGaussianNoiseAutoencoder(object): def __init__(self, n_input, n_hidden, transfer_function=tf.nn.softplus, optimizer=tf.train.AdamOptimizer(), scale=0.1): self.n_input = n_input self.n_hidden = n_hidden self.transfer = transfer_function self.training_scale = scale self.weights = dict() # 构建计算图 with tf.name_scope('raw_input'): self.x = tf.placeholder(tf.float32, [None, self.n_input]) with tf.name_scope('NoiseAdder'): self.scale = tf.placeholder(tf.float32) self.noise_x = self.x + self.scale * tf.random_normal((n_input,)) with tf.name_scope('encoder'): self.weights['w1'] = tf.Variable(xavier_init(self.n_input, self.n_hidden), name='weight1') self.weights['b1'] = tf.Variable(tf.zeros([self.n_hidden], dtype=tf.float32), name='bias1') self.hidden = self.transfer(tf.add(tf.matmul(self.noise_x, self.weights['w1']), self.weights['b1'])) with tf.name_scope('reconstruction'): self.weights['w2'] = tf.Variable(tf.zeros([self.n_hidden, self.n_input], dtype=tf.float32), name='weight2') self.weights['b2'] = tf.Variable(tf.zeros([self.n_input], dtype=tf.float32), name='bias2') self.reconstruction = tf.nn.xw_plus_b(self.hidden, self.weights['w2'], self.weights['b2']) # hidden * w2 + b2 with tf.name_scope('loss'): self.cost = 0.5 * tf.reduce_sum(tf.pow(tf.subtract(self.reconstruction, self.x), 2)) with tf.name_scope('train'): self.optimizer = optimizer.minimize(self.cost) init = tf.global_variables_initializer() self.sess = tf.Session() self.sess.run(init) print("begin to run session...") # 在一个批次上训练模型 def partial_fit(self, X): cost, opt = self.sess.run((self.cost, self.optimizer), feed_dict=&#123;self.x: X, self.scale: self.training_scale&#125;) return cost # 在给定样本集合上计算损失（用于测试阶段） def calc_total_cost(self, X): return self.sess.run(self.cost, feed_dict=&#123;self.x: X, self.scale: self.training_scale&#125;) # 返回自编码器隐含层的输出结果，获得抽象后的高阶特征表示 def transform(self, X): return self.sess.run(self.hidden, feed_dict=&#123;self.x: X, self.scale: self.training_scale&#125;) # 将隐藏层的高阶特征作为输入，将其重建为原始输入数据 def generate(self, hidden = None): if hidden == None: hidden = np.random.normal(size=self.weights['b1']) return self.sess.run(self.reconstruction, feed_dict=&#123;self.hidden: hidden&#125;) # 整体运行一遍复原过程，包括提取高阶特征以及重建原始数据，输入原始数据，输出复原后的数据 def reconstruction(self, X): return self.sess.run(self.reconstruction, feed_dict=&#123;self.x: X, self.scale: self.training_scale&#125;) # 获取隐含层的权重 def getWeights(self): return self.sess.run(self.weights['w1']) # 获取隐含层的偏置 def getBiases(self): return self.sess.run(self.weights['b1'])AGN_AutoEncoder = AdditiveGaussianNoiseAutoencoder(n_input=784, n_hidden=200, optimizer=tf.train.AdamOptimizer(learning_rate=0.01), scale=0.01)print("把计算图写入事件文件，在TensorBoard里面查看")writer = tf.summary.FileWriter(logdir='logs', graph=AGN_AutoEncoder.sess.graph)writer.close()# 读取数据集mnist = input_data.read_data_sets('../mnist_data/', one_hot=True)# 使用sklearn.preprocessing 的数据标准化操作(0均值标准差为1) 预处理数据# 首先在训练集上估计均值与方差，然后将其作用到训练集和测试集def standard_scale(x_train, x_test): preprocesser = prep.StandardScaler().fit(x_train) x_train = preprocesser.transform(x_train) x_test = preprocesser.transform(x_test) return x_train, x_test# 获取随机block数据的函数：取一个从0到len(data) - batch_size的随机整数# 以这个随机整数为起始索引，抽出一个batch_size的批次样本def get_random_block_from_data(data, batch_size): start_index = np.random.randint(0, len(data) - batch_size) return data[start_index: start_index + batch_size]# 使用标准化操作变换数据集X_train, X_test = standard_scale(mnist.train.images, mnist.test.images)# 定义训练参数n_samples = int(mnist.train.num_examples)training_epochs = 20batch_size = 128display_step = 1 # 输出训练结果的间隔# 开始训练，每次epoch开始时将avg_cost设为0，计算总共需要的batch数量，# 这里使用的是有放回抽样，所以不能保证每个样本被抽到并参与训练for epoch in range(training_epochs): avg_cost = 0 total_batch = int(n_samples / batch_size) for i in range(total_batch): batch_xs = get_random_block_from_data(X_train, batch_size) cost = AGN_AutoEncoder.partial_fit(batch_xs) avg_cost += cost / batch_size avg_cost /= total_batch if epoch % display_step == 0: print("epoch : %03d, cost = %.3f" % (epoch + 1, avg_cost))# 计算测试集上的costprint("total cost :", str(AGN_AutoEncoder.calc_total_cost(X_test))) 计算图 代码地址github]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>降噪自动编码器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-024-图像噪声]]></title>
    <url>%2F2019%2F04%2F03%2Fopencv-024%2F</url>
    <content type="text"><![CDATA[知识点图像噪声产生的原因很复杂，有的可能是数字信号在传输过程中发生了丢失或者受到干扰，有的是成像设备或者环境本身导致成像质量不稳定，反应到图像上就是图像的亮度与颜色呈现某种程度的不一致性。从噪声的类型上，常见的图像噪声可以分为如下几种： 椒盐噪声，是一种随机在图像中出现的稀疏分布的黑白像素点， 对椒盐噪声一种有效的去噪手段就是图像中值滤波 高斯噪声/符合高斯分布一般会在数码相机的图像采集(acquisition)阶段发生,这个时候它的物理/电/光等各种信号都可能导致产生高斯分布噪声。 均匀分布噪声均匀/规则噪声一般都是因为某些规律性的错误导致的 代码演示 图像椒盐噪声生成 图像高斯噪声生成 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;void add_salt_pepper_noise(Mat &amp;image);void add_gaussian_noise(Mat &amp;image);/* * 噪声生成 */int main() &#123; Mat src = imread("../images/test.png"); Mat src_clone = src.clone(); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; //imshow("input", src); // 产生椒盐噪声 add_salt_pepper_noise(src_clone); // 产生高斯噪声 add_gaussian_noise(src); waitKey(0); return 0;&#125;void add_gaussian_noise(Mat &amp;image) &#123; Mat noise = Mat::zeros(image.size(), image.type()); // 产生高斯噪声 randn(noise, (15,15,15), (30,30,30)); Mat dst; add(image, noise, dst); imshow("gaussian_noise", dst);&#125;void add_salt_pepper_noise(Mat &amp;image) &#123; // 随机数产生器 RNG rng(12345); for (int i = 0; i &lt; 1000; ++i) &#123; int x = rng.uniform(0, image.rows); int y = rng.uniform(0, image.cols); if (i % 2 == 1) &#123; image.at&lt;Vec3b&gt;(y, x) = Vec3b(255, 255, 255); &#125; else &#123; image.at&lt;Vec3b&gt;(y, x) = Vec3b(0, 0, 0); &#125; &#125; imshow("saltp_epper", image);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142import cv2 as cvimport numpy as npdef add_salt_pepper_noise(image): h, w = image.shape[:2] nums = 10000 rows = np.random.randint(0, h, nums, dtype=np.int) cols = np.random.randint(0, w, nums, dtype=np.int) for i in range(nums): if i % 2 == 1: image[rows[i], cols[i]] = (255, 255, 255) else: image[rows[i], cols[i]] = (0, 0, 0) return imagedef gaussian_noise(image): noise = np.zeros(image.shape, image.dtype) m = (15, 15, 15) s = (30, 30, 30) cv.randn(noise, m, s) dst = cv.add(image, noise) cv.imshow("gaussian noise", dst) return dstsrc = cv.imread("D:/vcprojects/images/cos.jpg")h, w = src.shape[:2]copy = np.copy(src)copy = add_salt_pepper_noise(copy)result = np.zeros([h, w*2, 3], dtype=src.dtype)result[0:h,0:w,:] = srcresult[0:h,w:2*w,:] = copycv.putText(result, "original image", (10, 30), cv.FONT_HERSHEY_PLAIN, 2.0, (0, 255, 255), 1)cv.putText(result, "salt pepper image", (w+10, 30), cv.FONT_HERSHEY_PLAIN, 2.0, (0, 255, 255), 1)cv.imshow("salt pepper noise", result)cv.imwrite("D:/result.png", result)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像噪声</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-023-中值模糊]]></title>
    <url>%2F2019%2F04%2F03%2Fopencv-023%2F</url>
    <content type="text"><![CDATA[知识点中值滤波本质上是统计排序滤波器的一种，中值滤波对图像特定噪声类型（椒盐噪声）会取得比较好的去噪效果，也是常见的图像去噪声与增强的方法之一。中值滤波也是窗口在图像上移动，其覆盖的对应ROI区域下，所有像素值排序，取中值作为中心像素点的输出值。 API 代码（c++,python）1234567891011121314151617181920#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;int main() &#123; Mat src = imread("../images/sp_noise.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); Mat dst; medianBlur(src, dst, 5); imshow("medianBlur", dst); waitKey(0); return 0;&#125; 12345678910111213import cv2 as cvimport numpy as npsrc = cv.imread("D:/sp_noise.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)dst = cv.medianBlur(src, 5)cv.imshow("blur ksize=5", dst)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>中值模糊</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-022-图像均值模糊与高斯模糊]]></title>
    <url>%2F2019%2F04%2F02%2Fopencv-022%2F</url>
    <content type="text"><![CDATA[知识点均值模糊 是卷积核的系数完全一致，高斯模糊考虑了中心像素距离的影响，对距离中心像素使用高斯分布公式生成不同的权重系数给卷积核，然后用此卷积核完成图像卷积得到输出结果就是图像高斯模糊之后的输出。 参考：高斯模糊原理-阮一峰 API void GaussianBlur( InputArray src, OutputArray dst, Size ksize, // Ksize为高斯滤波器窗口大小 double sigmaX, // X方向滤波系数 double sigmaY=0, // Y方向滤波系数 int borderType=BORDER_DEFAULT // 默认边缘插值方法)当Size(0, 0)就会从sigmaX开始计算生成高斯卷积核系数，当时size不为零时优先从size开始计算高斯卷积核系数 代码（c++,python）12345678910111213141516171819202122232425262728#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 均值模糊与高斯模糊 */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); Mat dst1, dst2, dst3; blur(src, dst1, Size(5, 5)); GaussianBlur(src, dst2, Size(5, 5), 15, 0); GaussianBlur(src, dst3, Size(15, 15), 15, 0); imshow("blur ksize=5", dst1); imshow("gaussian ksize=5", dst2); imshow("gaussian ksize=15", dst3); waitKey(0); return 0;&#125; 123456789101112131415161718import cv2 as cvimport numpy as npsrc = cv.imread("D:/javaopencv/snow.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)dst1 = cv.blur(src, (5, 5))dst2 = cv.GaussianBlur(src, (5, 5), sigmaX=15)dst3 = cv.GaussianBlur(src, (0, 0), sigmaX=15)cv.imshow("blur ksize=5", dst1)cv.imshow("gaussian ksize=5", dst2)cv.imshow("gaussian sigmax=15", dst3)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>均值模糊与高斯模糊</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-021-图像卷积及均值模糊]]></title>
    <url>%2F2019%2F04%2F01%2Fopencv-021%2F</url>
    <content type="text"><![CDATA[知识点图像卷积可以看成是一个窗口区域在另外一个大的图像上移动，对每个窗口覆盖的区域都进行点乘得到的值作为中心像素点的输出值。窗口的移动是从左到右，从上到下。窗口可以理解成一个指定大小的二维矩阵，里面有预先指定的值。（注意与深度学习卷积的区别） API 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像卷积及均值模糊 */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // 均值模糊 Mat dst; blur(src, dst, Size(15, 15), Point(-1, -1), 4); imshow("dst", dst); // 3x3 均值模糊，自定义版本实现 for (int row = 1; row &lt; h-1; row++) &#123; for (int col = 1; col &lt; w-1; col++) &#123; Vec3b p1 = src.at&lt;Vec3b&gt;(row-1, col-1); Vec3b p2 = src.at&lt;Vec3b&gt;(row-1, col); Vec3b p3 = src.at&lt;Vec3b&gt;(row-1, col+1); Vec3b p4 = src.at&lt;Vec3b&gt;(row, col-1); Vec3b p5 = src.at&lt;Vec3b&gt;(row, col); Vec3b p6 = src.at&lt;Vec3b&gt;(row, col+1); Vec3b p7 = src.at&lt;Vec3b&gt;(row+1, col-1); Vec3b p8 = src.at&lt;Vec3b&gt;(row+1, col); Vec3b p9 = src.at&lt;Vec3b&gt;(row+1, col+1); int b = p1[0] + p2[0] + p3[0] + p4[0] + p5[0] + p6[0] + p7[0] + p8[0] + p9[0]; int g = p1[1] + p2[1] + p3[1] + p4[1] + p5[1] + p6[1] + p7[1] + p8[1] + p9[1]; int r = p1[2] + p2[2] + p3[2] + p4[2] + p5[2] + p6[2] + p7[2] + p8[2] + p9[2]; dst.at&lt;Vec3b&gt;(row, col)[0] = saturate_cast&lt;uchar&gt;(b / 9); dst.at&lt;Vec3b&gt;(row, col)[1] = saturate_cast&lt;uchar&gt;(g / 9); dst.at&lt;Vec3b&gt;(row, col)[2] = saturate_cast&lt;uchar&gt;(r / 9); &#125; &#125; waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526272829303132333435import cv2 as cvimport numpy as npdef custom_blur(src): h, w, ch = src.shape print("h , w, ch", h, w, ch) result = np.copy(src) for row in range(1, h-1, 1): for col in range(1, w-1, 1): v1 = np.int32(src[row-1, col-1]) v2 = np.int32(src[row-1, col]) v3 = np.int32(src[row-1, col+1]) v4 = np.int32(src[row, col-1]) v5 = np.int32(src[row, col]) v6 = np.int32(src[row, col+1]) v7 = np.int32(src[row+1, col-1]) v8 = np.int32(src[row+1, col]) v9 = np.int32(src[row+1, col+1]) b = v1[0] + v2[0] + v3[0] + v4[0] + v5[0] + v6[0] + v7[0] + v8[0] + v9[0]; g = v1[1] + v2[1] + v3[1] + v4[1] + v5[1] + v6[1] + v7[1] + v8[1] + v9[1]; r = v1[2] + v2[2] + v3[2] + v4[2] + v5[2] + v6[2] + v7[2] + v8[2] + v9[2]; result[row, col] = [b//9, g//9, r//9] cv.imshow("result", result)src = cv.imread("D:/vcprojects/images/lena.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)dst = cv.blur(src, (15, 15))cv.imshow("blur", dst)custom_blur(src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像卷积及均值模糊</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy[..., None]的理解]]></title>
    <url>%2F2019%2F04%2F01%2Fnumpy_None%2F</url>
    <content type="text"><![CDATA[numpy数组维度1import numpy as np 12arr_1 = np.array([1, 2, 3, 4])print(arr_1, '\n' , 'shape of arr_1:', arr_1.shape, '， dimension of arr_1:',np.ndim(arr_1)) output: arr_1 = [1 2 3 4] shape of arr_1: (4,) ， dimension of arr_1: 1 12arr_2 = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])print(arr_2, '\n' , 'shape of arr_2:', arr_2.shape, '， dimension of arr_2:',np.ndim(arr_2)) output: arr_2 = [[1 2 3 4] [5 6 7 8]] shape of arr_2: (2, 4) ， dimension of arr_2: 2 12arr_3 = np.array([[[1, 2, 3, 4], [5, 6, 7, 8]], [[1, 2, 3, 4], [5, 6, 7, 8]], [[1, 2, 3, 4], [5, 6, 7, 8]]])print(arr_3, '\n' , 'shape of arr_3:', arr_3.shape, '， dimension of arr_3:',np.ndim(arr_3)) output: arr_3 = [[[1 2 3 4] [5 6 7 8]] [[1 2 3 4] [5 6 7 8]] [[1 2 3 4] [5 6 7 8]]] shape of arr_3: (3, 2, 4) ， dimension of arr_3: 3 numpy数组切片中[…]的理解假设 x 是一个数组，np.ndim(x) == 5 x[1,2,...] == x[1,2,:,:,:] x[...,3] == x[:,:,:,:,3] x[4,...,5,:] == x[4,:,:,5,:] numpy数组切片中None的理解None 的作用就是在相应的位置上增加了一个维度，在这个维度上只有一个元素 假设 x.shape == (a, b)，则 (a, b) ==&gt; [None, :, :] ==&gt; (1, a, b) (a, b) ==&gt; [:, None, :] ==&gt; (a, 1, b) (a, b) ==&gt; [:, :, None] ==&gt; (a, b, 1) 123import numpy as nparr = np.array([[1,2,3],[4,5,6]])print(arr, '\n' , 'shape of arr:', arr.shape, '， dimension of arr:',np.ndim(arr)) output: arr = [[1 2 3] [4 5 6]] shape of arr: (2, 3) ， dimension of arr: 2 12None_1 = arr[None, :, :]print(None_1, '\n' , 'shape of None_1:', None_1.shape, '， dimension of None_1:',np.ndim(None_1)) output: None_1 = [[[1 2 3] [4 5 6]]] shape of None_1: (1, 2, 3) ， dimension of None_1: 3 12None_2 = arr[:, None, :]print(None_2, '\n' , 'shape of None_2:', None_2.shape, '， dimension of None_2:',np.ndim(None_2)) output: None_2 = [[[1 2 3]] [[4 5 6]]] shape of None_2: (2, 1, 3) ， dimension of None_2: 3 12None_3 = arr[:, :, None]print(None_3, '\n' , 'shape of None_3:', None_3.shape, '， dimension of None_3:',np.ndim(None_3)) output: None_3 = [[[1] [2] [3]] [[4] [5] [6]]] shape of None_3: (2, 3, 1) ， dimension of None_3: 3 numpy[…, None]的理解12None_3 = arr[..., None] # 等价于 None_3 = arr[:, :, None]print(None_3, '\n' , 'shape of None_3:', None_3.shape, '， dimension of None_3:',np.ndim(None_3)) output: None_3 = [[[1] [2] [3]] [[4] [5] [6]]] shape of None_3: (2, 3, 1) ， dimension of None_3: 3 12y = np.arange(12).reshape((2,2,3))print(y, '\n' , 'shape of y:', y.shape, '， dimension of y:',np.ndim(y)) output: y = [[[ 0 1 2] [ 3 4 5]] [[ 6 7 8] [ 9 10 11]]] shape of y: (2, 2, 3) ， dimension of y: 3 12y = y[..., None]print(y, '\n' , 'shape of y:', y.shape, '， dimension of y:',np.ndim(y)) output: y = [[[[ 0] [ 1] [ 2]] [[ 3] [ 4] [ 5]]] [[[ 6] [ 7] [ 8]] [[ 9] [10] [11]]]] shape of y: (2, 2, 3, 1) ， dimension of y: 4]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-020-图像直方图反向投影]]></title>
    <url>%2F2019%2F03%2F30%2Fopencv-020%2F</url>
    <content type="text"><![CDATA[知识点文字解释图像直方图反向投影是通过构建指定模板图像的二维直方图空间与目标的二维直方图空间，进行直方图数据归一化之后， 进行比率操作，对所有得到非零数值，生成查找表对原图像进行像素映射之后，再进行图像模糊输出的结果。 直方图反向投影流程 计算直方图 计算比率R LUT查找表 卷积模糊 归一化输出 API 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;void backProjection_demo(Mat &amp;mat, Mat &amp;model);/* * 图像直方图反向投影 */int main() &#123; Mat src = imread("../images/target.png"); Mat model = imread("../images/sample.png"); if (src.empty() || model.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; namedWindow("model", WINDOW_NORMAL); imshow("input", src); imshow("model", model); backProjection_demo(src, model); waitKey(0); return 0;&#125;void backProjection_demo(Mat &amp;image, Mat &amp;model) &#123; Mat image_hsv, model_hsv; cvtColor(image, image_hsv, COLOR_BGR2HSV); cvtColor(model, model_hsv, COLOR_BGR2HSV); // 定义直方图参数与属性 int h_bins = 32, s_bins = 32; int histSize[] = &#123;h_bins, s_bins&#125;; float h_ranges[] = &#123;0, 180&#125;, s_ranges[] = &#123;0, 256&#125;; const float* ranges[] = &#123;h_ranges, s_ranges&#125;; int channels[] = &#123;0, 1&#125;; Mat roiHist; calcHist(&amp;model_hsv, 1, channels, Mat(), roiHist, 2, histSize, ranges); normalize(roiHist, roiHist, 0, 255, NORM_MINMAX, -1, Mat()); MatND backproj; calcBackProject(&amp;image_hsv, 1, channels, roiHist, backproj, ranges); imshow("BackProj", backproj);&#125; 1234567891011121314151617181920212223242526272829303132333435363738import cv2 as cvimport numpy as npfrom matplotlib import pyplot as pltdef back_projection_demo(): sample = cv.imread("D:/javaopencv/sample.png") # hist2d_demo(sample) target = cv.imread("D:/javaopencv/target.png") # hist2d_demo(target) roi_hsv = cv.cvtColor(sample, cv.COLOR_BGR2HSV) target_hsv = cv.cvtColor(target, cv.COLOR_BGR2HSV) # show images cv.imshow("sample", sample) cv.imshow("target", target) roiHist = cv.calcHist([roi_hsv], [0, 1], None, [32, 32], [0, 180, 0, 256]) cv.normalize(roiHist, roiHist, 0, 255, cv.NORM_MINMAX) dst = cv.calcBackProject([target_hsv], [0, 1], roiHist, [0, 180, 0, 256], 1) cv.imshow("backProjectionDemo", dst)def hist2d_demo(image): hsv = cv.cvtColor(image, cv.COLOR_BGR2HSV) hist = cv.calcHist([hsv], [0, 1], None, [32, 32], [0, 180, 0, 256]) dst = cv.resize(hist, (400, 400)) cv.imshow("image", image) cv.imshow("hist", dst) plt.imshow(hist, interpolation='nearest') plt.title("2D Histogram") plt.show()back_projection_demo()cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像直方图反向投影</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-019-图像直方图比较]]></title>
    <url>%2F2019%2F03%2F30%2Fopencv-019%2F</url>
    <content type="text"><![CDATA[知识点图像直方图比较，就是计算两幅图像的直方图数据，比较两组数据的相似性，从而得到两幅图像之间的相似程度，直方图比较在早期的CBIR(以图搜图)中是应用很常见的技术手段，通常会结合边缘处理、词袋等技术一起使用。APIcompareHist(hist1, hist2, method)常见比较方法有 相关性(常用) 卡方 交叉 巴氏(常用) 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像直方图比较 */int main() &#123; Mat src1 = imread("../images/left01.jpg"); Mat src2 = imread("../images/left13.jpg"); if (src1.empty() || src2.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input1", src1); imshow("input2", src2); // 一般在HSV色彩空间进行计算 Mat hsv1, hsv2; cvtColor(src1, hsv1, COLOR_BGR2HSV); cvtColor(src2, hsv2, COLOR_BGR2HSV); int h_bins = 60, s_bins = 64; int histSize[] = &#123;h_bins, s_bins&#125;; float h_ranges[] = &#123;0, 180&#125;; float s_ranges[] = &#123;0, 256&#125;; const float* ranges[] = &#123;h_ranges, s_ranges&#125;; int channels[] = &#123;0, 1&#125;; Mat hist1, hist2; calcHist(&amp;hsv1, 1, channels, Mat(), hist1, 2, histSize, ranges); calcHist(&amp;hsv2, 1, channels, Mat(), hist2, 2, histSize, ranges); normalize(hist1, hist1, 0, 1, NORM_MINMAX, -1, Mat()); normalize(hist2, hist2, 0, 1, NORM_MINMAX, -1, Mat()); // 比较 double src1_src2_1 = compareHist(hist1, hist2, HISTCMP_CORREL); double src1_src2_2 = compareHist(hist1, hist2, HISTCMP_BHATTACHARYYA); printf("HISTCMP_CORREL : %.2f\n", src1_src2_1); printf("HISTCMP_BHATTACHARYYA : %.2f\n", src1_src2_1); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import cv2 as cvimport numpy as npsrc1 = cv.imread("D:/vcprojects/images/m1.png")src2 = cv.imread("D:/vcprojects/images/m2.png")src3 = cv.imread("D:/vcprojects/images/flower.png")src4 = cv.imread("D:/vcprojects/images/wm_test.png")cv.imshow("input1", src1)cv.imshow("input2", src2)cv.imshow("input3", src3)cv.imshow("input4", src4)hsv1 = cv.cvtColor(src1, cv.COLOR_BGR2HSV)hsv2 = cv.cvtColor(src2, cv.COLOR_BGR2HSV)hsv3 = cv.cvtColor(src3, cv.COLOR_BGR2HSV)hsv4 = cv.cvtColor(src4, cv.COLOR_BGR2HSV)hist1 = cv.calcHist([hsv1], [0, 1], None, [60, 64], [0, 180, 0, 256])hist2 = cv.calcHist([hsv2], [0, 1], None, [60, 64], [0, 180, 0, 256])hist3 = cv.calcHist([hsv3], [0, 1], None, [60, 64], [0, 180, 0, 256])hist4 = cv.calcHist([hsv4], [0, 1], None, [60, 64], [0, 180, 0, 256])cv.normalize(hist1, hist1, 0, 1.0, cv.NORM_MINMAX, dtype=np.float32)cv.normalize(hist2, hist2, 0, 1.0, cv.NORM_MINMAX)cv.normalize(hist3, hist3, 0, 1.0, cv.NORM_MINMAX)cv.normalize(hist4, hist4, 0, 1.0, cv.NORM_MINMAX)methods = [cv.HISTCMP_CORREL, cv.HISTCMP_CHISQR, cv.HISTCMP_INTERSECT, cv.HISTCMP_BHATTACHARYYA]str_method = ""for method in methods: src1_src2 = cv.compareHist(hist1, hist2, method) src3_src4 = cv.compareHist(hist3, hist4, method) if method == cv.HISTCMP_CORREL: str_method = "Correlation" if method == cv.HISTCMP_CHISQR: str_method = "Chi-square" if method == cv.HISTCMP_INTERSECT: str_method = "Intersection" if method == cv.HISTCMP_BHATTACHARYYA: str_method = "Bhattacharyya" print("%s src1_src2 = %.2f, src3_src4 = %.2f"%(str_method, src1_src2, src3_src4))cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像直方图比较</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-018-图像直方图均衡化]]></title>
    <url>%2F2019%2F03%2F29%2Fopencv-018%2F</url>
    <content type="text"><![CDATA[知识点图像直方图均衡化可以用于图像增强、对输入图像进行直方图均衡化处理，提升后续对象检测的准确率，在OpenCV人脸检测的代码演示中已经很常见。此外对医学影像图像与卫星遥感图像也经常通过直方图均衡化来提升图像质量。 API equalizeHist(src, dst) 代码（c++,python）123456789101112131415161718192021222324#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像直方图均衡化 */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; Mat gray, dst; cvtColor(src, gray, COLOR_BGR2GRAY); equalizeHist(gray, dst); imshow("input", gray); imshow("eq", dst); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324252627282930313233import cv2 as cvimport numpy as npfrom matplotlib import pyplot as pltdef custom_hist(gray): h, w = gray.shape hist = np.zeros([256], dtype=np.int32) for row in range(h): for col in range(w): pv = gray[row, col] hist[pv] += 1 y_pos = np.arange(0, 256, 1, dtype=np.int32) plt.bar(y_pos, hist, align='center', color='r', alpha=0.5) plt.xticks(y_pos, y_pos) plt.ylabel('Frequency') plt.title('Histogram') plt.show()src = cv.imread("../images/test.png")gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", gray)dst = cv.equalizeHist(gray)cv.imshow("eh", dst)custom_hist(gray)custom_hist(dst)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像直方图均衡化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-017-图像直方图]]></title>
    <url>%2F2019%2F03%2F28%2Fopencv-017%2F</url>
    <content type="text"><![CDATA[知识点图像直方图的解释图像直方图是图像像素值的统计学特征、计算代价较小，具有图像平移、旋转、缩放不变性等众多优点，广泛地应用于图像处理的各个领域，特别是灰度图像的阈值分割、基于颜色的图像检索以及图像分类、反向投影跟踪。常见的分为 灰度直方图 颜色直方图 Bins是指直方图的大小范围， 对于像素值取值在0～255之间的，最少有256个bin，此外还可以有16、32、48、128等，256除以bin的大小应该是整数倍。 OpenCV中相关APIcalcHist(&amp;bgr_plane[0], 1, 0, Mat(), b_hist, 1, bins, ranges);cv.calcHist([image], [i], None, [256], [0, 256]) 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;#include &lt;vector&gt;using namespace std;using namespace cv;const int bins = 256;Mat src;const char *winTitle = "input image";void showHistogram();/* * 图像直方图 */int main() &#123; src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow(winTitle, src); showHistogram(); waitKey(0); return 0;&#125;void showHistogram() &#123; // 三通道分离 vector&lt;Mat&gt; bgr_plane; split(src, bgr_plane); // 定义参数变量 const int channels[1] = &#123;0&#125;; const int bins[1] = &#123;256&#125;; float hranges[2] = &#123;0, 255&#125;; const float *ranges[1] = &#123;hranges&#125;; Mat b_hist, g_hist, r_hist; // 计算三通道直方图 calcHist(&amp;bgr_plane[0], 1, 0, Mat(), b_hist, 1, bins, ranges); calcHist(&amp;bgr_plane[1], 1, 0, Mat(), g_hist, 1, bins, ranges); calcHist(&amp;bgr_plane[2], 1, 0, Mat(), r_hist, 1, bins, ranges); /* * 显示直方图 */ int hist_w = 512; int hist_h = 400; int bin_w = cvRound((double) hist_w / bins[0]); Mat histImage = Mat::zeros(hist_h, hist_w, CV_8UC3); // 归一化直方图数据 normalize(b_hist, b_hist, 0, histImage.rows, NORM_MINMAX, -1); normalize(g_hist, g_hist, 0, histImage.rows, NORM_MINMAX, -1); normalize(r_hist, r_hist, 0, histImage.rows, NORM_MINMAX, -1); // 绘制直方图曲线 for (int i = 1; i &lt; bins[0]; ++i) &#123; line(histImage, Point(bin_w * (i - 1), hist_h - cvRound(b_hist.at&lt;float&gt;(i - 1))), Point(bin_w * (i), hist_h - cvRound(b_hist.at&lt;float&gt;(i))), Scalar(255, 0, 0), 2, 8, 0); line(histImage, Point(bin_w * (i - 1), hist_h - cvRound(g_hist.at&lt;float&gt;(i - 1))), Point(bin_w * (i), hist_h - cvRound(g_hist.at&lt;float&gt;(i))), Scalar(0, 255, 0), 2, 8, 0); line(histImage, Point(bin_w * (i - 1), hist_h - cvRound(r_hist.at&lt;float&gt;(i - 1))), Point(bin_w * (i), hist_h - cvRound(r_hist.at&lt;float&gt;(i))), Scalar(0, 0, 255), 2, 8, 0); &#125; imshow("Histogram", histImage);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142import cv2 as cvimport numpy as npfrom matplotlib import pyplot as pltdef custom_hist(gray): h, w = gray.shape hist = np.zeros([256], dtype=np.int32) for row in range(h): for col in range(w): pv = gray[row, col] hist[pv] += 1 y_pos = np.arange(0, 256, 1, dtype=np.int32) plt.bar(y_pos, hist, align='center', color='r', alpha=0.5) plt.xticks(y_pos, y_pos) plt.ylabel('Frequency') plt.title('Histogram') # plt.plot(hist, color='r') # plt.xlim([0, 256]) plt.show()def image_hist(image): cv.imshow("input", image) color = ('blue', 'green', 'red') for i, color in enumerate(color): hist = cv.calcHist([image], [i], None, [256], [0, 256]) plt.plot(hist, color=color) plt.xlim([0, 256]) plt.show()src = cv.imread("../images/test.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)cv.imshow("input", gray)#custom_hist(gray)image_hist(src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像直方图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-016-图像ROI与ROI操作]]></title>
    <url>%2F2019%2F03%2F28%2Fopencv-016%2F</url>
    <content type="text"><![CDATA[知识点图像的ROI(region of interest)是指图像中感兴趣区域、在OpenCV中图像设置图像ROI区域，实现只对ROI区域操作。 矩形ROI区域提取 矩形ROI区域copy 不规则ROI区域 ROI区域mask生成 像素位 and操作 提取到ROI区域 加背景or操作 add 背景与ROI区域 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * ROI及相关操作 */int main() &#123; Mat src = imread("../images/test.png"); imshow("input", src); int h = src.rows; int w = src.cols; // 获取ROI int cy = h / 2; int cx = w / 2; Rect rect(cx - 100, cy - 100, 200, 200); // 注意：roi 与 src指向同一块内存区域，改变roi,src也会改变 Mat roi = src(rect); imshow("roi", roi); // 人物背景图，换背景 // load image Mat image = imread("../images/boy.jpg"); imshow("input", image); // generate mask Mat hsv, mask, mask_not; cvtColor(image, hsv, COLOR_BGR2HSV); inRange(hsv, Scalar(35, 43, 46), Scalar(99, 255, 255), mask); imshow("mask", mask); // extract person Mat person; bitwise_not(mask, mask_not); imshow("mask_not", mask_not); bitwise_and(image, image, person, mask_not); imshow("person", person); // gengerate background Mat background = Mat::zeros(image.size(), image.type()); background.setTo(Scalar(255, 0 ,0)); imshow("background", background); // combine background + person Mat dst; bitwise_or(person, background, dst, mask); add(dst, person, dst); imshow("dst", dst); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import cv2 as cvimport numpy as npsrc = cv.imread("D:/javaopencv/dahlia_4.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)h, w = src.shape[:2]# 获取ROIcy = h//2cx = w//2roi = src[cy-100:cy+100,cx-100:cx+100,:]cv.imshow("roi", roi)# copy ROIimage = np.copy(roi)# modify ROIroi[:, :, 0] = 0cv.imshow("result", src)# modify copy roiimage[:, :, 2] = 0cv.imshow("result", src)cv.imshow("copy roi", image)# example with ROI - generate masksrc2 = cv.imread("D:/javaopencv/tinygreen.png");cv.imshow("src2", src2)hsv = cv.cvtColor(src2, cv.COLOR_BGR2HSV)mask = cv.inRange(hsv, (35, 43, 46), (99, 255, 255))# extract person ROImask = cv.bitwise_not(mask)person = cv.bitwise_and(src2, src2, mask=mask);# generate backgroundresult = np.zeros(src2.shape, src2.dtype)result[:,:,0] = 255# combine background + personmask = cv.bitwise_not(mask)dst = cv.bitwise_or(person, result, mask=mask)dst = cv.add(dst, person)cv.imshow("dst", dst)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像ROI与ROI操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-015-绘制几何形状及随机数的生成]]></title>
    <url>%2F2019%2F03%2F28%2Fopencv-015%2F</url>
    <content type="text"><![CDATA[知识点绘制几何形状 绘制直线 绘制圆 绘制矩形 绘制椭圆 填充几何形状 OpenCV没有专门的填充方法，只是把绘制几何形状时候的线宽thickness参数值设置为负数即表示填充该几何形状或者使用参数CV_FILLED 随机数方法：RNG 表示OpenCV C++版本中的随机数对象，rng.uniform(a, b)生成[a, b)之间的随机数，包含a，但是不包含b。 np.random.rand() 表示numpy中随机数生成，生成浮点数0～1的随机数, 包含0，不包含1。 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 绘制几何形状及随机数 */int main() &#123; Mat image = Mat::zeros(Size(512, 512), CV_8UC3); Rect rect(100, 100, 200, 200); // 绘制 rectangle(image, rect, Scalar(255, 0, 0), 2, LINE_8, 0); circle(image, Point(256, 256), 50, Scalar(0, 255, 0), 2, LINE_8, 0); ellipse(image, Point(256, 256), Size(150, 50), 360, 0, 360, Scalar(0, 0, 255), 2, LINE_8, 0); imshow("image_draw", image); // 填充 thickness=-1 or FILLED rectangle(image, rect, Scalar(255, 0, 0), FILLED, LINE_8, 0); ellipse(image, Point(256, 256), Size(150, 50), 360, 0, 360, Scalar(0, 0, 255), FILLED, LINE_8, 0); circle(image, Point(256, 256), 50, Scalar(0, 255, 0), -1, LINE_8, 0); imshow("image_fill", image); // 随机数 RNG rng(0xFFFFFF); image.setTo(Scalar(0, 0, 0)); Mat image_copy = image.clone(); for (int i = 0; i &lt; 100000; ++i) &#123; int x1 = rng.uniform(0, 512); int y1 = rng.uniform(0, 512); int x2 = rng.uniform(0, 512); int y2 = rng.uniform(0, 512); int b = rng.uniform(0, 256); int g = rng.uniform(0, 256); int r = rng.uniform(0, 256); rect.x = x1; rect.y = y1; rect.width = x2 - x1; rect.height = y2 - y1; // LINE_AA 反锯齿 line(image, Point(x1, y1), Point(x2, y2), Scalar(b, g, r), 1, LINE_AA, 0); rectangle(image_copy, rect, Scalar(b, g, r), 1, LINE_AA, 0); imshow("image_line", image); imshow("image_rect", image_copy); char c = waitKey(20); if (c == 27)&#123; // ESC break; &#125; &#125; waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526272829303132import cv2 as cvimport numpy as npimage = np.zeros((512, 512, 3), dtype=np.uint8)cv.rectangle(image, (100, 100), (300, 300), (255, 0, 0), 2, cv.LINE_8, 0)cv.circle(image, (256, 256), 50, (0, 0, 255), 2, cv.LINE_8, 0)cv.ellipse(image, (256, 256), (150, 50), 360, 0, 360, (0, 255, 0), 2, cv.LINE_8, 0)cv.imshow("image", image)cv.waitKey(0)for i in range(100000): image[:,:,:]= 0 x1 = np.random.rand() * 512 y1 = np.random.rand() * 512 x2 = np.random.rand() * 512 y2 = np.random.rand() * 512 b = np.random.randint(0, 256) g = np.random.randint(0, 256) r = np.random.randint(0, 256) # cv.line(image, (np.int(x1), np.int(y1)), (np.int(x2), np.int(y2)), (b, g, r), 4, cv.LINE_8, 0) cv.rectangle(image, (np.int(x1), np.int(y1)), (np.int(x2), np.int(y2)), (b, g, r), 1, cv.LINE_8, 0) cv.imshow("image", image) c = cv.waitKey(20) if c == 27: break # ESCä¸cv.imshow("image", image)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>绘制几何形状</tag>
        <tag>随机数生成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-014-使用resize进行图像插值(Image Interpolation)]]></title>
    <url>%2F2019%2F03%2F27%2Fopencv-014%2F</url>
    <content type="text"><![CDATA[知识点最常见四种插值算法 INTER_NEAREST = 0 #最近邻插值，速度快，没考虑周围像素影响 INTER_LINEAR = 1 #双线性插值 INTER_CUBIC = 2 #双立方插值，高质量 INTER_LANCZOS4 = 4 #高质量 关于这四种插值算法的详细代码实现与解释 三种常见双立方插值算法-CSDN 图像放缩之双立方插值 图像放缩之双线性内插值 Lanczos采样放缩算法 相关的应用场景几何变换、透视变换、插值计算新像素 API resize(InputArray src, OutputArray dst, Size dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR ) 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像插值 */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); int h = src.rows; int w = src.cols; float fx = 0.0, fy = 0.0; Mat dst = Mat::zeros(src.size(), src.type()); Size S(w * 2, h * 2); resize(src, dst, S, fx, fy, INTER_NEAREST); imshow("INTER_NEAREST", dst); resize(src, dst, S, fx, fy, INTER_LINEAR); imshow("INTER_LINEAR", dst); resize(src, dst, S, fx, fy, INTER_CUBIC); imshow("INTER_CUBIC", dst); resize(src, dst, S, fx, fy, INTER_LANCZOS4); imshow("INTER_LANCZOS4", dst); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324import cv2 as cvsrc = cv.imread("D:/vcprojects/images/test.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)h, w = src.shape[:2]print(h, w)dst = cv.resize(src, (w*2, h*2), fx=0.75, fy=0.75, interpolation=cv.INTER_NEAREST)cv.imshow("INTER_NEAREST", dst)dst = cv.resize(src, (w*2, h*2), interpolation=cv.INTER_LINEAR)cv.imshow("INTER_LINEAR", dst)dst = cv.resize(src, (w*2, h*2), interpolation=cv.INTER_CUBIC)cv.imshow("INTER_CUBIC", dst)dst = cv.resize(src, (w*2, h*2), interpolation=cv.INTER_LANCZOS4)cv.imshow("INTER_LANCZOS4", dst)cv.warpAffine()cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像插值</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-013-图像翻转(Image Flip)]]></title>
    <url>%2F2019%2F03%2F27%2Fopencv-013%2F</url>
    <content type="text"><![CDATA[知识点图像翻转的本质像素映射，OpenCV支持三种图像翻转方式 X轴翻转，flipcode = 0 Y轴翻转, flipcode = 1 XY轴翻转, flipcode = -1 相关的APIflip(src, dst, flipcode) src输入参数 dst 翻转后图像 flipcode 应用：摄像头拍摄后经常需要翻转 代码（c++,python）1234567891011121314151617181920212223242526272829303132#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像翻转 */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); Mat dst; // X轴 倒影 flip(src, dst, 0); imshow("x_flip", dst); // Y轴 镜像 flip(src, dst, 1); imshow("y_flip", dst); // XY轴 对角 flip(src, dst, -1); imshow("xy_flip", dst); waitKey(0); return 0;&#125; 123456789101112131415161718192021222324252627282930import cv2 as cvimport numpy as npsrc = cv.imread("D:/vcprojects/images/test.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# X Flip 倒影dst1 = cv.flip(src, 0);cv.imshow("x-flip", dst1);# Y Flip 镜像dst2 = cv.flip(src, 1);cv.imshow("y-flip", dst2);# XY Flip 对角dst3 = cv.flip(src, -1);cv.imshow("xy-flip", dst3);# custom y-fliph, w, ch = src.shapedst = np.zeros(src.shape, src.dtype)for row in range(h): for col in range(w): b, g, r = src[row, col] dst[row, w - col - 1] = [b, g, r]cv.imshow("custom-y-flip", dst)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像翻转</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-012-视频读写]]></title>
    <url>%2F2019%2F03%2F26%2Fopencv-012%2F</url>
    <content type="text"><![CDATA[知识点VideoCapture 视频文件读取、摄像头读取、视频流读取VideoWriter 视频写出、文件保存、 CAP_PROP_FRAME_HEIGHT #高度 CAP_PROP_FRAME_WIDTH #宽度 CAP_PROP_FRAME_COUNT #数量 CAP_PROP_FPS #帧率 不支持音频编码与解码保存，不是一个音视频处理的库！主要是分析与解析视频内容。保存文件最大支持单个文件为2G。 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 视频读写 */int main() &#123; // 打开摄像头 // VideoCapture capture(0); // 打开视频文件 VideoCapture capture; capture.open("../images/vtest.avi"); if (!capture.isOpened()) &#123; cout &lt;&lt; "could not load video.." &lt;&lt; endl; return -1; &#125; Size S = Size((int) capture.get(CAP_PROP_FRAME_WIDTH), (int) capture.get(CAP_PROP_FRAME_HEIGHT)); int fps = capture.get(CAP_PROP_FPS); cout &lt;&lt; "capture fps: " &lt;&lt; fps &lt;&lt; endl; VideoWriter writer("D:/test.mp4", cv::VideoWriter::fourcc('D', 'I','V','X'), fps, S, true); Mat frame; while(capture.read(frame))&#123; imshow("input", frame); writer.write(frame); char c = waitKey(50); if(c == 27)&#123; break; &#125; &#125; capture.release(); writer.release(); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526import cv2 as cvimport numpy as npcapture = cv.VideoCapture("D:/vcprojects/images/768x576.avi")# capture = cv.VideoCapture(0) 打开摄像头height = capture.get(cv.CAP_PROP_FRAME_HEIGHT)width = capture.get(cv.CAP_PROP_FRAME_WIDTH)count = capture.get(cv.CAP_PROP_FRAME_COUNT)fps = capture.get(cv.CAP_PROP_FPS)print(height, width, count, fps)out = cv.VideoWriter("D:/test.mp4", cv.VideoWriter_fourcc('D', 'I', 'V', 'X'), 15, (np.int(width), np.int(height)), True)while True: ret, frame = capture.read() if ret is True: cv.imshow("video-input", frame) out.write(frame) c = cv.waitKey(50) if c == 27: # ESC break else: breakcapture.release()out.release() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>视频读写</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-011-图像像素归一化]]></title>
    <url>%2F2019%2F03%2F25%2Fopencv-011%2F</url>
    <content type="text"><![CDATA[知识点OpenCV中提供了四种归一化的方法 NORM_MINMAX NORM_INF NORM_L1 NORM_L2 最常用的就是NORM_MINMAX归一化方法. 四种归一化方法示例 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像像素归一化 */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; // imshow("input", src); Mat gray; cvtColor(src, gray, COLOR_BGR2GRAY); imshow("input", gray); // 显示图像用uchar类型，计算时转为float类型 gray.convertTo(gray, CV_32F); // NORM_MINMAX Mat dst = Mat::zeros(gray.size(), CV_32FC1); normalize(gray, dst, 1.0, 0, NORM_MINMAX); Mat res = dst * 255; res.convertTo(dst, CV_8UC1); // 显示图像用uchar类型 imshow("NORM_MINMAX", dst); // scale and shift by NORM_INF normalize(gray, dst, 1.0, 0, NORM_INF); res = dst * 255; res.convertTo(dst, CV_8UC1); imshow("NORM_INF", dst); // scale and shift by NORM_L1 normalize(gray, dst, 1.0, 0, NORM_L1); res = dst * 10000000; res.convertTo(dst, CV_8UC1); imshow("NORM_L1", dst); // scale and shift by NORM_L2 normalize(gray, dst, 1.0, 0, NORM_L2); res = dst * 10000; res.convertTo(dst, CV_8UC1); imshow("NORM_L2", dst); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526272829303132333435363738import cv2 as cvimport numpy as npsrc = cv.imread("D:/vcprojects/images/test.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)# 转换为浮点数类型数组gray = np.float32(gray)print(gray)# scale and shift by NORM_MINMAXdst = np.zeros(gray.shape, dtype=np.float32)cv.normalize(gray, dst=dst, alpha=0, beta=1.0, norm_type=cv.NORM_MINMAX)print(dst)cv.imshow("NORM_MINMAX", np.uint8(dst*255))# scale and shift by NORM_INFdst = np.zeros(gray.shape, dtype=np.float32)cv.normalize(gray, dst=dst, alpha=1.0, beta=0, norm_type=cv.NORM_INF)print(dst)cv.imshow("NORM_INF", np.uint8(dst*255))# scale and shift by NORM_L1dst = np.zeros(gray.shape, dtype=np.float32)cv.normalize(gray, dst=dst, alpha=1.0, beta=0, norm_type=cv.NORM_L1)print(dst)cv.imshow("NORM_L1", np.uint8(dst*10000000))# scale and shift by NORM_L2dst = np.zeros(gray.shape, dtype=np.float32)cv.normalize(gray, dst=dst, alpha=1.0, beta=0, norm_type=cv.NORM_L2)print(dst)cv.imshow("NORM_L2", np.uint8(dst*10000))cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像像素归一化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-010-图像像素值统计及应用（普通图像转化为二值图像）]]></title>
    <url>%2F2019%2F03%2F25%2Fopencv-010%2F</url>
    <content type="text"><![CDATA[知识点 最小(min) 最大(max) 均值(mean) 标准方差(standard deviation) API知识点 最大最小值minMaxLoc 计算均值与标准方差meanStdDev 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像像素值统计及应用（普通图像转化为二值图像） */int main() &#123; Mat src_bgr = imread("../images/test.png"); Mat src_gray; cvtColor(src_bgr, src_gray, COLOR_BGR2GRAY); if (src_bgr.empty() || src_gray.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input_bgr", src_bgr); // 计算灰度图像的最大最小值 double minVal, maxVal; Point minLoc, maxLoc; minMaxLoc(src_gray, &amp;minVal, &amp;maxVal, &amp;minLoc, &amp;maxLoc); cout &lt;&lt; "paramenters of src_gray:" &lt;&lt; endl; printf("min:%.2f, max:%.2f \n", minVal, maxVal); printf("min loc: (%d, %d) \n", minLoc.x, minLoc.y); printf("max loc: (%d, %d) \n", maxLoc.x, maxLoc.y); // 普通图像转二值图像 Mat mean, stddev; meanStdDev(src_bgr, mean, stddev); cout &lt;&lt; "paramenters of src_bgr:" &lt;&lt; endl; printf("blue channel mean:%.2f, stddev: %.2f \n", mean.at&lt;double&gt;(0, 0), stddev.at&lt;double&gt;(0, 0)); printf("green channel mean:%.2f, stddev: %.2f \n", mean.at&lt;double&gt;(1, 0), stddev.at&lt;double&gt;(1, 0)); printf("red channel mean:%.2f, stddev: %.2f \n", mean.at&lt;double&gt;(2, 0), stddev.at&lt;double&gt;(2, 0)); for (int row = 0; row &lt; src_bgr.rows; ++row) &#123; for (int col = 0; col &lt; src_bgr.cols; ++col) &#123; Vec3b bgr = src_bgr.at&lt;Vec3b&gt;(row, col); bgr[0] = bgr[0] &lt; mean.at&lt;double&gt;(0, 0) ? 0 : 255; bgr[1] = bgr[1] &lt; mean.at&lt;double&gt;(1, 0) ? 0 : 255; bgr[2] = bgr[2] &lt; mean.at&lt;double&gt;(2, 0) ? 0 : 255; src_bgr.at&lt;Vec3b&gt;(row, col) = bgr; &#125; &#125; imshow("binary", src_bgr); waitKey(0); return 0;&#125; 1234567891011121314151617181920import cv2 as cvimport numpy as npsrc = cv.imread("../images/test.png", cv.IMREAD_GRAYSCALE)cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)min, max, minLoc, maxLoc = cv.minMaxLoc(src)print("min: %.2f, max: %.2f"% (min, max))print("min loc: ", minLoc)print("max loc: ", maxLoc)means, stddev = cv.meanStdDev(src)print("mean: %.2f, stddev: %.2f"% (means, stddev))src[np.where(src &lt; means)] = 0src[np.where(src &gt; means)] = 255cv.imshow("binary", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像像素值统计</tag>
        <tag>普通图像转化为二值图像</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-009-色彩空间及其应用（提取图像的前景和背景）]]></title>
    <url>%2F2019%2F03%2F25%2Fopencv-009%2F</url>
    <content type="text"><![CDATA[知识点 RGB色彩空间 HSV色彩空间 -维基百科 ### 直方图算法中常用 YUV色彩空间 YCrCb色彩空间 # 皮肤检测常用 API知识点 色彩空间转换cvtColor 提取指定色彩范围区域inRange 代码（c++,python）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 色彩空间及其应用 */int main() &#123; Mat src = imread("../images/test.png"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); // RGB ==&gt; HSV YUV YCrCb Mat hsv, yuv, ycrcb; cvtColor(src, hsv, COLOR_BGR2HSV); cvtColor(src, yuv, COLOR_BGR2YUV); cvtColor(src, ycrcb, COLOR_BGR2YCrCb); imshow("hsv", hsv); imshow("yuv", yuv); imshow("ycrcb", ycrcb); /* * 提取图像前景和背景 */ Mat src2 = imread("../images/boy.jpg"); imshow("input boy", src2); cvtColor(src2, hsv, COLOR_BGR2HSV); // 从HSV表中查到绿色的最低值和最高值，建立掩模 Mat mask, mask_not; inRange(hsv, Scalar(35, 43, 46), Scalar(77, 255, 255), mask); imshow("mask", mask); Mat fg, bg; // 提取背景 bitwise_and(src2, src2, bg, mask); // 提取前景 bitwise_not(mask, mask_not); imshow("mask_not", mask_not); bitwise_and(src2, src2, fg, mask_not); imshow("background", bg); imshow("foreground" ,fg); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425262728import cv2 as cvsrc = cv.imread("../images/test.png")cv.namedWindow("rgb", cv.WINDOW_AUTOSIZE)cv.imshow("rgb", src)# RGB to HSVhsv = cv.cvtColor(src, cv.COLOR_BGR2HSV)cv.imshow("hsv", hsv)# RGB to YUVyuv = cv.cvtColor(src, cv.COLOR_BGR2YUV)cv.imshow("yuv", yuv)# RGB to YUVycrcb = cv.cvtColor(src, cv.COLOR_BGR2YCrCb)cv.imshow("ycrcb", ycrcb)src2 = cv.imread("../images/boy.jpg");cv.imshow("src2", src2)hsv = cv.cvtColor(src2, cv.COLOR_BGR2HSV)mask = cv.inRange(hsv, (35, 43, 46), (99, 255, 255))dst = cv.bitwise_and(src2, src2, mask=mask)cv.imshow("mask", mask)cv.imshow("dst", dst)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>色彩空间</tag>
        <tag>提取图像前景和背景</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-008-图像通道的分离与合并]]></title>
    <url>%2F2019%2F03%2F24%2Fopencv-008%2F</url>
    <content type="text"><![CDATA[知识点OpenCV中默认imread函数加载图像文件，加载进来的是三通道彩色图像，色彩空间是RGB色彩空间、通道顺序是BGR（蓝色、绿色、红色）、对于三通道的图像OpenCV中提供了两个API函数用以实现通道分离与合并。 split // 通道分类 merge // 通道合并 扩展在很多CNN的卷积神经网络中输入的图像一般会要求[h, w, ch]其中h是高度、w是指宽度、ch是指通道数数目、OpenCV DNN模块中关于图像分类的googlenet模型输入[224,224,3]表示的就是224x224大小的三通道的彩色图像输入。 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像通道分离与合并 */int main() &#123; Mat src = imread("../images/baboon.jpg"); if (src.empty()) &#123; cout &lt;&lt; "could not load image.." &lt;&lt; endl; &#125; imshow("input", src); vector&lt;Mat&gt; mv; // mv用于存储图像分离后各通道像素 Mat dst1, dst2, dst3; // 令蓝色通道为0 split(src, mv); mv[0] = Scalar(0); merge(mv, dst1); imshow("blue == 0", dst1); // 令绿色通道为0 split(src, mv); mv[1] = Scalar(0); merge(mv, dst2); imshow("green == 0", dst2); // 令红色通道为0 split(src, mv); mv[2] = Scalar(0); merge(mv, dst3); imshow("red == 0", dst3); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526import cv2 as cvsrc = cv.imread("../images/baboon.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# 蓝色通道为零mv = cv.split(src)mv[0][:, :] = 0dst1 = cv.merge(mv)cv.imshow("output1", dst1)# 绿色通道为零mv = cv.split(src)mv[1][:, :] = 0dst2 = cv.merge(mv)cv.imshow("output2", dst2)# 红色通道为零mv = cv.split(src)mv[2][:, :] = 0dst3 = cv.merge(mv)cv.imshow("output3", dst3)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像通道的分离与合并</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-007-图像像素之逻辑操作]]></title>
    <url>%2F2019%2F03%2F24%2Fopencv-007%2F</url>
    <content type="text"><![CDATA[知识点下面三个操作类似，都是针对两张图像的位操作 bitwise_and bitwise_xor bitwise_or 针对输入图像, 图像取反操作，二值图像分析中经常用 bitwise_not 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;/* * 图像像素的逻辑操作 */int main() &#123; // create image one, CV_8UC3创建三通道图像 Mat src1 = Mat::zeros(Size(400, 400), CV_8UC3); Rect rect(100,100,100,100); // Scalar() 参数为BGR三通道值，绿色和红色加起来是黄色 src1(rect) = Scalar(0, 255, 255); imshow("input1", src1); // create image two Mat src2 = Mat::zeros(Size(400, 400), CV_8UC3); rect.x = 150; rect.y = 150; src2(rect) = Scalar(0, 0, 255); imshow("input2", src2); // 逻辑操作 Mat dst1, dst2, dst3; bitwise_and(src1, src2, dst1); bitwise_xor(src1, src2, dst2); bitwise_or(src1, src2, dst3); imshow("and", dst1); imshow("xor", dst2); imshow("or", dst3); // 演示取反操作 Mat src = imread("../images/test1.jpg"); Mat dst; imshow("input", src); bitwise_not(src,dst); imshow("not", dst); waitKey(0); return 0;&#125; 1234567891011121314151617181920212223242526272829import cv2 as cvimport numpy as np# create image onesrc1 = np.zeros(shape=[400, 400, 3], dtype=np.uint8)src1[100:200, 100:200, 1] = 255src1[100:200, 100:200, 2] = 255cv.imshow("input1", src1)# create image twosrc2 = np.zeros(shape=[400, 400, 3], dtype=np.uint8)src2[150:250, 150:250, 2] = 255cv.imshow("input2", src2)dst1 = cv.bitwise_and(src1, src2)dst2 = cv.bitwise_xor(src1, src2)dst3 = cv.bitwise_or(src1, src2)cv.imshow("dst1", dst1)cv.imshow("dst2", dst2)cv.imshow("dst3", dst3)src = cv.imread("../images/test1.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)dst = cv.bitwise_not(src)cv.imshow("dst", dst)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像像素逻辑操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生成假数据用于卷积神经网络模型训练]]></title>
    <url>%2F2019%2F03%2F24%2Fget_faked_data%2F</url>
    <content type="text"><![CDATA[背景在设计神经网络时，用于测试的imageNet等数据集太大，所以生成假数据用来测试神经网络能不能正常运行 代码123456789101112131415161718192021import tensorflow as tf# 参数设置batch_size = 32image_size = 24image_channel = 3n_classes = 10# 生成假数据用于训练模型def get_faked_train_batch(batch_size): images = tf.Variable(tf.random_normal(shape=[batch_size, image_size, image_size, image_channel], mean=0.0, stddev=1.0, dtype=tf.float32)) # tf.random_uniform() 标准均匀分布 labels = tf.Variable(tf.random_uniform(shape=[batch_size], minval=0, maxval=n_classes, dtype=tf.int32)) return images, labels # 生成假数据用于测试模型def get_faked_test_batch(batch_size): images = tf.Variable(tf.random_normal(shape=[batch_size, image_size, image_size, image_channel], mean=0.0, stddev=1.0, dtype=tf.float32)) # tf.random_uniform() 标准均匀分布 labels = tf.Variable(tf.random_uniform(shape=[batch_size], minval=0, maxval=n_classes, dtype=tf.int32)) return images, labels]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>卷积神经网络假数据生成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-006-Look Up Table(LUT)查找表的使用]]></title>
    <url>%2F2019%2F03%2F23%2Fopencv-006%2F</url>
    <content type="text"><![CDATA[知识点LUT查找表的简单原理 LUT查找表的作用 颜色匹配，比如讲灰度图像进行伪彩色增强 加快计算速度 API：applyColorMap(src, dst, COLORMAP) src 表示输入图像 dst表示输出图像 匹配到的颜色LUT， OpenCV支持13种颜色风格的查找表映射 COLORMAP ：13种色彩风格 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace std;using namespace cv;// 自定义LUTMat &amp;myColorMap(Mat &amp;image);/* * Look Up Table(LUT)查找表的使用 */int main() &#123; Mat src = imread("../images/LinuxLogo.jpg"); imshow("input", src); // 使用LUT Mat dst; applyColorMap(src, dst, COLORMAP_SUMMER); imshow("colorMap", dst); // 使用自己的LUT Mat my_dst, gray; cvtColor(src, gray, COLOR_BGR2GRAY); my_dst = myColorMap(gray); imshow("my_dst", my_dst); waitKey(0); return 0;&#125;// 自定义LUTMat &amp;myColorMap(Mat &amp;image) &#123; int lut[256]; for (int i = 0; i &lt; 256; ++i) &#123; if (i &lt; 127) lut[i] = 0; else lut[i] = 255; &#125; for (int row = 0; row &lt; image.rows; ++row) &#123; for (int col = 0; col &lt; image.cols; ++col) &#123; int pv = image.at&lt;uchar&gt;(row, col); image.at&lt;uchar&gt;(row, col) = lut[pv]; &#125; &#125; return image;&#125; 12345678910import cv2 as cvsrc = cv.imread("../images/LinuxLogo.jpg")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)dst = cv.applyColorMap(src, cv.COLORMAP_COOL)cv.imshow("output", dst)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>查找表（LUT）</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-005-像素算术操作]]></title>
    <url>%2F2019%2F03%2F23%2Fopencv-005%2F</url>
    <content type="text"><![CDATA[知识点像素算术操作 加add、减subtract、乘multiply、除divide saturate_cast&lt;T&gt;(value) # 类型转换注意点：图像的数据类型、通道数目、大小必须相同 代码（c++,python）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace std;using namespace cv;/* * 图像像素的加减乘除，两张图像大小类型要完全相同 */ int main()&#123; Mat src1 = imread("../images/opencv_images/LinuxLogo.jpg"); Mat src2 = imread("../images/opencv_images/WindowsLogo.jpg"); if(src1.empty() || src2.empty())&#123; cout&lt;&lt;"conld not read image..."&lt;&lt;endl; return -1; &#125; imshow("input1", src1); imshow("input2", src2); // 加法 Mat add_result = Mat::zeros(src1.size(),src1.type()); add(src1, src2, add_result); imshow("add_result", add_result); // 带权重的加法，一般推荐使用这个 Mat add_weight_result = Mat::zeros(src1.size(),src1.type()); addWeighted(src1, 0.5, src2, (1.0 - 0.5), 0.0, add_weight_result); imshow("add_weight_result", add_weight_result); // 减法 Mat sub_result = Mat::zeros(src1.size(),src1.type()); subtract(src1, src2, sub_result); imshow("sub_result", sub_result); // 乘法 Mat mul_result = Mat::zeros(src1.size(),src1.type()); multiply(src1, src2, mul_result); imshow("mul_result", mul_result); // 除法 Mat div_result = Mat::zeros(src1.size(),src1.type()); divide(src1, src2, div_result); imshow("div_result", div_result); // 自己实现加法操作 int b1 = 0, g1 = 0, r1 = 0; int b2 = 0, g2 = 0, r2 = 0; int b = 0, g = 0, r = 0; Mat my_add_result = Mat::zeros(src1.size(), src1.type()); for (int row = 0; row &lt; src1.rows; ++row) &#123; for (int col = 0; col &lt; src1.cols; ++col) &#123; b1 = src1.at&lt;Vec3b&gt;(row, col)[0]; g1 = src1.at&lt;Vec3b&gt;(row, col)[1]; r1 = src1.at&lt;Vec3b&gt;(row, col)[2]; b2 = src2.at&lt;Vec3b&gt;(row, col)[0]; g2 = src2.at&lt;Vec3b&gt;(row, col)[1]; r2 = src2.at&lt;Vec3b&gt;(row, col)[2]; // b1:0~255,b2:0~255, b1+b2可能大于255，所以需要转换，通过saturate_cast&lt;uchar&gt;() my_add_result.at&lt;Vec3b&gt;(row, col)[0] = saturate_cast&lt;uchar&gt;(b1 + b2); my_add_result.at&lt;Vec3b&gt;(row, col)[1] = saturate_cast&lt;uchar&gt;(g1 + g2); my_add_result.at&lt;Vec3b&gt;(row, col)[2] = saturate_cast&lt;uchar&gt;(r1 + r2); &#125; &#125; imshow("my_add_result", my_add_result); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425262728import cv2 as cvimport numpy as npsrc1 = cv.imread("../images/opencv_images/LinuxLogo.jpg");src2 = cv.imread("../images/opencv_images/WindowsLogo.jpg");cv.imshow("input1", src1)cv.imshow("input2", src2)h, w, ch = src1.shapeprint("h , w, ch", h, w, ch)add_result = np.zeros(src1.shape, src1.dtype);cv.add(src1, src2, add_result);cv.imshow("add_result", add_result);sub_result = np.zeros(src1.shape, src1.dtype);cv.subtract(src1, src2, sub_result);cv.imshow("sub_result", sub_result);mul_result = np.zeros(src1.shape, src1.dtype);cv.multiply(src1, src2, mul_result);cv.imshow("mul_result", mul_result);div_result = np.zeros(src1.shape, src1.dtype);cv.divide(src1, src2, div_result);cv.imshow("div_result", div_result);cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像像素算术操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用tensorflow对cifar10数据集进行图像分类]]></title>
    <url>%2F2019%2F03%2F22%2Fcifar10%2F</url>
    <content type="text"><![CDATA[步骤 定义神经网络计算图 运行计算图 导包12345import tensorflow as tfimport osimport cifar10_input # tensorflow/modle模块中自带案例，可以去github下载import numpy as npos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' 设置算法超参数1234567891011learning_rate_init = 0.001l2loss_ratio = 0.001keep_prob = 0.7 #dropouttraining_epochs = 5batch_size = 100display_step = 100conv1_kernel_num = 64conv2_kernel_num = 64fc1_units_num = 256fc2_units_num = 128fc3_units_num = cifar10_input.NUM_CLASSES 数据集中输入图像的参数123456dataset_dir = './cifar10_data/'image_size = cifar10_input.IMAGE_SIZEimage_channel = 3n_classes = cifar10_input.NUM_CLASSESnum_examples_per_epoch_for_train = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAINnum_examples_per_epoch_for_eval = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL 得到每一批次的训练数据123456def get_distorted_train_batch(data_dir, batch_size): if not data_dir: raise ValueError('please supply a data_dir') data_dir = os.path.join(data_dir, 'cifar-10-batches-bin') images, labels = cifar10_input.distorted_inputs(data_dir=data_dir, batch_size=batch_size) return images, labels 得到每一批次的测试数据123456def get_undistorted_eval_batch(data_dir, eval_data, batch_size): if not data_dir: raise ValueError('please supply a data_dir') data_dir = os.path.join(data_dir, 'cifar-10-batches-bin') images, labels = cifar10_input.inputs(eval_data=eval_data, data_dir=data_dir, batch_size=batch_size) return images, labels 根据指定的维数返回初始化好的指定名称的权重 Variable12345678def WeightsVariable(shape, name_str='weights', stddev=0.1): # 单cpu initial = tf.truncated_normal(shape=shape, stddev=stddev, dtype=tf.float32) return tf.Variable(initial, dtype=tf.float32, name=name_str) # 多gpu # weights = tf.get_variable(name_str, shape=shape, dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer_conv2d()) # return weights 根据指定的维数返回初始化好的指定名称的权重 Variable123def BiasesVariable(shape, name_str='biases', init_value=0.0): initial = tf.constant(init_value, shape=shape) return tf.Variable(initial, dtype=tf.float32, name=name_str) 2维卷积层的封装（包含激活函数）1234567def Conv2d(x, W, b, stride=1, padding='SAME', activation=tf.nn.relu, act_name='relu'): with tf.name_scope('conv2d_bias'): y = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=padding) y = tf.nn.bias_add(y, b) with tf.name_scope(act_name): y = activation(y) return y 2维池化层pool的封装12def Pool2d(x, pool=tf.nn.max_pool, k=2, stride=2, padding='SAME'): return pool(x, ksize=[1, k, k, 1], strides=[1, stride, stride, 1], padding=padding) 全连接层的封装1234567def FullyConnected(x, W, b, activation=tf.nn.relu, act_name='relu'): with tf.name_scope('Wx_b'): y = tf.matmul(x, W) y = tf.add(y, b) with tf.name_scope(act_name): y = activation(y) return y 为每一层的激活输出添加汇总节点123def AddActivationSummary(x): tf.summary.histogram('/activations', x) tf.summary.scalar('/sparsity', tf.nn.zero_fraction(x)) # 稀疏性 为所有损失节点添加标量汇总操作12345678910def AddLossesSummary(losses): # 计算所有损失的滑动平均 loss_averages = tf.train.ExponentialMovingAverage(decay=0.9, name='avg') loss_averages_op = loss_averages.apply(losses) # 为所有损失及平滑处理的损失绑定标量汇总节点 for loss in losses: tf.summary.scalar(loss.op.name + '(raw)', loss) tf.summary.scalar(loss.op.name + '(avg)', loss_averages.average(loss)) return loss_averages_op 打印每一层输出张量的shape12def print_layers_shape(t): print(t.op.name, ' ', t.get_shape().as_list()) 前向推断过程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263def Inference(images_holder): # 第一个卷积层 with tf.name_scope('Conv2d_1'): weights = WeightsVariable(shape=[5, 5, image_channel, conv1_kernel_num], stddev=5e-2) biases = BiasesVariable(shape=[conv1_kernel_num]) conv1_out = Conv2d(images_holder, weights, biases) AddActivationSummary(conv1_out) print_layers_shape(conv1_out) # 第一个池化层 with tf.name_scope('Pool2d_1'): pool1_out = Pool2d(conv1_out, k=3, stride=2) # 第二个卷积层 with tf.name_scope('Conv2d_2'): weights = WeightsVariable(shape=[5, 5, conv1_kernel_num, conv2_kernel_num], stddev=5e-2) biases = BiasesVariable(shape=[conv2_kernel_num]) conv2_out = Conv2d(pool1_out, weights, biases) AddActivationSummary(conv2_out) # 第二个池化层 with tf.name_scope('Pool2d_2'): pool2_out = Pool2d(conv2_out, k=3, stride=2) # 将二维特征图变为一维特征向量 with tf.name_scope('FeatsReshape'): features = tf.reshape(pool2_out, [batch_size, -1]) feats_dim = features.get_shape()[1].value # 得到上一行 -1 所指代的值 # 第一个全连接层 with tf.name_scope('FC1_nonlinear'): weights = WeightsVariable(shape=[feats_dim, fc1_units_num], stddev=4e-2) biases = BiasesVariable(shape=[fc1_units_num], init_value=0.1) fc1_out = FullyConnected(features, weights, biases) AddActivationSummary(fc1_out) # 加入L2损失 with tf.name_scope('L2_loss'): weight_loss = tf.multiply(tf.nn.l2_loss(weights), l2loss_ratio, name='fc1_weight_loss') tf.add_to_collection('losses', weight_loss) # Dropout # with tf.name_scope('dropout_1'): # fc1_dropout = tf.nn.dropout(fc1_out, keep_prob=keep_prob) # 第二个全连接层 with tf.name_scope('FC2_nonlinear'): weights = WeightsVariable(shape=[fc1_units_num, fc2_units_num], stddev=4e-2) biases = BiasesVariable(shape=[fc2_units_num], init_value=0.1) fc2_out = FullyConnected(fc1_out, weights, biases) AddActivationSummary(fc2_out) # 加入L2损失 with tf.name_scope('L2_loss'): weight_loss = tf.multiply(tf.nn.l2_loss(weights), l2loss_ratio, name='fc2_weight_loss') tf.add_to_collection('losses', weight_loss) # 第三个全连接层 with tf.name_scope('FC3_linear'): weights = WeightsVariable(shape=[fc2_units_num, fc3_units_num], stddev=1.0/fc2_units_num) biases = BiasesVariable(shape=[fc3_units_num]) logits = FullyConnected(fc2_out, weights, biases, activation=tf.identity, act_name='linear') AddActivationSummary(logits) return logits 调用上面写的函数构造计算图，并设计会话流程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111def TrainModel(): with tf.Graph().as_default(): # 计算图输入 with tf.name_scope('Inputs'): images_holder = tf.placeholder(tf.float32, [batch_size, image_size, image_size, image_channel], name='images') labels_holder = tf.placeholder(tf.int32, [batch_size], name='labels') # 计算图前向推断过程 with tf.name_scope('Inference'): logits = Inference(images_holder) # 定义损失层 with tf.name_scope('Loss'): cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_holder, logits=logits) cross_entropy_loss = tf.reduce_mean(cross_entropy, name='xentropy_loss') tf.add_to_collection('losses', cross_entropy_loss) # 总损失 = 交叉熵损失 + L2损失 total_loss = tf.add_n(tf.get_collection('losses'), name='total_loss') average_losses = AddLossesSummary(tf.get_collection('losses') + [total_loss]) # 定义优化训练层 with tf.name_scope('Train'): learning_rate = tf.placeholder(tf.float32) global_step = tf.Variable(0, name='global_step', trainable=False, dtype=tf.int64) optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate) train_op = optimizer.minimize(total_loss, global_step=global_step) # 定义模型评估层 with tf.name_scope('Evaluate'): top_K_op = tf.nn.in_top_k(predictions=logits, targets=labels_holder, k=1) # 定义获取训练样本批次的节点 with tf.name_scope('GetTrainBatch'): images_train, labels_train = get_distorted_train_batch(data_dir=dataset_dir, batch_size=batch_size) # 定义获取测试样本批次的节点 with tf.name_scope('GetTestBatch'): images_test, labels_test = get_undistorted_eval_batch(eval_data=True, data_dir=dataset_dir, batch_size=batch_size) # 收集所有汇总节点 merged_summaries = tf.summary.merge_all() # 添加所有变量的初始化节点 init_op = tf.global_variables_initializer() print("把计算图写入事件文件...") # graph_writer = tf.summary.FileWriter(logdir='events/', graph=tf.get_default_graph()) # graph_writer.close() summary_writer = tf.summary.FileWriter(logdir='events/') summary_writer.add_graph(graph=tf.get_default_graph()) summary_writer.flush() with tf.Session() as sess: sess.run(init_op) print('==&gt;&gt;&gt;&gt;&gt;&gt;&gt;==开始在训练集上训练模型==&lt;&lt;&lt;&lt;&lt;&lt;&lt;==') total_batches = int(num_examples_per_epoch_for_train / batch_size) print("per batch size: ", batch_size) print("train sample count per epoch:", num_examples_per_epoch_for_train) print("total batch count per epoch:", total_batches) # 启动数据读取队列 tf.train.start_queue_runners() # 记录模型被训练的步数 training_step = 0 # 训练指定轮数，每一轮的训练样本总数为：num_examples_per_epoch_for_train for epoch in range(training_epochs): # 每一轮都要把所有的batch跑一遍 for batch_idx in range(total_batches): # 运行获取批次训练数据的计算图，取出一个批次数据 images_batch, labels_batch = sess.run([images_train, labels_train]) # 运行优化器训练节点 _, loss_value, avg_losses= sess.run([train_op, total_loss, average_losses], feed_dict=&#123;images_holder:images_batch, labels_holder:labels_batch, learning_rate:learning_rate_init&#125;) # 每调用一次训练节点，training_step就加1，最终 == training_epochs * total_batch training_step = sess.run(global_step) # 每训练display_step次，计算当前模型的损失和分类准确率 if training_step % display_step == 0: # 运行Evaluate节点，计算当前批次的训练样本的准确率 predictions = sess.run([top_K_op], feed_dict=&#123;images_holder:images_batch, labels_holder:labels_batch&#125;) # 计算当前批次的预测正确样本量 batch_accuracy = np.sum(predictions) / batch_size print("train step: " + str(training_step) + ", train loss= " + "&#123;:.6f&#125;".format(loss_value) + ", train accuracy=" + "&#123;:.5f&#125;".format(batch_accuracy)) # 运行汇总节点 summaries_str = sess.run(merged_summaries, feed_dict= &#123;images_holder: images_batch, labels_holder: labels_batch&#125;) summary_writer.add_summary(summary=summaries_str, global_step=training_step) summary_writer.flush() summary_writer.close() print("训练完毕！") print('==&gt;&gt;&gt;&gt;&gt;&gt;&gt;==开始在测试集上评估模型==&lt;&lt;&lt;&lt;&lt;&lt;&lt;==') total_batches = int(num_examples_per_epoch_for_eval / batch_size) total_examples = total_batches * batch_size # 当除不尽batch_size时，num_examples_per_epoch_for_evalv ！= total_examples print("per batch size: ", batch_size) print("test sample count per epoch:", total_examples) print("total batch count per epoch:", total_batches) correc_predicted = 0 for test_step in range(total_batches): # 运行获取批次测试数据的计算图，取出一个批次数据 images_batch, labels_batch = sess.run([images_test, labels_test]) # 运行Evaluate节点，计算当前批次的训练样本的准确率 predictions = sess.run([top_K_op], feed_dict=&#123;images_holder:images_batch, labels_holder:labels_batch&#125;) # 累计每个批次的预测正确样本量 correc_predicted += np.sum(predictions) accuracy_score = correc_predicted / total_examples print("--------&gt;accuracy on test examples: ",accuracy_score) 123456def main(argv=None): train_dir = './events/' if tf.gfile.Exists(train_dir): tf.gfile.DeleteRecursively(train_dir) tf.gfile.MakeDirs(train_dir) TrainModel() 12if __name__ == '__main__': tf.app.run() 结果 训练结果 测试结果 Tensorboard 中查看 代码地址github中没有上传cifar10数据集，需要的话请从百度云下载，或自行下载，按照如下解压 github 百度云 提取码：xw3x]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>图像分类</tag>
        <tag>tensorflow</tag>
        <tag>cifar10</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-004-图像像素读写操作]]></title>
    <url>%2F2019%2F03%2F21%2Fopencv-004%2F</url>
    <content type="text"><![CDATA[知识点 C++中的像素遍历与访问 数组遍历 指针方式遍历 Python中的像素遍历与访问 数组遍历 代码（c++,python）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace std;using namespace cv;/** * 读取图像，实现像素反转 */int main() &#123; Mat src = imread("../images/liuyifei_1.png"); Mat src_copy = src.clone(); int height = src.rows; int width = src.cols; int ch = src.channels(); imshow("input", src); // 直接读取图像像素 for (int row = 0; row &lt; height; ++row) &#123; for (int col = 0; col &lt; width; ++col) &#123; if (ch == 3) &#123; Vec3b bgr = src.at&lt;Vec3b&gt;(row, col); bgr[0] = 255 - bgr[0]; bgr[1] = 255 - bgr[1]; bgr[2] = 255 - bgr[2]; src.at&lt;Vec3b&gt;(row, col) = bgr; &#125; else if (ch == 1) &#123; int gray = src.at&lt;uchar&gt;(row, col); src.at&lt;uchar&gt;(row, col) = 255 - gray; &#125; &#125; &#125; imshow("output1", src); // 指针读取 Mat result = Mat::zeros(src_copy.size(), src_copy.type()); int blue = 0, green = 0, red = 0; int gray; for (int row = 0; row &lt; height; ++row) &#123; // curr_row为第row行的首地址，遍历时，前三个字节表示的是第一个像素的BGR值， // 注意BGR值顺序，接下来三个字节是第二个像素的值。 uchar *curr_row = src_copy.ptr&lt;uchar&gt;(row); uchar *result_row = result.ptr&lt;uchar&gt;(row); for (int col = 0; col &lt; width; ++col) &#123; if (ch == 3) &#123; blue = *curr_row++; green = *curr_row++; red = *curr_row++; *result_row++ = 255 - blue; *result_row++ = 255 - green; *result_row++ = 255 - red; &#125; else if (ch == 1) &#123; gray = *curr_row++; *result_row++ = gray; &#125; &#125; &#125; imshow("output2", result); waitKey(0); return 0;&#125; 123456789101112131415161718import cv2 as cvsrc = cv.imread("../images/liuyifei_1.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)h, w, ch = src.shapeprint("h , w, ch", h, w, ch)for row in range(h): for col in range(w): b, g, r = src[row, col] b = 255 - b g = 255 - g r = 255 - r src[row, col] = [b, g, r]cv.imshow("output", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>图像像素读写</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-003-图像对象(Mat)创建与赋值]]></title>
    <url>%2F2019%2F03%2F21%2Fopencv-003%2F</url>
    <content type="text"><![CDATA[知识点 C++中Mat对象与创建 Python中Numpy数组对象 代码（c++,python）123456789101112131415161718192021222324252627282930#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace std;using namespace cv;int main()&#123; Mat src = imread("../images/liuyifei_1.png"); // 通过克隆或复制创建图像对象，m1和src指向不同内存块 Mat m1 = src.clone(); Mat m2; src.copyTo(m2); // 赋值法，m3和src指向同一内存块 Mat m3 = src; // 创建空白图像 Mat m4 = Mat::zeros(src.size(),src.type()); Mat m5 = Mat::zeros(Size(512,512),CV_8UC3); Mat m6 = Mat::ones(Size(512,512),CV_8UC3); // kernel: [0, -1, 0 // -1, 5, -1 // 0, -1, 0] Mat kernel = (Mat_&lt;char&gt;(3,3)&lt;&lt;0,-1,0,-1,5,-1,0,-1,0); waitKey(0); return 0;&#125; 12345678910111213141516171819202122232425262728import cv2 as cvimport numpy as npsrc = cv.imread("../images/liuyifei_1.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)# 克隆图像m1 = np.copy(src)# 赋值m2 = srcsrc[100:200,200:300,:] = 255 # 第三维代表图像通道cv.imshow("m2",m2)m3 = np.zeros(src.shape, src.dtype)cv.imshow("m3", m3)m4 = np.zeros([512,512], np.uint8)# m4[:,:] =127 try to give gray value 127cv.imshow("m4", m4)m5 = np.ones(shape=[512,512,3], dtype=np.uint8)m5[:,:,0] = 255cv.imshow("m5", m5)cv.waitKey(0)cv.destroyAllWindows() 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>Mat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-002-色彩空间转换(cvtcolor)与图像保存(imwrite)]]></title>
    <url>%2F2019%2F03%2F20%2Fopencv-002%2F</url>
    <content type="text"><![CDATA[知识点 色彩空间转换函数- cvtColor COLOR_BGR2GRAY = 6 彩色到灰度 COLOR_GRAY2BGR = 8 灰度到彩色 COLOR_BGR2HSV = 40 BGR到HSV COLOR_HSV2BGR = 54 HSV到 BGR 图像保存 - imwrite 第一个参数是图像保存路径 第二个参数是图像内存对象 代码（c++,python）1234567891011121314151617181920212223242526#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace std;using namespace cv;int main()&#123; Mat src = imread("../images/liuyifei_1.png"); if (src.empty())&#123; cout &lt;&lt; "could not load image..." &lt;&lt; endl; return -1; &#125; namedWindow("input"); imshow("input",src); Mat dst; cvtColor(src,dst,COLOR_BGR2GRAY); imwrite("../images/result1.png",dst); namedWindow("output gray"); imshow("output gray",dst); waitKey(0); return 0;&#125; 123456789import cv2 as cvsrc = cv.imread("../images/liuyifei_1.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)cv.imshow("gray", gray)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>色彩空间转换(cvtcolor)</tag>
        <tag>图像保存(imwrite)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Clion无法读取相对路径文件或图像的解决方法]]></title>
    <url>%2F2019%2F03%2F20%2FClion_path_problem%2F</url>
    <content type="text"><![CDATA[项目目录 相对路径错误写法12// opencv读取图像，此时无法读取Mat image = imread("images/liuyifei_1.png") 解决方案 1 - 使用绝对路径1Mat image = imread("D:\\code-workspace\\Clion-workspace\\learnOpencv\\images\\liuyifei_1.png") 解决方案 2 - 返回根目录1Mat image = imread("../images/liuyifei_1.png") 解决方案 3 - 设置项目工作目录 设置项目工作目录 代码如下 12// 此时读取成功Mat image = imread("images/liuyifei_1.png")]]></content>
      <tags>
        <tag>Clion</tag>
        <tag>相对路径问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv-001-读取(imread)与显示(imshow)图像]]></title>
    <url>%2F2019%2F03%2F20%2Fopencv-001%2F</url>
    <content type="text"><![CDATA[知识点 读取图像 - imread() 显示图像 - imshow() 代码（c++,python）123456789101112131415161718192021#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace std;using namespace cv;int main() &#123; // Mat image = imread("../images/liuyifei_1.png"); // 读取的时候加参数，使读取后为灰度图像 Mat image = imread("../images/liuyifei_1.png",IMREAD_GRAYSCALE); if (image.empty()) &#123; cout &lt;&lt; "could not load image..." &lt;&lt; endl; return -1; &#125; namedWindow("input"); imshow("input",image); waitKey(0); return 0;&#125; 1234567import cv2 as cvsrc = cv.imread("../images/liuyifei_1.png")cv.namedWindow("input", cv.WINDOW_AUTOSIZE)cv.imshow("input", src)cv.waitKey(0)cv.destroyAllWindows() 结果 代码地址github]]></content>
      <categories>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>读取并显示图像</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pip配置阿里云镜像]]></title>
    <url>%2F2019%2F03%2F19%2Fpip_windows_aliyun%2F</url>
    <content type="text"><![CDATA[windows新建pip配置文件夹 在windows “文件资源管理器” 地址栏输入%APPDATA% 按回车，创建pip文件夹，用于存放pip配置文件 在pip文件夹中新建名为：pip.ini 的配置文件 在pip.ini中输入以下内容 123[global]trusted-host = mirrors.aliyun.comindex-url = https://mirrors.aliyun.com/pypi/simple linux新建.pip文件夹 1mkdir .pip 新建pip.conf文件 12cd .piptouch pip.conf 在pip.conf中输入以下内容 1vim pip.conf 123[global]trusted-host = mirrors.aliyun.comindex-url = https://mirrors.aliyun.com/pypi/simple]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>pip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python安装opencv]]></title>
    <url>%2F2019%2F03%2F19%2Fopencv_python%2F</url>
    <content type="text"><![CDATA[安装opencv123456# opencv-python 和 opencv-contrib-python只能安装一个，后者带有扩展包，建议直接安后者pip install opencv-python# 安装opencv-contrib-python前，要先卸载opencv-pythonpip uninstall opencv-pythonpip install opencv-contrib-python 更新opencv1pip install --upgrade opencv-python]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows下Clion配置opencv]]></title>
    <url>%2F2019%2F03%2F19%2Fopencv_Clion%2F</url>
    <content type="text"><![CDATA[所需环境MinGw + Cmake + Clion + opencv 安装MinGw参考：install MinGw 安装Cmake参考：install Cmake Cmake下载网址：Cmake download 注：Cmake最好安装跟Clion中配置一样的版本，省的麻烦 安装 opencv 下载地址 ：opencv download 解压到 opencv4文件夹中 解压后： 配置环境变量： Clion 配置 编译opencv源码 打开Cmake-GUI，选择源码路径和输出路径 点击Configure，选择MinGW Makefiles，点击Finish，开始编译 等待一段时间后，会有很多报红，再次点击Configure，红色消失，点击Generate 进入输出目录，在cmd 运行下面代码，等待完成 1mingw32-make -j8 运行mingw32-make install，等待片刻，输出目录下会多出install文件夹 添加…\install\x64\mingw\bin 添加到path系统环境变量环境变量 编辑CMakeLists.txt1234567891011121314151617cmake_minimum_required(VERSION 3.13)project(learnOpencv)set(CMAKE_CXX_STANDARD 11)# Where to find CMake modules and OpenCVset(OpenCV_DIR "D:\\software\\opencv4\\MinGW64_build\\install")set(CMAKE_MODULE_PATH $&#123;CMAKE_MODULE_PATH&#125; "$&#123;CMAKE_SOURCE_DIR&#125;/cmake/")find_package(OpenCV REQUIRED)include_directories($&#123;OpenCV_INCLUDE_DIRS&#125;)add_executable(learnOpencv test.cpp)# add libs you needset(OpenCV_LIBS opencv_core opencv_imgproc opencv_highgui opencv_imgcodecs)# linkingtarget_link_libraries(learnOpencv $&#123;OpenCV_LIBS&#125;) 注意：opencv4必须要c++11支持 测试12345678910111213#include &lt;opencv2\opencv.hpp&gt;using namespace cv;int main()&#123; Mat img = imread("D:\\code-workspace\\Clion-workspace\\learnOpencv\\images\\1.png",WINDOW_AUTOSIZE); namedWindow("刘亦菲"); imshow("刘亦菲", img); waitKey(0); return 0;&#125;]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>Clion</tag>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VS2017配置opencv]]></title>
    <url>%2F2019%2F03%2F19%2Fopencv_vs2017%2F</url>
    <content type="text"><![CDATA[安装 opencv 下载地址 ：opencv download 解压到 opencv4文件夹中 解压后： 配置环境变量： VS2017中配置opencv 新建一个工程 依次点击：视图 ==&gt; 其他窗口 ==&gt; 属性管理器 添加包含目录 添加库目录 添加附加依赖项 重启VS2017 测试 测试代码 1234567891011121314#include &lt;opencv2\opencv.hpp&gt;using namespace cv;int main()&#123; Mat img = imread("1.png"); namedWindow("hahaha"); imshow("hahaha", img); waitKey(0); return 0;&#125; 测试结果]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>VS2017</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装jupyter notebook插件]]></title>
    <url>%2F2019%2F03%2F02%2FjupyterPlugin%2F</url>
    <content type="text"><![CDATA[步骤12python -m pip install jupyter_contrib_nbextensionsjupyter contrib nbextension install --user --skip-running-check Autopep8 –&gt; 格式化代码]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>jupyter notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用keras用常规神经网络训练MNIST数据集]]></title>
    <url>%2F2019%2F03%2F02%2FmnistLinearNN%2F</url>
    <content type="text"><![CDATA[加载mnist数据集123456from keras.datasets import mnist(x_train,y_train),(x_test,y_test) = mnist.load_data() # 将下载好的mnist.npz方在 ~/.keras/datasets/ 目录下print(x_train.shape,type(x_train))print(y_train.shape,type(y_train))print(x_test.shape,type(x_test))print(y_test.shape,type(y_test)) (60000, 28, 28) &lt;class &apos;numpy.ndarray&apos;&gt; (60000,) &lt;class &apos;numpy.ndarray&apos;&gt; (10000, 28, 28) &lt;class &apos;numpy.ndarray&apos;&gt; (10000,) &lt;class &apos;numpy.ndarray&apos;&gt; 数据处理：规范化1234# 将图形从[28,28]变为[784,]X_train = x_train.reshape(60000,784)X_test = x_test.reshape(10000,784)print(X_train.shape,X_test.shape) (60000, 784) (10000, 784) 123456# 将数据转换为float32，为了进行归一化，不然/255得到全部是0X_train = X_train.astype('float32')X_test = X_test.astype('float32')# 数据归一化X_train /= 255X_test /= 255 统计训练数据中个标签数量12345import numpy as npimport matplotlib.pyplot as pltlabel, count = np.unique(y_train, return_counts=True)print(label, count) [0 1 2 3 4 5 6 7 8 9] [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949] 123456789101112fig = plt.figure(figsize=(8, 5))plt.bar(label, count, width=0.7, align='center')plt.title("Label Distribution")plt.xlabel('Label')plt.ylabel('Count')plt.xticks(label)plt.ylim(0, 7500)for a, b in zip(label, count): plt.text(a, b, '%d' % b, ha='center', va='bottom', fontsize=10)plt.show() 对标签进行one-hot编码123456789101112131415161718192021# import tensorflow as tf# n_classes = 10# Y_train = tf.one_hot(y_train, n_classes)# Y_test = tf.one_hot(y_test, n_classes)# with tf.Session() as sess:# sess.run(tf.global_variables_initializer())# Y_train=sess.run(Y_train)# Y_test=sess.run(Y_test)# print(Y_train.shape)# 下面代码同上，使用tensorflow需要建立会话，简单转换keras更方便from keras.utils import np_utilsn_classes = 10Y_train = np_utils.to_categorical(y_train,n_classes)Y_test = np_utils.to_categorical(y_test,n_classes)print(Y_train.shape) (60000, 10) 12print(y_train[0])print(Y_train[0]) 5 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] 使用Keras sequential model 定义神经网络1234567891011121314# 使用keras定义线性网络很方便from keras.models import Sequentialfrom keras.layers.core import Dense, Activationmodel = Sequential()# 第一隐藏层model.add(Dense(512, input_shape=(784,)))model.add(Activation('relu'))# 第二隐藏层model.add(Dense(512))model.add(Activation('relu'))# 输出层model.add(Dense(10))model.add(Activation('softmax')) 编译模型1model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy']) 训练模型，并将指标保存到history中12history = model.fit(X_train, Y_train, batch_size=128, epochs=5, verbose=2, validation_data=(X_test, Y_test)) Train on 60000 samples, validate on 10000 samples Epoch 1/5 - 7s - loss: 0.2156 - acc: 0.9373 - val_loss: 0.0970 - val_acc: 0.9710 Epoch 2/5 - 7s - loss: 0.0804 - acc: 0.9758 - val_loss: 0.0769 - val_acc: 0.9770 Epoch 3/5 - 7s - loss: 0.0504 - acc: 0.9838 - val_loss: 0.0791 - val_acc: 0.9746 Epoch 4/5 - 7s - loss: 0.0350 - acc: 0.9891 - val_loss: 0.0659 - val_acc: 0.9804 Epoch 5/5 - 8s - loss: 0.0264 - acc: 0.9913 - val_loss: 0.0734 - val_acc: 0.9794 可视化指标12345678910111213141516171819fig = plt.figure()plt.subplot(211)plt.plot(history.history['acc'])plt.plot(history.history['val_acc'])plt.title('Model Accuracy')plt.xlabel('epoch')plt.ylabel('accuracy')plt.legend(['train','test'])plt.subplot(212)plt.plot(history.history['loss'])plt.plot(history.history['val_loss'])plt.title('Model Loss')plt.xlabel('epoch')plt.ylabel('loss')plt.legend(['train','test'])plt.tight_layout()plt.show() 保存模型123456789101112import osimport tensorflow.gfile as gfilesave_dir = '.\model'if gfile.Exists(save_dir): gfile.DeleteRecursively(save_dir)gfile.MakeDirs(save_dir)model_name = 'keras_mnist.h5'model_path = os.path.join(save_dir,model_name)model.save(model_path)print('Saved trained model at %s' % model_path) Saved trained model at .\model\keras_mnist.h5 加载模型123from keras.models import load_modelmnist_model = load_model(model_path) 统计模型在测试集上的分类结果123456789loss_and_metrics = mnist_model.evaluate(X_test, Y_test, verbose=2)print("Test Loss: &#123;&#125;".format(loss_and_metrics[0]))print("Test Accuracy: &#123;&#125;%".format(loss_and_metrics[1]*100))predicted_classes = mnist_model.predict_classes(X_test)correct_indices = np.nonzero(predicted_classes == y_test)[0]incorrect_indices = np.nonzero(predicted_classes != y_test)[0]print("Classified correctly count: &#123;&#125;".format(len(correct_indices)))print("Classified incorrectly count: &#123;&#125;".format(len(incorrect_indices))) Test Loss: 0.07340353026344673 Test Accuracy: 97.94% Classified correctly count: 9794 Classified incorrectly count: 206 代码地址github]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>mnist</tag>
        <tag>keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用keras用卷积神经网络训练MNIST数据集]]></title>
    <url>%2F2019%2F03%2F02%2FmnistCNN%2F</url>
    <content type="text"><![CDATA[加载mnist数据集123456from keras.datasets import mnist(x_train,y_train),(x_test,y_test) = mnist.load_data() # 将下载好的mnist.npz方在 ~/.keras/datasets/ 目录下print(x_train.shape,type(x_train))print(y_train.shape,type(y_train))print(x_test.shape,type(x_test))print(y_test.shape,type(y_test)) (60000, 28, 28) &lt;class &apos;numpy.ndarray&apos;&gt; (60000,) &lt;class &apos;numpy.ndarray&apos;&gt; (10000, 28, 28) &lt;class &apos;numpy.ndarray&apos;&gt; (10000,) &lt;class &apos;numpy.ndarray&apos;&gt; 数据处理：规范化channels_last对应的输入：(batch,height,width,channels) channels_first对应的输入：(batch,channels,height,width) 默认channels_last，修改：~/.keras/keras.json 123456789101112131415from keras import backend as Kimg_rows, img_cols = 28, 28if K.image_data_format() == 'channels_first': x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols) x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols) input_shape = (1, img_rows, img_cols)else: x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1) x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1) input_shape = (img_rows, img_cols, 1)print(x_train.shape, type(x_train))print(x_test.shape, type(x_test)) (60000, 28, 28, 1) &lt;class &apos;numpy.ndarray&apos;&gt; (10000, 28, 28, 1) &lt;class &apos;numpy.ndarray&apos;&gt; 123456# 将数据转换为float32，为了进行归一化，不然/255得到全部是0X_train = x_train.astype('float32')X_test = x_test.astype('float32')# 数据归一化X_train /= 255X_test /= 255 统计训练数据中个标签数量12345import numpy as npimport matplotlib.pyplot as pltlabel, count = np.unique(y_train, return_counts=True)print(label, count) [0 1 2 3 4 5 6 7 8 9] [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949] 123456789101112fig = plt.figure(figsize=(8, 5))plt.bar(label, count, width=0.7, align='center')plt.title("Label Distribution")plt.xlabel('Label')plt.ylabel('Count')plt.xticks(label)plt.ylim(0, 7500)for a, b in zip(label, count): plt.text(a, b, '%d' % b, ha='center', va='bottom', fontsize=10)plt.show() 对标签进行one-hot编码1234567from keras.utils import np_utilsn_classes = 10Y_train = np_utils.to_categorical(y_train,n_classes)Y_test = np_utils.to_categorical(y_test,n_classes)print(Y_train.shape) (60000, 10) 12print(y_train[0])print(Y_train[0]) 5 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] 使用Keras sequential model 定义MNIST CNN网络123456789101112131415161718192021222324from keras.models import Sequentialfrom keras.layers import Dense, Dropout, Flattenfrom keras.layers import Conv2D, MaxPooling2Dmodel = Sequential()## Feature Extraction# 第一层卷积，32个3*3的卷积核，激活函数使用relumodel.add(Conv2D(filters=32,kernel_size=(3,3),activation='relu',input_shape=input_shape))# 第二层卷积，64个3*3的卷积核，激活函数使用relumodel.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu'))# 最大池化层model.add(MaxPooling2D(pool_size=(2,2)))# Dropout 25% 的输入神经元model.add(Dropout(0.25))# 将Pooled feature map 摊平后输入全连接网络model.add(Flatten())## Classification# 全连接层model.add(Dense(128,activation='relu'))# Dropout 50% 的输入神经元model.add(Dropout(0.5))# 使用softmax 激活函数做多分类，输出各数字的概率model.add(Dense(10, activation='softmax')) 查看 MNIST CNN 模型网络结构1model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 26, 26, 32) 320 _________________________________________________________________ conv2d_2 (Conv2D) (None, 24, 24, 64) 18496 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 12, 12, 64) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 9216) 0 _________________________________________________________________ dense_1 (Dense) (None, 128) 1179776 _________________________________________________________________ dropout_2 (Dropout) (None, 128) 0 _________________________________________________________________ dense_2 (Dense) (None, 10) 1290 ================================================================= Total params: 1,199,882 Trainable params: 1,199,882 Non-trainable params: 0 _________________________________________________________________ 12for layer in model.layers: print(layer.get_output_at(0).get_shape().as_list()) [None, 26, 26, 32] [None, 24, 24, 64] [None, 12, 12, 64] [None, 12, 12, 64] [None, None] [None, 128] [None, 128] [None, 10] 编译模型1model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy']) 训练模型，并将指标保存到history中1history = model.fit(X_train, Y_train, batch_size=128, epochs=5,verbose=2, validation_data=(X_test, Y_test)) Train on 60000 samples, validate on 10000 samples Epoch 1/5 - 131s - loss: 0.2330 - acc: 0.9290 - val_loss: 0.0540 - val_acc: 0.9817 Epoch 2/5 - 146s - loss: 0.0853 - acc: 0.9747 - val_loss: 0.0372 - val_acc: 0.9882 Epoch 3/5 - 136s - loss: 0.0605 - acc: 0.9812 - val_loss: 0.0315 - val_acc: 0.9898 Epoch 4/5 - 129s - loss: 0.0514 - acc: 0.9843 - val_loss: 0.0283 - val_acc: 0.9913 Epoch 5/5 - 130s - loss: 0.0416 - acc: 0.9873 - val_loss: 0.0272 - val_acc: 0.9911 可视化指标12345678910111213141516171819fig = plt.figure()plt.subplot(211)plt.plot(history.history['acc'])plt.plot(history.history['val_acc'])plt.title('Model Accuracy')plt.xlabel('epoch')plt.ylabel('accuracy')plt.legend(['train','test'])plt.subplot(212)plt.plot(history.history['loss'])plt.plot(history.history['val_loss'])plt.title('Model Loss')plt.xlabel('epoch')plt.ylabel('loss')plt.legend(['train','test'])plt.tight_layout()plt.show() 保存模型123456789101112import osimport tensorflow.gfile as gfilesave_dir = '.\model'if gfile.Exists(save_dir): gfile.DeleteRecursively(save_dir)gfile.MakeDirs(save_dir)model_name = 'keras_mnist.h5'model_path = os.path.join(save_dir,model_name)model.save(model_path)print('Saved trained model at %s' % model_path) Saved trained model at .\model\keras_mnist.h5 加载模型123from keras.models import load_modelmnist_model = load_model(model_path) 统计模型在测试集上的分类结果123456789loss_and_metrics = mnist_model.evaluate(X_test, Y_test, verbose=2)print("Test Loss: &#123;&#125;".format(loss_and_metrics[0]))print("Test Accuracy: &#123;&#125;%".format(loss_and_metrics[1]*100))predicted_classes = mnist_model.predict_classes(X_test)correct_indices = np.nonzero(predicted_classes == y_test)[0]incorrect_indices = np.nonzero(predicted_classes != y_test)[0]print("Classified correctly count: &#123;&#125;".format(len(correct_indices)))print("Classified incorrectly count: &#123;&#125;".format(len(incorrect_indices))) Test Loss: 0.027159390095694836 Test Accuracy: 99.11% Classified correctly count: 9911 Classified incorrectly count: 89 代码地址github]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>mnist</tag>
        <tag>keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv+tensorflow实时检测人脸]]></title>
    <url>%2F2018%2F12%2F29%2FfaceDetection%2F</url>
    <content type="text"><![CDATA[关于人脸检测的说明本文代码使用了opencv自带的人脸检测算法和mtcnn算法，mtcnn有明显的优势，检测成功率基本维持在100%，而且人脸各角度都可以检测成功，所以建议使用mtcnn来进行人脸检测，电脑cpu也可以流畅运行。 需要提前配置的环境：python + opencv + tensorflow 关于mtcnn的介绍，请参见压缩包中的电子书 代码结构说明 detect_face.py定义了mtcnn模型 det 1-3.npy是预训练好的模型，所以不用再对mtcnn进行训练 detect 1-3.py是三种实现方式，下面一一介绍 代码演示detect1.py使用mtcnn对一张图片进行检测，效果如下： detect2.py使用opencv自带的HAAR进行实时人脸检测，当人脸倾斜时无法检测到，效果如下： detect3.py使用MTCNN进行实时人脸检测，无论人脸各个角度，都可以检测到，效果如下： 代码地址github地址 百度云地址 注意：github地址中没有mtcnn的预训练模型，需要自己下载，百度云是完整的]]></content>
      <categories>
        <category>ML/DL</category>
      </categories>
      <tags>
        <tag>人脸检测</tag>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下查看端口占用进程及杀死进程]]></title>
    <url>%2F2018%2F12%2F08%2Flinux-kill-process%2F</url>
    <content type="text"><![CDATA[直接查看进程1ps 通过端口查看进程1lsof –i:端口号 杀死进程1kill -9 pid号]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>查看linux进程并杀死</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下关于screen命令的使用]]></title>
    <url>%2F2018%2F12%2F08%2Flinux-screen%2F</url>
    <content type="text"><![CDATA[因为进入服务器只有一个窗口，当我们用这个窗口跑代码时，就没有办法同时用命令编辑一些文件。为了解决这个问题，我们可以使用screen开启多个进程，用一个进程跑代码，然后将这个窗口折叠到后台，创建新的进程来编辑代码。 当我们想要断开服务器连接仍然让一些程序运行的时候，可以使用screen让程序在后台一直运行。 安装screen (ubuntu系统)1sudo apt-get install screen 创建进程1screen -S 进程名 之后，会进入一个干净的窗口，可以执行相应操作，连续按Ctrl+A、Ctrl+D回到主线程，之前执行的操作会一直在后台运行，直到杀死该进程。 这条命令可以多次使用，创建多个进程。 查看当前screen进程1screen -ls 进入某一进程123#两条命令选其一screen -r 进程名screen -r 进程pid号 终止进程12345#方法一screen -X -S 进程名 quit#方法二先进入要杀死的进程，然后输入exit]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux下screen的使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux添加用户并赋予sudo权限]]></title>
    <url>%2F2018%2F11%2F29%2FaddLinuxUser%2F</url>
    <content type="text"><![CDATA[创建用户123# 在root用户下不用写sudosudo adduser fanfan # 在/home 下会自动创建同名文件夹passwd fanfan # 设置密码，上个命令有时会直接让输入密码，就不需要执行这一步了 删除用户1sudo userdel fanfan 添加sudo权限 su -切换到root vim /etc/sudoers ，在root ALL=(ALL) ALL的下一行添加： 12345# sudo时需要输入密码fanfan ALL=(ALL) ALL# sudo时不需要输入密码fanfan ALL=(ALL) NOPASSWD: ALL 按Esc，再输入:wq!保存文件，要加!，不然保存会出问题]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux添加用户</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[本地访问服务器端jupyter notebook]]></title>
    <url>%2F2018%2F11%2F29%2FremoteJupyter%2F</url>
    <content type="text"><![CDATA[1 登陆远程服务器2 生成配置文件1$ jupyter notebook --generate-config 3 生成密码打开ipython，创建一个密文的密码12345In [1]: from notebook.auth import passwdIn [2]: passwd()Enter password: Verify password: Out[2]: 'sha1:ce23d945972f:34769685a7ccd3d08c84a18c63968a41f1140274' 把生成的密文‘sha:ce…’复制下来 4 修改默认配置文件1$ vim ~/.jupyter/jupyter_notebook_config.py 进行如下修改：12345c.NotebookApp.ip='*'#c.NotebookApp.ip='0.0.0.0' # 有时版本不一样c.NotebookApp.password = u'sha:ce...刚才复制的那个密文'c.NotebookApp.open_browser = Falsec.NotebookApp.port =8888 #随便指定一个端口 5 启动jupyter notebook1$ jupyter notebook 6 远程访问此时应该可以直接从本地浏览器直接访问http://address_of_remote:8888就可以看到jupyter的登陆界面，输入第三步中设置的密码。 7 建立SSH通道如果登陆失败，则有可能是服务器防火墙设置的问题，此时最简单的方法是在本地建立一个ssh通道：在本地终端中输入：12ssh fanfan@222.92.146.251 -L127.0.0.1:1234:127.0.0.1:6666ssh fanfan@47.106.208.254 -L127.0.0.1:1234:127.0.0.1:8888 便可以在localhost:1234直接访问远程的jupyter了。 8 防火墙开端口如果远程不能访问，可能是因为端口没有开，需要开启相应端口，重启防火墙]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>jupyter notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器端安装Anaconda]]></title>
    <url>%2F2018%2F11%2F29%2FinstallAnaconda%2F</url>
    <content type="text"><![CDATA[步骤打开网址：Anaconda清华镜像，复制要下载的文件地址，执行以下命令：123456789101112wget 复制的网址（会下载一个sh文件）sh sh文件名 #执行后，会显示使用条款，按enter继续阅读，会让回答几个问题，全部yesrm -rf sh文件名source ~/.bashrc （使conda生效）#设置清华conda镜像conda config --prepend channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config --set show_channel_urls yes 注意事项 Anaconda3-5.2.0.Linux-x86_64.sh ==&gt; python3.6 Anaconda3-5.3.1.Linux-x86_64.sh ==&gt; python3.7 若wget显示网络不可达，执行以下操作： 123456#centossudo yum -y install wget#ubuntusudo apt-get updatesudo apt-get install wget 若不能运行jupyter notebook，进行如下配置：jupyter notebook配置]]></content>
      <categories>
        <category>环境配置与安装</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>Anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux终端bash美化教程]]></title>
    <url>%2F2018%2F11%2F29%2FbeautifyBash%2F</url>
    <content type="text"><![CDATA[美化步骤12345vim .bashrc添加下行export PS1="Time:\[\033[1;35m\]\T \[\033[0m\]User:\[\033[1;33m\]\u \[\033[0m\]Dir:\[\033[1;32m\]\w\[\033[0m\]\n\$"退出vimsource .bashrc 美化效果 PS1中参数的具体含义参考链接]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo写博客步骤]]></title>
    <url>%2F2018%2F11%2F29%2FwriteArtical%2F</url>
    <content type="text"><![CDATA[博客编写步骤1 进入D:\Blog文件夹下，打开终端 2 输入：hexo new &quot;文件名&quot;，在D:\Blog\source\\_posts目录下创建了文件名.md文件 3 打开文件名.md，编写博客 4 终端输入：hexo d -g提交博客 md文件编写注意事项1234567---title: 博客名categories: 分类名tags: - 标签1 - 标签2--- 更新博客分类与标签页面12hexo cleanhexo d -g]]></content>
      <tags>
        <tag>Hexo发送文章</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简易安卓聊天软件之思路架构及源码]]></title>
    <url>%2F2018%2F11%2F04%2FweChat%2F</url>
    <content type="text"><![CDATA[安卓聊天软件完成的功能罗列1 登陆2 动态显示好友列表3 服务端程序4 客户端程序5 安全退出6 与多好友聊天，屏幕切换，可以保存原信息，每次新登陆，可以读取历史记录7 缓存消息，及离线完成 软件架构图 本地Sqlite数据库设计只有一张表，存储聊天消息，表中有三个属性，分别为：发送者 接收者 消息内容 客户端与服务端传输消息协议约定： 客户端新上线的时候，向服务端发送用户名，服务端向客户端发送好友列表与离线消息 客户端 ==&gt; 服务端：发送者：接收者：消息 服务端 ==&gt; 客户端：发送者：接收者：消息 服务端向客户端发送的是消息还是好友列表，以开头是否是”&amp;”符号区分 客户端目录结构（Android Studio） 客户端的基本思路Service负责与服务器进行网络连接与IO读写，无论是发送消息还是接受消息，Service都先把消息存到本地数据库，FriendListActivity与ChatActivity中ListView的显示，都是直接从数据库读取数据。Service与Activity的通信主要使用Intent和广播来进行。 服务端程序服务端基本思路（具体代码见文末源码地址）： 使用一个List存储所有好友 使用一个Map存储在线好友及对应Socket 使用一个Map存储离线消息 软件开发经验总结这次软件开发是以小组形式进行的，最后算是完成了聊天软件的基本功能，这次开发做的好的地方在于一开始小组就先把真个架构图设计好了，包括数据库，后面写代码基本很顺畅，得到的经验就是开发一个软件，做一个项目，写代码真的是很靠后的事情了，前期一定是先通过写用例，梳理好逻辑，画好架构图，后期按照梳理好的逻辑来写代码。后期还需要努力的地方在于UML类图，希望下次开发前期能把UML类图画出来，这样前期工作会更完善，加油，希望可以成为一个专业的程序员。 源码地址源码：github地址 注意：源码中的ImServeFinal.java文件时服务端程序，应该拿出来用java的IDE运行，记得更改ip与端口]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>java</tag>
      </tags>
  </entry>
</search>
